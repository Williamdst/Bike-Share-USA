{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "**Dataset I - CitiBike Trip Data**\n",
    "(https://www.citibikenyc.com/system-data)\n",
    "\n",
    "The goal of this notebook is to get all the required data needed to complete the project. The first dataset that will be compiled is the Trip data from CitiBike. The trip data holds key information about each trip that was taken by customers of the service. For example, columns such as the start time, end station, and gender are recorded for each trip.\n",
    "\n",
    "**Dataset II - Neighborhood Profiles**\n",
    "(https://furmancenter.org/neighborhoods)\n",
    "\n",
    "The next dataset that is needed is the characteristics of each neighborhood in New York City (NYC). The data was gathered by the Furman Center for Real Estate and Urban Policy at New York University. Each dataset has different categories of information about each neighborhood in the city. For example two categories that exist in the dataset are demographics and housing. \n",
    "\n",
    "**Dataset III - Community District GeoJson**\n",
    "(https://data.cityofnewyork.us/City-Government/Community-Districts/yfnk-k7r4)\n",
    "\n",
    "The final piece of information needed is the GeoJson data that actually segments the community districts of NYC. That data is obtained directly from NYCOpenData. *Note: What Furman Center calls Neighborhoods, NYCOpenData calls Community Districts. NYCOpenData has a different dataset called Neighborhood Tabulation Areas which is a more granular division of the city*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the CitiBike S3 Bucket\n",
    "The purpose of this section is to connect, extract, and store all of the tripdata files from the CitiBike S3 bucket into a temporary folder in the working directory. We will use the requests, zipfile, and io packages to retrieve the zipped data and extract it to the temporary folder. \n",
    "\n",
    "*The vision for this project is that all files will be stored in the cloud (AWS S3), separate from the directory of the code. In the section named {} we will upload the extracted data from the temporary folder to a personal S3 bucket and then delete the temporary folder. For the remainder of the project, all data will be pulled from that S3 bucket*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io   # Needed to pull data from CitiBike S3 bucket\n",
    "import os   # Needed to work with folders that will be created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/tripdata/\"    \n",
    "TEMP_BIKE_FOLDER = os.path.join(os.getcwd(),\"TempBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(TEMP_BIKE_FOLDER):\n",
    "    os.makedirs(TEMP_BIKE_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_citi_data(filename: str) -> None:\n",
    "    \"\"\"Connects to Citibike's S3 bucket, extracts, and stores the trip data into the temp_data_folder\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The name of a file in the Citibike S3 bucket (stem only)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a CSV file in the TEMP_BIKE_FOLDER.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exists the function\n",
    "    \n",
    "    # This function would need the CITIBIKE folder and the filename as inputs and it would output the the request (r)\n",
    "    # Put in own script\n",
    "    try:\n",
    "        r = requests.get(CITIBIKE_DATA_FOLDER + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 2017 the bucket endings changed from .zip -> .csv.zip\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(CITIBIKE_DATA_FOLDER + filename[:-4] + '.csv.' + filename[-3:], stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from Citibike S3 Bucket\")       \n",
    "    except requests.exceptions.ConnectionError as errc:\n",
    "        print(errc)\n",
    "        return None\n",
    "    except requests.exceptions.Timeout as errt:\n",
    "        print(errt)\n",
    "        return None\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(err)\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Citibike S3 Bucket\")\n",
    "    \n",
    "    # ==============================================================================================================\n",
    "    #The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    \n",
    "    # This function would need the request and the output folder as input and wouldn't output anything\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0] \n",
    "               \n",
    "        if os.path.exists(TEMP_BIKE_FOLDER + datafile):\n",
    "            print(f\"Skipped: {datafile} already extracted from Citbike S3 Bucket \\n\")\n",
    "            return None\n",
    "        \n",
    "        zip.extract(datafile, path = TEMP_BIKE_FOLDER)\n",
    "    \n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {TEMP_BIKE_FOLDER} \\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2013\",\"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "#Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        pull_citi_data(f\"{year}{month}-citibike-tripdata.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data I - Getting the Neighborhood Codes\n",
    "To download the xlsx files from Furman Center we need the 4 character code for each community district. To get those values we'll use beautifulsoup to scrap the dropdown menu and store the code:name pairs of each community in a dictionary. For example, BK04: Bushwick will be an entry in the dictionary (The BK portion represents the borough Brooklyn).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_codes = {}\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_codes[code.text[:4]] = code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")   # Borough names will be used as filename in the next secion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BK01': 'Greenpoint-Williamsburg',\n",
       " 'BK02': 'Fort_Greene-Brooklyn_Heights',\n",
       " 'BK03': 'Bedford_Stuyvesant',\n",
       " 'BK04': 'Bushwick',\n",
       " 'BK05': 'East_New_York-Starrett_City',\n",
       " 'BK06': 'Park_Slope-Carroll_Gardens',\n",
       " 'BK07': 'Sunset_Park',\n",
       " 'BK08': 'Crown_Heights-Prospect_Heights',\n",
       " 'BK09': 'South_Crown_Heights-Lefferts_Gardens',\n",
       " 'BK10': 'Bay_Ridge-Dyker_Heights',\n",
       " 'BK11': 'Bensonhurst',\n",
       " 'BK12': 'Borough_Park',\n",
       " 'BK13': 'Coney_Island',\n",
       " 'BK14': 'Flatbush-Midwood',\n",
       " 'BK15': 'Sheepshead_Bay',\n",
       " 'BK16': 'Brownsville',\n",
       " 'BK17': 'East_Flatbush',\n",
       " 'BK18': 'Flatlands-Canarsie',\n",
       " 'BX01': 'Mott_Haven-Melrose',\n",
       " 'BX02': 'Hunts_Point-Longwood',\n",
       " 'BX03': 'Morrisania-Crotona',\n",
       " 'BX04': 'Highbridge-Concourse',\n",
       " 'BX05': 'Fordham-University_Heights',\n",
       " 'BX06': 'Belmont-East_Tremont',\n",
       " 'BX07': 'Kingsbridge_Heights-Bedford',\n",
       " 'BX08': 'Riverdale-Fieldston',\n",
       " 'BX09': 'Parkchester-Soundview',\n",
       " 'BX10': 'Throgs_Neck-Co-op_City',\n",
       " 'BX11': 'Morris_Park-Bronxdale',\n",
       " 'BX12': 'Williamsbridge-Baychester',\n",
       " 'MN01': 'Financial_District',\n",
       " 'MN02': 'Greenwich_Village-Soho',\n",
       " 'MN03': 'Lower_East_Side-Chinatown',\n",
       " 'MN04': 'Clinton-Chelsea',\n",
       " 'MN05': 'Midtown',\n",
       " 'MN06': 'Stuyvesant_Town-Turtle_Bay',\n",
       " 'MN07': 'Upper_West_Side',\n",
       " 'MN08': 'Upper_East_Side',\n",
       " 'MN09': 'Morningside_Heights-Hamilton',\n",
       " 'MN10': 'Central_Harlem',\n",
       " 'MN11': 'East_Harlem',\n",
       " 'MN12': 'Washington_Heights-Inwood',\n",
       " 'QN01': 'Astoria',\n",
       " 'QN02': 'Woodside-Sunnyside',\n",
       " 'QN03': 'Jackson_Heights',\n",
       " 'QN04': 'Elmhurst-Corona',\n",
       " 'QN05': 'Ridgewood-Maspeth',\n",
       " 'QN06': 'Rego_Park-Forest_Hills',\n",
       " 'QN07': 'Flushing-Whitestone',\n",
       " 'QN08': 'Hillcrest-Fresh_Meadows',\n",
       " 'QN09': 'Kew_Gardens-Woodhaven',\n",
       " 'QN10': 'South_Ozone_Park-Howard_Beach',\n",
       " 'QN11': 'Bayside-Little_Neck',\n",
       " 'QN12': 'Jamaica-Hollis',\n",
       " 'QN13': 'Queens_Village',\n",
       " 'QN14': 'Rockaway-Broad_Channel',\n",
       " 'SI01': 'St._George-Stapleton',\n",
       " 'SI02': 'South_Beach-Willowbrook',\n",
       " 'SI03': 'Tottenville-Great_Kills'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hood_codes[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data II - Getting the Neighborhood Data Files\n",
    "Similar to the tripdata files from S3, but with exponetially less work, we will use requests to get the data from the Furman Center, store it in a temporary folder where we will upload the data to S3 from there and then delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_HOOD_FOLDER = \"/root/Citi-Bike-Expansion/TempHoodData/\"\n",
    "\n",
    "if not os.path.exists(TEMP_HOOD_FOLDER):\n",
    "    os.makedirs(TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_hood_data(code: str, name: str) -> None:\n",
    "    \"\"\"Uses the scraped neighborhood code to download the xlsx data from Furman Center\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    code: str\n",
    "        The 4 character neighborhood string\n",
    "    name: str\n",
    "        The actual name of the neighborhood\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be an XLSX file in the TEMP_HOOD_FOLDER.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = f\"https://furmancenter.org/files/NDP/{code}_NeighborhoodDataProfile.xlsx\"\n",
    "    \n",
    "    if os.path.exists(TEMP_HOOD_FOLDER + f\"{code}_{name}.xlsx\"):\n",
    "        print(f\"Skipped: {code}_{name} already downloaded from Furman Center\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        r3 = requests.get(file)\n",
    "        r3.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Request Success: {file} from Furman Center\")\n",
    "    \n",
    "    with open(TEMP_HOOD_FOLDER + f\"{code}_{name}.xlsx\", 'wb') as output:\n",
    "        output.write(r3.content)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hood_codes.items():\n",
    "    pull_hood_data(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload TripData to Personal S3 Bucket\n",
    "The purpose of this section is to take the downloaded files and upload them to my own personal S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This code can be executed with your own S3 bucket by changing the following values:\n",
    "# ACCESS_KEY_ID, ACCESS_SECRET_KEY, bucket, prefix (optional)\n",
    "\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "     's3',\n",
    "     aws_access_key_id = ACCESS_KEY_ID,\n",
    "     aws_secret_access_key = ACCESS_SECRET_KEY\n",
    ")\n",
    "\n",
    "bucket = 'williams-citibike'   # Premade bucket in S3\n",
    "trip_prefix = 'TripData'   # Premade folder inside the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted([file for file in os.listdir(TEMP_BIKE_FOLDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket is where you want to store the file\n",
    "# Object is what you want the name of the file to be\n",
    "# Upload_file is the file that you want to upload\n",
    "\n",
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(trip_prefix,key)).upload_file(TEMP_BIKE_FOLDER + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_BIKE_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Neighborhood Data to Personal S3 Bucket\n",
    "The purpose of this section is to take the downloaded files and upload them to my own personal S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_prefix = \"HoodData\"\n",
    "filenames = sorted([file for file in os.listdir(TEMP_HOOD_FOLDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(hood_prefix,key)).upload_file(TEMP_HOOD_FOLDER + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_HOOD_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
