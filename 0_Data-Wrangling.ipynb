{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "**Dataset I - CitiBike Trip Data**\n",
    "(https://www.citibikenyc.com/system-data)\n",
    "\n",
    "The goal of this notebook is to get all the required data needed to complete the project. The first dataset that will be compiled is the Trip data from CitiBike. The trip data holds key information about each trip that was taken by customers of the service. For example, columns such as the start time, end station, and gender are recorded for each trip.\n",
    "\n",
    "**Dataset II - Neighborhood Profiles**\n",
    "(https://furmancenter.org/neighborhoods)\n",
    "\n",
    "The next dataset that is needed is the characteristics of each neighborhood in New York City (NYC). The data was gathered by the Furman Center for Real Estate and Urban Policy at New York University. Each dataset has different categories of information about each neighborhood in the city. For example two categories that exist in the dataset are demographics and housing. \n",
    "\n",
    "**Dataset III - Community District GeoJson**\n",
    "(https://data.cityofnewyork.us/City-Government/Community-Districts/yfnk-k7r4)\n",
    "\n",
    "The third data is GeoJSON data that actually segments the community districts of NYC. That data is obtained directly from NYCOpenData. *Note: What Furman Center calls Neighborhoods, NYCOpenData calls Community Districts. NYCOpenData has a different dataset called Neighborhood Tabulation Areas which is a more granular division of the city.*\n",
    "\n",
    "**Dataset IV - Subway Entrance GeoJson**\n",
    "(https://data.cityofnewyork.us/Transportation/Subway-Entrances/drex-xx56)\n",
    "\n",
    "The final dataset is another GeoJSON file that has the information on the entrances of all the subway stations in the city. Again, this data is obtained directly from NYCOpendata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the CitiBike S3 Bucket\n",
    "The purpose of this section is to connect, extract, and store all of the tripdata files from the CitiBike S3 bucket into a temporary folder in the working directory. We will use the requests, zipfile, and io packages to retrieve the zipped data and extract it to the temporary folder. \n",
    "\n",
    "*The vision for this project is that all files needed for any analysis be stored in the cloud (AWS S3), separate from the directory of the code. In the \"Upload...\" sections we will upload the extracted data from the temporary folder to a personal S3 bucket and then delete the temporary folder. For the remainder of the project, all data will be pulled from that S3 bucket*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io   # Needed to pull data from CitiBike S3 bucket\n",
    "import os   # Needed to work with folders that will be created\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/tripdata/\" \n",
    "MY_CITIBIKE = os.path.join(os.getcwd(),\"CitiBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CITIBIKE):\n",
    "    os.makedirs(MY_CITIBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_request(citi_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(citi_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 2017 the bucket endings changed from .zip -> .csv.zip\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(citi_bucket + filename[:-4] + '.csv.' + filename[-3:], stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from Citibike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Citibike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0] \n",
    "               \n",
    "        if os.path.exists(folder + datafile):\n",
    "            print(f\"Skipped: {datafile} already extracted from Citbike S3 Bucket \\n\")\n",
    "            return None\n",
    "        \n",
    "        zip.extract(datafile, path = folder)\n",
    "    \n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2013\",\"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = citi_request(CITIBIKE_DATA_FOLDER, f\"{year}{month}-citibike-tripdata.zip\")\n",
    "        citi_download(r,MY_CITIBIKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the BayWheels S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAYWHEELS_DATA_FOLDER = \"https://s3.amazonaws.com/baywheels-data/\" \n",
    "MY_BAYWHEELS = os.path.join(os.getcwd(),\"BayWheelsData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BAYWHEELS):\n",
    "    os.makedirs(MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_request(bay_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(bay_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201905 the buckets changed from fordgobike -> baywheels\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(bay_bucket + filename.replace('fordgobike','baywheels'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from BayWheels S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BayWheels S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "\n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "            \n",
    "        if os.path.exists(folder + datafile):\n",
    "            print(f\"Skipped: {datafile} already extracted from BayWheels S3 Bucket \\n\")\n",
    "            return None\n",
    "        \n",
    "        zip.extract(datafile, path = folder)\n",
    "    \n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = bay_request(BAYWHEELS_DATA_FOLDER, f\"{year}{month}-fordgobike-tripdata.csv.zip\")\n",
    "        bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = bay_request(BAYWHEELS_DATA_FOLDER, f\"2017-fordgobike-tripdata.csv.zip\")\n",
    "bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the Capital Bikeshare S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPITAL_DATA_FOLDER = \"https://s3.amazonaws.com/capitalbikeshare-data/\" \n",
    "MY_CAPITAL = os.path.join(os.getcwd(),\"CapitalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CAPITAL):\n",
    "    os.makedirs(MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_request(capital_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(capital_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None   \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Capital S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        if zip.namelist()[0][4] == 'Q':\n",
    "            for i in range(len(zip.namelist())):\n",
    "                datafile = zip.namelist()[i]\n",
    "                \n",
    "                if os.path.exists(folder + datafile):\n",
    "                    print(f\"Skipped: {datafile} already extracted from Capital S3 Bucket \\n\")\n",
    "                    continue\n",
    "        \n",
    "                zip.extract(datafile, path = folder)\n",
    "                print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "\n",
    "            if os.path.exists(folder + datafile):\n",
    "                print(f\"Skipped: {datafile} already extracted from Capital S3 Bucket \\n\")\n",
    "                return None\n",
    "\n",
    "            zip.extract(datafile, path = folder)\n",
    "            print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}{month}-capitalbikeshare-tripdata.zip\")\n",
    "        capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = [\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}-capitalbikeshare-tripdata.zip\")\n",
    "    capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the BlueBike S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUEBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/hubway-data/\" \n",
    "MY_BLUEBIKE = os.path.join(os.getcwd(),\"BlueBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BLUEBIKE):\n",
    "    os.makedirs(MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_request(blue_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(blue_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201905 the buckets changed from fordgobike -> baywheels\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(blue_bucket + filename.replace('hubway','bluebikes'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename.replace('hubway','bluebikes')} requested from BlueBike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BlueBike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "            \n",
    "        if os.path.exists(folder + datafile):\n",
    "            print(f\"Skipped: {datafile} already extracted from BayWheels S3 Bucket \\n\")\n",
    "            return None\n",
    "        \n",
    "        zip.extract(datafile, path = folder)\n",
    "    \n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2015\",\"2016\",\"2017\",\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = blue_request(BLUEBIKE_DATA_FOLDER, f\"{year}{month}-hubway-tripdata.zip\")\n",
    "        blue_download(r, MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = ['2011','2012','2013','2014_1','2014_2']\n",
    "\n",
    "for year in yearlist:\n",
    "    r = blue_request(BLUEBIKE_DATA_FOLDER, f\"hubway_Trips_{year}.csv\")\n",
    "    url_content = r.content\n",
    "    csv_file = open(f'/root/Citi-Bike-Expansion/BlueBikeData/{year}-hubway-tripdata.csv', 'wb')\n",
    "\n",
    "    csv_file.write(url_content)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the Divvy S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVVY_DATA_FOLDER = \"https://divvy-tripdata.s3.amazonaws.com/\" \n",
    "MY_DIVVY = os.path.join(os.getcwd(),\"DivvyData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_DIVVY):\n",
    "    os.makedirs(MY_DIVVY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_request(divvy_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(divvy_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None    \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BayWheels S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_download(r: requests.models.Response, folder: str, year) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        if year == '2013':\n",
    "            datafile = zip.namelist()[2]\n",
    "            \n",
    "            if os.path.exists(folder + datafile):\n",
    "                print(f\"Skipped: {datafile} already extracted from Divvy S3 Bucket \\n\")\n",
    "                return None\n",
    "        \n",
    "            zip.extract(datafile, path = folder)\n",
    "    \n",
    "            print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "        \n",
    "        elif year == '2014':        \n",
    "            for file in zip.namelist():\n",
    "                if re.match('(Divvy_Trips_\\d{4}.{3,5}.csv)$', file):\n",
    "                    datafile = file\n",
    "\n",
    "                    if os.path.exists(folder + datafile):\n",
    "                        print(f\"Skipped: {datafile} already extracted from Divvy S3 Bucket \\n\")\n",
    "                        return None\n",
    "\n",
    "                    zip.extract(datafile, path = folder)\n",
    "\n",
    "                    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "        \n",
    "        elif int(year) < 2018:        \n",
    "            for file in zip.namelist():\n",
    "                if re.match('(Divvy_Trips_\\d{4}.{3,4}.csv)$', file):\n",
    "                    datafile = file\n",
    "\n",
    "                    if os.path.exists(folder + datafile):\n",
    "                        print(f\"Skipped: {datafile} already extracted from Divvy S3 Bucket \\n\")\n",
    "                        return None\n",
    "\n",
    "                    zip.extract(datafile, path = folder)\n",
    "\n",
    "                    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "        \n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "\n",
    "            if os.path.exists(folder + datafile):\n",
    "                print(f\"Skipped: {datafile} already extracted from Divvy S3 Bucket \\n\")\n",
    "                return None\n",
    "\n",
    "            zip.extract(datafile, path = folder)\n",
    "\n",
    "            print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Success: 202004-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202004-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202005-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202005-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202006-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202006-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202007-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202007-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202008-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202008-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202009-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202009-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202010-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202010-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202011-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202011-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: 202012-divvy-tripdata.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 202012-divvy-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2020\"]\n",
    "monthlist = [\"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"{year}{month}-divvy-tripdata.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Success: Divvy_Stations_Trips_2013.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Stations_Trips_2013/Divvy_Trips_2013.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Stations_Trips_2014_Q1Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2014_Q1Q2.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Stations_Trips_2014_Q3Q4.zip requested from BayWheels S3 Bucket\n",
      "404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2015_Q1Q2.zip\n",
      "Request Success: Divvy_Trips_2015-Q1Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2015-Q1.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2015-Q2.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2015_Q3Q4.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2015_Q4.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2015_09.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2015_08.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2015_07.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2016_Q1Q2.zip requested from BayWheels S3 Bucket\n",
      "Request Success: Divvy_Trips_2016_Q3Q4.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2016_Q3.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2016_Q4.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2017_Q1Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2017_Q1.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2017_Q2.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2017_Q3Q4.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2017_Q3.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Extract Success: Divvy_Trips_2017_Q4.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2018_Q1.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2018_Q1.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2018_Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2018_Q2.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2018_Q3.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2018_Q3.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2018_Q4.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2018_Q4.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2019_Q1.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2019_Q1 unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2019_Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2019_Q2 unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2019_Q3.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2019_Q3.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2019_Q4.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2019_Q4.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Trips_2020_Q1.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2020_Q1.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2020_Q2.zip\n",
      "404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2020_Q3.zip\n",
      "404 Client Error: Not Found for url: https://divvy-tripdata.s3.amazonaws.com/Divvy_Trips_2020_Q4.zip\n"
     ]
    }
   ],
   "source": [
    "yearlist = ['2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "\n",
    "for year in yearlist:\n",
    "    if year == '2013':\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)\n",
    "    \n",
    "    elif year == '2014':\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}_{half}.zip\")\n",
    "            divvy_download(r, MY_DIVVY, year)       \n",
    "    \n",
    "    elif int(year) < 2018:\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            try:\n",
    "                # 404 Error for 2015_Q1Q2 (the only file that gets run in the except)\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}-{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "    else:\n",
    "        for quarter in ['Q1','Q2','Q3','Q4']:\n",
    "            try:\n",
    "                # 404 Errors for 2020Q2 - 2020Q4\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{quarter}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move one file from a subdirectory into the main DivvyData folder\n",
    "source = os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2013','Divvy_Trips_2013.csv')\n",
    "destination = os.path.join(os.getcwd(),'DivvyData')\n",
    "remove = os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the file and delete the subdirectory\n",
    "shutil.move(source,destination)\n",
    "shutil.rmtree(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data I - Getting the Neighborhood Codes\n",
    "To download the xlsx files from Furman Center we need the 4 character code for each community district. To get those values we'll use beautifulsoup to scrap the dropdown menu and store the code:name pairs of each community in a dictionary. For example, BK04: Bushwick will be an entry in the dictionary (The BK portion represents the borough Brooklyn).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_codes = {}\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_codes[code.text[:4]] = code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")   # Borough names will be used as filename in the next secion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data II - Getting the Neighborhood Data Files\n",
    "With the neighborhood codes available, we can send a request to the Furman Center, download their excel files, and store it in a temporary folder. This is going to be a simpler and similar process to the tripdata files because we don't have to deal with zipped folders. Later the data will be uploaded to the S3 Bucket and then deleted from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_HOOD_FOLDER = \"/root/Citi-Bike-Expansion/TempHoodData/\"\n",
    "\n",
    "if not os.path.exists(TEMP_HOOD_FOLDER):\n",
    "    os.makedirs(TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_hood_data(code: str, name: str, folder: str) -> None:\n",
    "    \"\"\"Uses the scraped neighborhood code to download the xlsx data from Furman Center\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    code: str\n",
    "        The 4 character neighborhood string\n",
    "    name: str\n",
    "        The actual name of the neighborhood\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be an XLSX file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = f\"https://furmancenter.org/files/NDP/{code}_NeighborhoodDataProfile.xlsx\"\n",
    "    \n",
    "    if os.path.exists(folder + f\"{code}_{name}.xlsx\"):\n",
    "        print(f\"Skipped: {code}_{name} already downloaded from Furman Center\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        r3 = requests.get(file)\n",
    "        r3.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Request Success: {file} from Furman Center\")\n",
    "    \n",
    "    with open(folder + f\"{code}_{name}.xlsx\", 'wb') as output:\n",
    "        output.write(r3.content)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hood_codes.items():\n",
    "    pull_hood_data(key, value, TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload TripData to Personal S3 Bucket\n",
    "The purpose of this section is to take the downloaded files and upload them to my own personal S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This code can has to be executed with your own S3 bucket by changing the following string values:\n",
    "# ACCESS_KEY_ID, ACCESS_SECRET_KEY, bucket, prefix \n",
    "\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "     's3',\n",
    "     aws_access_key_id = ACCESS_KEY_ID,\n",
    "     aws_secret_access_key = ACCESS_SECRET_KEY\n",
    ")\n",
    "\n",
    "bucket = 'williams-citibike'   # Premade bucket in S3\n",
    "trip_prefix = 'TripData'   # Premade folder inside the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted([file for file in os.listdir(MY_CITIBIKE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket is where you want to store the file\n",
    "# Object is what you want the name of the file to be\n",
    "# Upload_file is the file that you want to upload\n",
    "\n",
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(trip_prefix,key)).upload_file(os.path.join(MY_CITIBIKE,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(MY_CITIBIKE)   # Deletes the temporary folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Neighborhood Data to Personal S3 Bucket\n",
    "The purpose of this section is to take the downloaded files and upload them to my own personal S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_prefix = \"HoodData\"\n",
    "filenames = sorted([file for file in os.listdir(TEMP_HOOD_FOLDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(hood_prefix,key)).upload_file(TEMP_HOOD_FOLDER + key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_HOOD_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
