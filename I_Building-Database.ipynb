{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.7/site-packages (2.8.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction I - Creating the Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A - Installs, Imports, Functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.7)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from fsspec>=0.8.0->s3fs) (1.5.0)\n",
      "Requirement already satisfied: botocore<1.19.53,>=1.19.52 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.19.52)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.8.0->s3fs) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries\n",
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data, table: str, sep = ','):\n",
    "    \"\"\"Uploads dataframe to the table in the database\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: psycopg2.extensions.connection\n",
    "        The connection to the database\n",
    "    data: pandas.DataFrame\n",
    "        The dataframe to be uploaded\n",
    "    table: str\n",
    "        The name of the table where the data will be stored\n",
    "    sep: str\n",
    "        The seperator to use when saving the dataframe to a csv\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the data should be in specified table of the database\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    datastream = StringIO()\n",
    "    \n",
    "    data.to_csv(datastream, sep=sep, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.execute('rollback;')\n",
    "    cursor.copy_from(datastream, table, sep=sep)\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS staging;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the baywheels data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "        \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/2017-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201801-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201802-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201803-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201804-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201805-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201806-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201807-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201808-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201809-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201810-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201811-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201812-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201901-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201902-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201903-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201904-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201905-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201906-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201907-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201908-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201909-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201910-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201911-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201912-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202001-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202002-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202003-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202004-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202005-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202006-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202007-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202008-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202009-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202010-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202011-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202012-baywheels-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the blue bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201501-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201502-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201503-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201504-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201505-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201506-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201507-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201508-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201509-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201510-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201511-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201512-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201601-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201602-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201603-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201604-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201605-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201606-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201607-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201608-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201609-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201610-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201611-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201612-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201701-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201702-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201703-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201704-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201705-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201706-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201707-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201708-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201709-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201710-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201711-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201712-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201801_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201802_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201803_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201804-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201805-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201806-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201807-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201808-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201809-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201810-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201811-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201812-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201901-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201902-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201903-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201904-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201905-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201906-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201907-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201908-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201909-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201910-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201911-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201912-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202001-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202002-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202003-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202004-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202005-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202006-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202007-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202008-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202009-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202010-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202011-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202012-bluebikes-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the capital bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201801_capitalbikeshare_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201802-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201803-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201804-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201805-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201806-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201807-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201808-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201809-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201810-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201811-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201812-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201901-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201902-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201903-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201904-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201905-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201906-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201907-capitalbikeshare-tripdata\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201908-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201909-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201910-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201911-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201912-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202001-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202002-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202003-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202004-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202005-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202006-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202007-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202008-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202009-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202012-capitalbikeshare-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bikeID:gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the citi bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\", usecols=list(range(0,11)))   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-09 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-10 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-11 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-12 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201306-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-01 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-02 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-03 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-04 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-05 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-06 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201409-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201410-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201411-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201412-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201501-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201502-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201503-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201504-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201505-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201506-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201507-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201508-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201509-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201510-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201511-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201512-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201601-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201602-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201603-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201604-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201605-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201606-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201607-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201608-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201609-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201610-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201611-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201612-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201701-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201702-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201703-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201704-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201705-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201706-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201707-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201708-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201709-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201710-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201711-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201712-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201801-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201802-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201803-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201804-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201805-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201806-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201807-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201808-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201809-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201810-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201811-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201812-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201901-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201902-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201903-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201904-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201905-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201906-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201907-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201908-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201909-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201910-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201911-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201912-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202001-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202002-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202003-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202004-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202005-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202006-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202007-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202008-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202009-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202010-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202011-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202012-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in citi_filenames:\n",
    "    populate_citi_staging(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_tripduration_query = \"\"\"\n",
    "        ALTER TABLE staging.citi_trip\n",
    "        DROP COLUMN tripduration\n",
    "        \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(drop_tripduration_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the divvy bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "            \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        \n",
    "        upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "        \n",
    "    print(f\"Finished Uploading to Divvy Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202004-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202005-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202006-divvy-tripdata.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a9bcff6e90a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdivvy_filenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpopulate_divvy_staging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-6e351700509b>\u001b[0m in \u001b[0;36mpopulate_divvy_staging\u001b[0;34m(datafile)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mupload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'staging.divvy_trip_test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-3615b52aaff0>\u001b[0m in \u001b[0;36mupload_data\u001b[0;34m(conn, data, table, sep)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mdatastream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatastream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mdatastream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   3202\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3203\u001b[0m         )\n\u001b[0;32m-> 3204\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m             )\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_save_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_i\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/formats/csvs.py\u001b[0m in \u001b[0;36m_save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    336\u001b[0m                 \u001b[0mdecimal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m                 \u001b[0mdate_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquoting\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             )\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mto_native_types\u001b[0;34m(self, slicer, na_rep, float_format, decimal, quoting, **kwargs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2070\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2071\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Derving the Station Tables\n",
    "\n",
    "There isn't an explicit list of stations for each service however, embedded in each trip is the starting and ending station information. With that information it is possible to use SQL to derive the unique stations that are in each service. For each service the same set of steps will be followed:\n",
    "\n",
    "- Determine the unique list of stations from the staging table and save it as a dataframe. Stations that aren't consumer stations are dropped in the retrieval process. \n",
    "- Convert dataframe into a geodataframe by using combining the  latitude, longitude coordinates into a point geometry \n",
    "- Get the zipcode data for each station using the google API. Some stations require a manual zipcode entry\n",
    "- Create the table in the database and upload the data to it\n",
    "- Add a column to the table that identifies which service the stations are associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: geopandas in /opt/conda/lib/python3.7/site-packages (0.9.0)\n",
      "Requirement already satisfied: fiona>=1.8 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.8.18)\n",
      "Requirement already satisfied: pyproj>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (3.0.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.0.1)\n",
      "Requirement already satisfied: shapely>=1.6 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.7.1)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (19.3.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (0.7.1)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (7.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2020.12.5)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.14.0)\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(conn, service: str, drop_indices: list=[]) -> pd.DataFrame():\n",
    "    \"\"\"Derives the unqiue stations from the trip data in the staging schema\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: psycopg2.extensions.connection\n",
    "        The connection to the database\n",
    "    service : str\n",
    "        One of the five bikeshare services of interest\n",
    "    drop_indices: list\n",
    "        A list of indices to drop before returning the stations. Used for stations that aren't actual stations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        Returns a dataframe containing the stations information\n",
    "    \"\"\"\n",
    "    \n",
    "    station_query = f\"\"\"\n",
    "            SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "              FROM staging.{service}_trip\n",
    "             WHERE end_lat > 0\n",
    "             UNION\n",
    "            SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long\n",
    "              FROM staging.{service}_trip\n",
    "            ORDER BY endid, end_lat\n",
    "            \"\"\"\n",
    "    \n",
    "    station = pd.read_sql(station_query, conn)\n",
    "    station.dropna(inplace=True)\n",
    "    station.drop_duplicates(subset=['endid'], keep='last', inplace=True)\n",
    "    \n",
    "    if len(drop_indices) > 0:\n",
    "        station = station.set_index('endid').drop(drop_indices).reset_index()\n",
    "    \n",
    "    return station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bike_service_name(conn, table: str, name: str, schema: str):\n",
    "    \"\"\"Adds a bikeshare column in the table where every value is the name passed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    conn: psycopg2.extensions.connection\n",
    "        The connection to the database\n",
    "    table : str\n",
    "        The name of the table to be altered\n",
    "    name: str\n",
    "        The value that will fill the new column\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the table will have a new column called bikeshare that is populated with the value of name\n",
    "    \"\"\"\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('rollback;')\n",
    "   \n",
    "    add_name_query = f\"\"\"\n",
    "            alter table {schema}.{table}\n",
    "            add column bikeshare varchar(18);\n",
    "\n",
    "            update {schema}.{table}\n",
    "            set bikeshare = '{name}';\n",
    "            \"\"\"\n",
    "          \n",
    "    \n",
    "    cursor.execute(add_name_query)\n",
    "    conn.commit()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address: str, state_initials = \"\") -> list:\n",
    "    \"\"\"Uses the name of the station to get the zipcode via the Google API\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    address: str\n",
    "        The name of the station\n",
    "    state_initials : str\n",
    "        The 2 CHAR initials of the state to use with componenet filtering\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of Dicts:\n",
    "        A list of the different components of the address\n",
    "    \"\"\"\n",
    "    \n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\" + f\"&components=administrative_area:{state_initials}|country:US\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_latlong_components(lat: float, long: float) -> list:\n",
    "    \"\"\"Uses the coordinates of the station to get the zipcode via the Google API\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat: float\n",
    "        The latitude coordinate of the station\n",
    "    long: float\n",
    "        The longitude corrdinate of the station\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of Dicts:\n",
    "        A list of the different components of the address\n",
    "    \"\"\"\n",
    "        \n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return -1\n",
    "   \n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def extract_zipcode(components: list) -> int:\n",
    "    \"\"\"Iterates through the address components to find the postal code\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    components: list\n",
    "        The latitude coordinate of the station\n",
    "    long: float\n",
    "        The longitude corrdinate of the station\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int:\n",
    "        The zip code value of the address component\n",
    "    \"\"\"\n",
    "    \n",
    "    if components == -1: \n",
    "        return -1\n",
    "    \n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df, state_initials = \"\"):\n",
    "    \"\"\"Uses the df data to determine a station's zip code\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DatFrame\n",
    "        The dataframe that the function will be applied on\n",
    "    state_initials: str\n",
    "        The 2 CHAR state initial to use for component filtering\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly, the df will have a zipcode column\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.end_lat < 10:   # No coordinate data means we use the address\n",
    "        return extract_zipcode(get_address_components(address = df.endname, state_initials = state_initials))\n",
    "    else:   # Use the coordinates\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_zip_entry(geodf, entries: list):\n",
    "    \"\"\"Manual inputs zip code data into the dataframe based on the entries passed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    geodf: geopandas.geodataframe.GeoDataFrame\n",
    "        The dataframe that the function will be applied on\n",
    "    entries: list of tuples\n",
    "        A list of tuples where each tuple is in the form (stationid, zipcode)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.geodataframe.GeoDataFrame:\n",
    "        A geodataframe with all the missing zipcodes inputted\n",
    "    \"\"\"\n",
    "    \n",
    "    geodf = geodf.set_index('endid')\n",
    "    \n",
    "    for entry in entries:\n",
    "        geodf.loc[entry[0], 'zipcode'] = entry[1]\n",
    "    \n",
    "    return geodf.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS stations;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(stations_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_remove = ['', '449', '449.0', '420.0', '408', '408.0', '484.0', \n",
    "              '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "              'SF Depot', 'SF Depot-2 (Minnesota St Outbound)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station = get_stations(conn, 'bay', bay_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_decimal(x):\n",
    "    \"\"\"Drops the .0 from a string, if it has it\"\"\"\n",
    "    \n",
    "    if x.endswith('.0'):\n",
    "        return(x[:-2])\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station['endid'] = bay_station.endid.apply(drop_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations that have the same ID was a station relocation\n",
    "bay_station['endid'] = bay_station.endid.apply(drop_decimal)\n",
    "bay_station.drop_duplicates(subset=['endid'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = gpd.GeoDataFrame(bay_station, geometry=gpd.points_from_xy(bay_station.end_long, bay_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "bay_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.bay_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Zip Code Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [('98', 94103)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = manual_zip_entry(bay_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(conn, bay_spatial, 'stations.bay_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'bay_station','bay', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_remove = [153, 158, 164, 223, 229, 230, 308, 382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_station = get_stations(conn, 'blue', blue_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial = gpd.GeoDataFrame(blue_station, geometry=gpd.points_from_xy(blue_station.end_long, blue_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial['zipcode'] = blue_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "blue_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.blue_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(conn, blue_spatial, 'stations.blue_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'blue_station','blue', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 Isn't included because -1 isn't a \"stationary\" station so when we go to calculate distance, the values won't be correct\n",
    "capital_remove = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_station = get_stations(conn, 'capital', capital_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial = gpd.GeoDataFrame(capital_station, geometry=gpd.points_from_xy(capital_station.end_long, capital_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "capital_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.capital_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_input_zipcode(df):\n",
    "    \"\"\"Uses the df data to determine a station's zip code for capital bike only\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DatFrame\n",
    "        The dataframe that the function will be applied on\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly, the df will have a zipcode column\n",
    "    \"\"\"\n",
    "    \n",
    "    # component filter through the three states until we find a match\n",
    "    state_initials = ['DC', 'VA', 'MD']\n",
    "    \n",
    "    if df.end_lat < 10:   # No coordinates\n",
    "        for state in state_initials:\n",
    "            zip_code = extract_zipcode(get_address_components(address = df.endname, state_initials = state))\n",
    "            if isinstance(zip_code, str):   # If get_address fails it returns -1\n",
    "                return zip_code\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial['zipcode'] = capital_spatial.apply(capital_input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(conn, capital_spatial, 'stations.capital_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'capital_station','capital', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_remove = [-1, 3036, 3650, 3247, 3248, 3446, 3480, 3488, 3633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_station = get_stations(conn, 'citi', citi_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = gpd.GeoDataFrame(citi_station, geometry=gpd.points_from_xy(citi_station.end_long, citi_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "citi_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.citi_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial['zipcode'] = citi_spatial.apply(input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Zip Code Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [(152, 10007)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = manual_zip_entry(citi_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(conn, citi_spatial, 'stations.citi_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'citi_station','citi', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_remove =  ['-1', '1', '360', '361', '363', '512']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_station = get_stations(conn, 'divvy', divvy_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations that have the same ID was a station relocation\n",
    "divvy_station['endid'] = divvy_station.endid.apply(drop_decimal)\n",
    "divvy_station.drop_duplicates(subset=['endid'], keep='last' inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = gpd.GeoDataFrame(divvy_station, geometry=gpd.points_from_xy(divvy_station.end_long, divvy_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "divvy_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.divvy_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial['zipcode'] = divvy_spatial.apply(input_zipcode, state_initials='IL',axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Zip Codes - Manual Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [\n",
    "    ('606', 60302), ('609', 60305), ('610', 60302), ('613', 60302), \n",
    "    ('614',60302), ('617', 60304), ('669', 60611) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = manual_zip_entry(divvy_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Database Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data(conn, divvy_spatial, 'stations.divvy_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'divvy_station','divvy', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction III - Creating the Trip Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"CREATE SCHEMA IF NOT EXISTS trips\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for this Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_from_staging(conn, service):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('rollback;')\n",
    "    \n",
    "    trip_from_staging_query = f\"\"\"\n",
    "            CREATE TABLE trips.{service}_trip as (\n",
    "                SELECT \n",
    "                *, \n",
    "                CASE WHEN \n",
    "                     duration > 0 \n",
    "                   THEN ROUND(distance/(duration / 60), 2) \n",
    "                END AS speed\n",
    "                FROM (\n",
    "                    SELECT \n",
    "                      starttime, \n",
    "                      endtime, \n",
    "                      ROUND((EXTRACT(epoch FROM (endtime - starttime))/60)::NUMERIC, 2) AS duration, \n",
    "                      startid, \n",
    "                      startname, \n",
    "                      endid, \n",
    "                      endname,\n",
    "                      CASE WHEN \n",
    "                            s1.latitude > 0 AND s2.latitude > 0 \n",
    "                           THEN ROUND(CAST(ST_Distance(s1.geometry, s2.geometry)*0.000621371 AS NUMERIC),2)\n",
    "                      END AS distance\n",
    "                    FROM staging.{service}_trip AS {service}\n",
    "                    LEFT JOIN stations.{service}_station AS s1\n",
    "                      ON {service}.startid = s1.stationid::NUMERIC\n",
    "                    LEFT JOIN stations.{service}_station AS s2\n",
    "                      ON {service}.endid = s2.stationid::NUMERIC\n",
    "                ) AS {service}_table\n",
    "            );\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor.execute(trip_from_staging_query)\n",
    "    conn.commit()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_trips(conn, service: str, drop_indices: list):\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('rollback;')\n",
    "    \n",
    "    drop_indices = [str(element) for element in drop_indices]\n",
    "    drop_indices = '(' + \",\".join(drop_indices) + ')'\n",
    "    \n",
    "    delete_non_trips_query = f\"\"\"\n",
    "            DELETE FROM trips.{service}_trip\n",
    "            WHERE startid IN {drop_indices}\n",
    "                OR endid IN {drop_indices}\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor.execute(delete_non_trips_query)\n",
    "    conn.commit()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BayWheels Trip Table\n",
    "The BayWheels table doesn't fit the generic format for both of the functions and has to be hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station_table_query = \"\"\"\n",
    "        CREATE TABLE trips.bay_trip AS (\n",
    "            SELECT \n",
    "              *, \n",
    "              CASE WHEN \n",
    "                     duration > 0 \n",
    "                   THEN ROUND(distance/(duration / 60), 2) \n",
    "              END AS speed\n",
    "            FROM (\n",
    "                SELECT \n",
    "                  bay.starttime, \n",
    "                  bay.endtime, \n",
    "                  ROUND((EXTRACT(epoch FROM (endtime - starttime))/60)::NUMERIC, 2) AS duration, \n",
    "                  replace(bay.startid, '.0','') as startid \n",
    "                  startname, \n",
    "                  replace(bay.endid, '.0','') as endid \n",
    "                  endname,\n",
    "                  CASE WHEN \n",
    "                        s1.latitude > 0 AND s2.latitude > 0\n",
    "                       THEN ROUND(CAST(ST_Distance(s1.geometry, s2.geometry)*0.000621371 AS NUMERIC),2)\n",
    "                  END AS distance\n",
    "                FROM staging.bay_trip AS bay\n",
    "                LEFT JOIN stations.bay_station AS s1\n",
    "                  ON replace(bay.startid,'.0','') = s1.stationid\n",
    "                LEFT JOIN stations.bay_station AS s2\n",
    "                  ON replace(bay.endid, '.0','') = s2.stationid\n",
    "            ) AS bay_table \n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "Queries.execute_query(conn, bay_station_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_bay_non_trips_query = \"\"\"\n",
    "            DELETE FROM trips.bay_trip\n",
    "            WHERE startid IN ('449', '449.0', '420.0', '408', '408.0', '484.0', \n",
    "                              '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "                              'SF Depot', 'SF Depot-2 (Minnesota St Outbound)'\n",
    "                          )\n",
    "            \n",
    "              OR endid IN ('449', '449.0', '420.0', '408', '408.0', '484.0', \n",
    "                           '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "                           'SF Depot', 'SF Depot-2 (Minnesota St Outbound)'\n",
    "                       );\n",
    "            \"\"\"\n",
    "Queries.execute_query(conn, delete_bay_non_trips_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'bay_trip','bay', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BlueBike Trips Derviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_from_staging(conn, 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_non_trips(conn, 'blue', blue_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'blue_trip','blue', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CapitalBike Trips Derviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_from_staging(conn, 'capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'capital_trip','capital', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CitiBike Trips Derviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_from_staging(conn, 'citi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_non_trips(conn, 'citi', citi_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'citi_trip','citi', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DivvyBike Trips Derviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_from_staging(conn, 'divvy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some things that shouldn't be stations and thus not included in the trips. For example DIVVVY PARTS TESTING shouldn't be a station and the trips that that 'station' are involved in aren't consumer trips. \n",
    "\n",
    "Then, there are some things that shouldn't be stations, but still included in the trips. For example, a blank start name (startid=-1) corresponds to an error recording the station information, but it is still a valid trip. A blank name shouldn't be in the stations table, but we shouldn't remove that trip from the trip table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_non_trips(conn, 'divvy', divvy_remove[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_bike_service_name(conn, 'divvy_trip','divvy', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of the updates and deletes we should do a full vacuum before moving forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.VACUUM_FULL(conn)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
