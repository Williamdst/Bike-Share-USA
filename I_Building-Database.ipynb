{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.8.6-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction I - Creating the Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.4)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.1.2)\n",
      "Collecting botocore<1.17.45,>=1.17.44\n",
      "  Using cached botocore-1.17.44-py2.py3-none-any.whl (6.5 MB)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "\u001b[31mERROR: boto3 1.16.36 has requirement botocore<1.20.0,>=1.19.36, but you'll have botocore 1.17.44 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.196 has requirement botocore==1.19.36, but you'll have botocore 1.17.44 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.36\n",
      "    Uninstalling botocore-1.19.36:\n",
      "      Successfully uninstalled botocore-1.19.36\n",
      "Successfully installed botocore-1.17.44\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)\n",
    "trip_filenames = fs.ls(\"s3://williams-citibike/TripData/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "staging_table_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(64),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(64),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL,\n",
    "                   bikeID INTEGER,\n",
    "                   usertype VARCHAR(16),\n",
    "                   birthyear REAL,\n",
    "                   gender SMALLINT                \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data: pd.DataFrame(), table: str):\n",
    "    datastream = StringIO()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    data.to_csv(datastream, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.copy_from(datastream,table,sep=',')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\")   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        # data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        # data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging')\n",
    "        \n",
    "    datastream.close()\n",
    "    print(f\"Finished Uploading to Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Staging Table: williams-citibike/TripData/202011-citibike-tripdata.csv\n",
      "Finished Uploading to Staging Table: williams-citibike/TripData/202012-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "for file in trip_filenames:\n",
    "    populate_staging(file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Creating the Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "trip_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS trip (\n",
    "                starttime TIMESTAMP,\n",
    "                endtime TIMESTAMP,\n",
    "                tripduration NUMERIC,\n",
    "                startID NUMERIC,\n",
    "                endID NUMERIC,\n",
    "                usertype VARCHAR(16),\n",
    "                age REAL,\n",
    "                gender SMALLINT\n",
    "            ) PARTITION BY RANGE (starttime);\n",
    "            \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(trip_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(year: int, month: int) -> None: #Tables\n",
    "    \"\"\"Docstring \n",
    "    \n",
    "    \"\"\"\n",
    "    nxt_month = month+1\n",
    "    nxt_year = year   # Always the same as current year unless the month is December\n",
    "    \n",
    "    if month == 12:   # If Decemember sets the year-mon to January of the next year\n",
    "        nxt_month = 1\n",
    "        nxt_year = year+1\n",
    "    \n",
    "    month = str(month).zfill(2)\n",
    "    nxt_month = str(nxt_month).zfill(2)\n",
    "    \n",
    "    # Move this to the Tables module\n",
    "    # ----- This can use Queries.execute_query(conn, partition_query)\n",
    "    partition_query = f\"\"\"\n",
    "            CREATE TABLE trip_y{year}m{month} PARTITION OF trip\n",
    "            FOR VALUES FROM ('{year}-{month}-01') TO ('{nxt_year}-{nxt_month}-01');\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor.execute(\"rollback;\")\n",
    "    cursor.execute(partition_query)\n",
    "    conn.commit()\n",
    "    # --------------------------\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist13 = [2013]\n",
    "monthlist13 = [6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for year in yearlist13:\n",
    "    for month in monthlist13:\n",
    "        create_partition(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist14_20 = [2014, 2015, 2016, 2017, 2018, 2019,2020]\n",
    "monthlist14_20 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for year in yearlist14_20:\n",
    "    for month in monthlist14_20:\n",
    "        create_partition(year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following code we will be converting the tripduration from seconds to minutes and converting the birthyear to age. On a db.t3.micro rds instance it will take 3.3hrs to execute** \n",
    "\n",
    "*Style using CSS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Tables module\n",
    "insert_query2 = \"\"\"\n",
    "        INSERT INTO trip\n",
    "        SELECT DISTINCT starttime, endtime, ROUND(tripduration/60,2), startid, endid, usertype, \n",
    "               CASE WHEN birthyear > 0 THEN 2020 - birthyear\n",
    "                    ELSE birthyear\n",
    "                    END AS age,\n",
    "               gender\n",
    "          FROM staging\n",
    "         ORDER BY starttime, endtime;\n",
    "        \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(insert_query2)\n",
    "conn.commit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the DISTINCT clause we are filtering out trips that are exact duplicates. The reason is that in our data, only exact duplicates are trips that were accidentally duplicated. If all the values are the same except a single value then that represents a different trip. For example, two friends may take a ride from the same stations at the same exact time but one may be male and the other may be female. \n",
    "\n",
    "*Note: It is possible in reality that two separate trips have exactly the same data. However,that would require two people of the same age and gender, starting and stoping at the same stations at the exact same time (down to the second). Additionally, getting rid of duplicates removed only 0.004% of trips. Therefore on the off chance that all 4,797 counted duplicates weren't actually duplicates in real life we removed a miniscule amount of data from our dataset*\n",
    "\n",
    "*Note 2: Our trip table doesn't include the bikeid, so there is a chance that those 4,797 duplicates aren't errors. Those people with the same age and gender, starting and stoping at the same stations at the exact same time (down to the second) might be on different bikes.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Neighborhood Table I - Without the Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_code_names = []\n",
    "\n",
    "#Instead of creating a dictionary like before, we create a list of tuples so that we can make a df\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_code_names.append((code.text[:4], code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_df = pd.DataFrame(hood_code_names, columns=[\"code\", \"hoodname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = {\n",
    "        \"BK\": \"Brooklyn\", \n",
    "        \"BX\": \"Bronx\",\n",
    "        \"MN\": \"Manhattan\",\n",
    "        \"QN\": \"Queens\",\n",
    "        \"SI\": \"Staten\"\n",
    "        }\n",
    "\n",
    "hood_df[\"borough\"] = hood_df[\"code\"].str[0:2].map(borough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>hoodname</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BK01</td>\n",
       "      <td>Greenpoint-Williamsburg</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BK02</td>\n",
       "      <td>Fort_Greene-Brooklyn_Heights</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BK03</td>\n",
       "      <td>Bedford_Stuyvesant</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BK04</td>\n",
       "      <td>Bushwick</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK05</td>\n",
       "      <td>East_New_York-Starrett_City</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                      hoodname   borough\n",
       "0  BK01       Greenpoint-Williamsburg  Brooklyn\n",
       "1  BK02  Fort_Greene-Brooklyn_Heights  Brooklyn\n",
       "2  BK03            Bedford_Stuyvesant  Brooklyn\n",
       "3  BK04                      Bushwick  Brooklyn\n",
       "4  BK05   East_New_York-Starrett_City  Brooklyn"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hood_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Neighborhood Table II - Adding the Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.8.1-py2.py3-none-any.whl (962 kB)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.0.1)\n",
      "Collecting fiona\n",
      "  Using cached Fiona-1.8.18-cp37-cp37m-manylinux1_x86_64.whl (14.8 MB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.0.0.post1-cp37-cp37m-manylinux2010_x86_64.whl (6.4 MB)\n",
      "Collecting shapely\n",
      "  Using cached Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (1.18.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (19.3.0)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (7.0)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.7.1-py3-none-any.whl (7.1 kB)\n",
      "Installing collected packages: munch, click-plugins, cligj, fiona, pyproj, shapely, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.18 geopandas-0.8.1 munch-2.5.0 pyproj-3.0.0.post1 shapely-1.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofile = \"s3://williams-citibike/Community_Districts.geojson\"\n",
    "\n",
    "with fs.open(geofile, 'rb') as file:\n",
    "    districts = gpd.read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boro_cd</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>shape_leng</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311</td>\n",
       "      <td>103177785.365</td>\n",
       "      <td>51549.5578567</td>\n",
       "      <td>MULTIPOLYGON (((-73.97299 40.60881, -73.97259 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313</td>\n",
       "      <td>88195686.2748</td>\n",
       "      <td>65821.875577</td>\n",
       "      <td>MULTIPOLYGON (((-73.98372 40.59582, -73.98305 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312</td>\n",
       "      <td>99525500.0655</td>\n",
       "      <td>52245.8304843</td>\n",
       "      <td>MULTIPOLYGON (((-73.97140 40.64826, -73.97121 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206</td>\n",
       "      <td>42664311.3238</td>\n",
       "      <td>35875.7111725</td>\n",
       "      <td>MULTIPOLYGON (((-73.87185 40.84376, -73.87192 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226</td>\n",
       "      <td>50566410.6415</td>\n",
       "      <td>32820.3983295</td>\n",
       "      <td>MULTIPOLYGON (((-73.86790 40.90294, -73.86796 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  boro_cd     shape_area     shape_leng  \\\n",
       "0     311  103177785.365  51549.5578567   \n",
       "1     313  88195686.2748   65821.875577   \n",
       "2     312  99525500.0655  52245.8304843   \n",
       "3     206  42664311.3238  35875.7111725   \n",
       "4     226  50566410.6415  32820.3983295   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-73.97299 40.60881, -73.97259 ...  \n",
       "1  MULTIPOLYGON (((-73.98372 40.59582, -73.98305 ...  \n",
       "2  MULTIPOLYGON (((-73.97140 40.64826, -73.97121 ...  \n",
       "3  MULTIPOLYGON (((-73.87185 40.84376, -73.87192 ...  \n",
       "4  MULTIPOLYGON (((-73.86790 40.90294, -73.86796 ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes from the Furman Center are exactly the same as the codes seen in the boro_cd column. However, the first number in the boro_cd acts as a category that represents the borough. The original Furman codes, seen in the hood_df, have to be reversed engineered using a maping. Once the mapping is complete, the two dataframes can be merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_num_to_abr = {\n",
    "        \"3\": \"BK\", \n",
    "        \"2\": \"BX\",\n",
    "        \"1\": \"MN\",\n",
    "        \"4\": \"QN\",\n",
    "        \"5\": \"SI\"\n",
    "        }\n",
    "\n",
    "districts[\"boro_cd\"] = districts[\"boro_cd\"].str[0].map(borough_num_to_abr) + districts['boro_cd'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = districts[['boro_cd','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial = hood_df.merge(districts, left_on='code', right_on='boro_cd', how='left').loc[:,['code', 'hoodname', 'borough', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial.sort_values(by='code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial = gpd.GeoDataFrame(hood_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>hoodname</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BK01</td>\n",
       "      <td>Greenpoint-Williamsburg</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.92406 40.71411, -73.92404 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BK02</td>\n",
       "      <td>Fort_Greene-Brooklyn_Heights</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.96929 40.70709, -73.96839 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BK03</td>\n",
       "      <td>Bedford_Stuyvesant</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.91805 40.68721, -73.91800 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BK04</td>\n",
       "      <td>Bushwick</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.89647 40.68234, -73.89653 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK05</td>\n",
       "      <td>East_New_York-Starrett_City</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.86841 40.69473, -73.86868 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                      hoodname   borough  \\\n",
       "0  BK01       Greenpoint-Williamsburg  Brooklyn   \n",
       "1  BK02  Fort_Greene-Brooklyn_Heights  Brooklyn   \n",
       "2  BK03            Bedford_Stuyvesant  Brooklyn   \n",
       "3  BK04                      Bushwick  Brooklyn   \n",
       "4  BK05   East_New_York-Starrett_City  Brooklyn   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-73.92406 40.71411, -73.92404 ...  \n",
       "1  MULTIPOLYGON (((-73.96929 40.70709, -73.96839 ...  \n",
       "2  MULTIPOLYGON (((-73.91805 40.68721, -73.91800 ...  \n",
       "3  MULTIPOLYGON (((-73.89647 40.68234, -73.89653 ...  \n",
       "4  MULTIPOLYGON (((-73.86841 40.69473, -73.86868 ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hood_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction III - Creating the Neighborhood Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "neighborhood_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS neighborhood (\n",
    "            code CHAR(4) PRIMARY KEY,\n",
    "            hoodname VARCHAR NOT NULL,\n",
    "            borough VARCHAR(16) NOT NULL,\n",
    "            geometry GEOGRAPHY(MULTIPOLYGON,4326) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(neighborhood_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the new function\n",
    "hoodstream = StringIO()\n",
    "\n",
    "hood_spatial.to_csv(hoodstream,sep='\\t', index=False, header=False)\n",
    "hoodstream.seek(0)\n",
    "\n",
    "cursor.copy_from(hoodstream,'neighborhood',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table I - Querying from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endid has more distinct values than startid\n",
    "# Tables module\n",
    "stations_query = \"\"\"\n",
    "        SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "          FROM staging \n",
    "         ORDER BY endid;\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = pd.read_sql(stations_query, conn) # Expect long execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations.end_long, stations.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table II - SJoining the Neighborhood Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inner join will remove stations that aren't in NYC (some stations are in NJ).\n",
    "# Additionally it will remove the handful of stations that didn't have information other than the ID\n",
    "\n",
    "stations_spatial = gpd.sjoin(stations_spatial, hood_spatial, how='inner', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = stations_spatial[['endid','endname','code','geometry']].rename(columns={'endid':'stationID','endname':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial.name = stations_spatial.name.str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stationID</th>\n",
       "      <th>name</th>\n",
       "      <th>code</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "      <td>W 52 St &amp; 11 Ave</td>\n",
       "      <td>MN04</td>\n",
       "      <td>POINT (-73.99393 40.76727)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116.0</td>\n",
       "      <td>W 17 St &amp; 8 Ave</td>\n",
       "      <td>MN04</td>\n",
       "      <td>POINT (-74.00150 40.74178)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>212.0</td>\n",
       "      <td>W 16 St &amp; The High Line</td>\n",
       "      <td>MN04</td>\n",
       "      <td>POINT (-74.00682 40.74335)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>334.0</td>\n",
       "      <td>W 20 St &amp; 7 Ave</td>\n",
       "      <td>MN04</td>\n",
       "      <td>POINT (-73.99726 40.74239)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>388.0</td>\n",
       "      <td>W 26 St &amp; 10 Ave</td>\n",
       "      <td>MN04</td>\n",
       "      <td>POINT (-74.00295 40.74972)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     stationID                     name  code                    geometry\n",
       "1         72.0         W 52 St & 11 Ave  MN04  POINT (-73.99393 40.76727)\n",
       "5        116.0          W 17 St & 8 Ave  MN04  POINT (-74.00150 40.74178)\n",
       "28       212.0  W 16 St & The High Line  MN04  POINT (-74.00682 40.74335)\n",
       "123      334.0          W 20 St & 7 Ave  MN04  POINT (-73.99726 40.74239)\n",
       "171      388.0         W 26 St & 10 Ave  MN04  POINT (-74.00295 40.74972)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Consruction IV - Creating the Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "station_table_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS station (\n",
    "                   stationID NUMERIC PRIMARY KEY,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   code CHAR(4) NOT NULL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(station_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with function\n",
    "stationstream = StringIO()\n",
    "stations_spatial.to_csv(stationstream,sep='\\t', index=False, header=False)\n",
    "stationstream.seek(0)\n",
    "\n",
    "cursor.copy_from(stationstream,'station',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction V - Creating the Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_filenames = fs.ls(\"s3://williams-citibike/HoodData/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "lookup_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS lookup(\n",
    "                    alias VARCHAR(5) PRIMARY KEY,\n",
    "                    indicator VARCHAR,\n",
    "                    description VARCHAR\n",
    "                );\n",
    "                \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(lookup_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lst = [2,3,4]\n",
    "names_lst = [\"indicator_category\", \"indicator\", \"description\"]\n",
    "lookup = pd.read_excel(\"s3://\" + hood_filenames[0], sheet_name=1, usecols = cols_lst, names = names_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.sort_values(by=[\"indicator_category\",'indicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Demographics': 'DEM',\n",
    "    'Housing Market and Conditions': 'HSC',\n",
    "    'Land Use and Development': 'LUD',\n",
    "    'Neighborhood Services and Conditions': 'NSC',\n",
    "    'Renters': 'RNT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['indicator_category'] = lookup[\"indicator_category\"].map(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.rename(columns={'indicator_category':'alias'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_group_order = lookup.groupby(\"alias\").cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['alias'] = lookup['alias'] + indicator_group_order.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with function\n",
    "lookupstream = StringIO()\n",
    "\n",
    "lookup.to_csv(lookupstream,sep='\\t', index=False, header=False)\n",
    "lookupstream.seek(0)\n",
    "\n",
    "cursor.copy_from(lookupstream,'lookup',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neighborhood Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hooddata(datafile: str) -> pd.DataFrame:\n",
    "    \"\"\"Grabs the data from the s3 bucket and flattens it to a single row consisting of the neighborhood attributes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        A single row DataFrame that contains the attributes of the neighborhood\n",
    "    \"\"\"\n",
    "    cols_lst = [0,2,3,8]\n",
    "    names_lst = [\"code\", \"indicator category\", \"indicator\", \"2018\"]\n",
    "\n",
    "    # This function is a mess\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'rb') as file:\n",
    "        data = pd.read_excel(file, sheet_name=1, usecols = cols_lst, names = names_lst)\n",
    "       \n",
    "        #In the previous section we did all the alias work, now we can simply input it into the df from lookup['alias']\n",
    "        data = data.sort_values(by=['indicator category','indicator'])\n",
    "        data.insert(1, 'alias', lookup['alias'])\n",
    "        data = data.drop(columns = ['indicator category', 'indicator'])\n",
    "\n",
    "        # Prep the '2018' column so that it can used as the value argument in the pivot_table \n",
    "        data['2018'] = data['2018'].str.replace('$',\"\")\n",
    "        data['2018'] = data['2018'].str.replace(',',\"\")\n",
    "\n",
    "        # Values that are percents get turned into decimals\n",
    "        for index, value in data['2018'].items():\n",
    "            if isinstance(value,str):\n",
    "                if value[-1] == '%':\n",
    "                    data['2018'][index] = float(value.strip('%')) / 100\n",
    "\n",
    "        data['2018'] = pd.to_numeric(data['2018'])\n",
    "\n",
    "        # The pivot_table alphabatizes the columns, but we want to maintain the original order\n",
    "        column_order = ['code'] + list(data['alias'])\n",
    "\n",
    "        data = data.pivot_table(index=['code'],values='2018', columns='alias', dropna=False)\n",
    "        data = data.rename_axis(None, axis=1).reset_index()   # The pivot creates a unnecessary column axis\n",
    "        data['code'] = data['code'][0].replace(\" \",\"\")\n",
    "        data = data.reindex(column_order, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = pd.DataFrame()\n",
    "\n",
    "# This loop only works successfully if there are those specific neighborhood excel files in the HoodData folder\n",
    "for hood in hood_filenames:\n",
    "    hood_profile = hood_profile.append(flatten_hooddata(hood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.fillna(-1)   # We need to fill NaN with -1 so they can be put into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VI - Importing the Neighborhood Profiles into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Module\n",
    "profile_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS profile(\n",
    "                );\n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(profile_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in hood_profile.columns:\n",
    "    if name == 'code':\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} CHAR(4) PRIMARY KEY;\n",
    "                    \"\"\"\n",
    "    else:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} REAL;\n",
    "                    \"\"\"\n",
    "        \n",
    "    cursor.execute(\"rollback;\")\n",
    "    cursor.execute(import_column_query)\n",
    "    conn.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use the function\n",
    "profilestream = StringIO()\n",
    "\n",
    "hood_profile.to_csv(profilestream,sep='\\t', index=False, header=False)\n",
    "profilestream.seek(0)\n",
    "\n",
    "cursor.copy_from(profilestream,'profile',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VII - Purging the Database: Removing Trips that aren't Contained in NYC\n",
    "\n",
    "When the neighbborhood data was inner joined to the station data, the stations that were not in NYC were dropped. Although removed from the stations table, there are still trips in the trip table that have the dropped stations. In this section the goal is to remove those trips that are not fully contained within NYC. \n",
    "\n",
    "*Note: Not in NYC is defined as trip either starting or ending at a station that is not in NYC.*\n",
    "\n",
    "**Before we drop the trips that involve New Jersey (NJ), let's see how much of the market share NJ is gathering over time.**\n",
    "\n",
    "*Note: There are other important questions that could be asked about the NJ data, however, this project is focused on NYC data. For now, more complex NJ based questions are out of scope.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Queries # This is actually going to be the Analyze module in the Queries package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of trips per year\n",
    "all_trips_df = Queries.countYearlyTrips(conn)    # Query-0001 in file # How to use the context manager in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_trips_df = Queries.countYearlyNJTrips(conn)   # Query-0002 in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share = NJ_trips_df.merge(all_trips_df, on='year',suffixes=['_nj','_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share['nj_percent'] = round(market_share['trips_nj'] / market_share['trips_all'], 4)* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>trips_nj</th>\n",
       "      <th>trips_all</th>\n",
       "      <th>nj_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.0</td>\n",
       "      <td>67094</td>\n",
       "      <td>5614874</td>\n",
       "      <td>1.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014.0</td>\n",
       "      <td>55277</td>\n",
       "      <td>8081195</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015.0</td>\n",
       "      <td>182610</td>\n",
       "      <td>9921596</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016.0</td>\n",
       "      <td>618062</td>\n",
       "      <td>13842693</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017.0</td>\n",
       "      <td>948753</td>\n",
       "      <td>16362322</td>\n",
       "      <td>5.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2018.0</td>\n",
       "      <td>1010194</td>\n",
       "      <td>17548339</td>\n",
       "      <td>5.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2019.0</td>\n",
       "      <td>1128002</td>\n",
       "      <td>20551697</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020.0</td>\n",
       "      <td>1264092</td>\n",
       "      <td>19506857</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  trips_nj  trips_all  nj_percent\n",
       "0  2013.0     67094    5614874        1.19\n",
       "1  2014.0     55277    8081195        0.68\n",
       "2  2015.0    182610    9921596        1.84\n",
       "3  2016.0    618062   13842693        4.46\n",
       "4  2017.0    948753   16362322        5.80\n",
       "5  2018.0   1010194   17548339        5.76\n",
       "6  2019.0   1128002   20551697        5.49\n",
       "7  2020.0   1264092   19506857        6.48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_share # Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the NJ data\n",
    "Queries.deleteNJTrips(conn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
