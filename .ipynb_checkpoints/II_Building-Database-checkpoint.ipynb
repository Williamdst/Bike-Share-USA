{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building the Database**\n",
    "\n",
    "In the last notebook we wrangled ten different sets of data which summed to over 350 files. To work with all this data, the best course of action will be to build a database that we can query from in the future. The database that is built in this notebook will have the majority of the data needed to complete the project. Four schemas are used to organize the database:\n",
    "- The Staging Schema: Has all the raw trip data for each of the different bike share services\n",
    "- The Trips Schema: Has all the filtered and edited trip data for each of the different bike share services\n",
    "- The Stations Schema: Has all the stations for each of the different bike share services\n",
    "- The Neighborhoods Schema: Has all the information for different zipcodes within the United States"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connecting to the Database**\n",
    "\n",
    "Using the AWS Management Console a RDS Database running PostgreSQL 12.5 on a db.t3.micro instance was created. In this section, we connect to it using the credentials that were generated at creation time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.8.6-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Construction I - Creating the Staging Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each trip file contains more or less the same information about every single trip that was taken during some time period of the year; the majority being a single month. For the tables in our staging schema we are going to standardize our tables so that every table has the same columns below:\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.2)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.7)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Collecting botocore<1.19.53,>=1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.4.post0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from fsspec>=0.8.0->s3fs) (1.5.0)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: chardet<5.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->fsspec>=0.8.0->s3fs) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "\u001b[31mERROR: boto3 1.17.34 has requirement botocore<1.21.0,>=1.20.34, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.19.34 has requirement botocore==1.20.34, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.34\n",
      "    Uninstalling botocore-1.20.34:\n",
      "      Successfully uninstalled botocore-1.20.34\n",
      "Successfully installed botocore-1.19.52\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import s3fs\n",
    "import sys\n",
    "from urllib.parse import urlencode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introducing the Queries Package**\n",
    "\n",
    "The Queries Package is a custom package containing custom queries that execute against our database. The most used function in this package is the execute_query function which takes in a SQL statement and makes the execution. There are queries that return results and the execute_query function has the option of returning the results as a pandas dataframe or a 2 length tuple of the (column_names, data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(),'Data','Scripts'))\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Access Keys for AWS | GCP**\n",
    "\n",
    "The bucket that was created is private, therefore we need credentials to access it. If the bucket is public then no credentials are need.  The Google Cloud Access Key isn't necessary. The geocoding portion of the project could get very expensive without the free credits that they offer. In the Data folder of the repository I provide the post-geocoding data that to avoid those charges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 bucket is private and to connect to it we need a valid AWS Access Key\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)\n",
    "\n",
    "# For the geocoding we need a valid GCP Api Key \n",
    "google_api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Subsection Structure** -  Each of the 5 following subsections in this section have the same structure\n",
    "<ol>\n",
    "    <li> Create the staging.*_trips table in the database.\n",
    "    <li> Create a custom function to pull the data from S3 and upload it to the table created\n",
    "    <li> Iterate through all the files and call the custom function\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS staging;\"\"\"\n",
    "Queries.execute_query(conn, staging_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/ERD-Staging.png\" width=\"200\" height=\"266\" align=\"center\"/>\n",
    "<p style = \"text-align:center\"> The Entity Relationship Diagram for the 5 Trip Tables in the Staging Schema </p>\n",
    "<p style = \"text-align:center;font-style:italic\"> The start/endID could either both be VARCHAR OR NUMERIC depending on the service </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the BayWheels Staging Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "\n",
    "Queries.execute_query(conn, bay_staging_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the baywheels data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "       \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    # The purpose of the with block is to process and upload the files to the database\n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        # The try-except block handles all the possibilites when trying to read the data\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        # Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        # In hindsight we could have used the tab separator\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        Queries.upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/2017-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201801-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201802-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201803-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201804-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201805-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201806-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201807-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201808-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201809-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201810-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201811-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201812-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201901-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201902-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201903-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201904-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201905-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201906-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201907-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201908-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201909-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201910-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201911-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201912-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202001-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202002-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202003-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202004-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202005-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202006-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202007-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202008-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202009-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202010-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202011-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202012-baywheels-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the BlueBike Staging Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "Queries.execute_query(conn, blue_staging_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the blue bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "\n",
    "    # The purpose of the with block is to process and upload the files to the database\n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        # Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        # In hindsight we could have used the tab separator\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        Queries.upload_data(conn, data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201501-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201502-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201503-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201504-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201505-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201506-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201507-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201508-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201509-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201510-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201511-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201512-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201601-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201602-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201603-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201604-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201605-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201606-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201607-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201608-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201609-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201610-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201611-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201612-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201701-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201702-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201703-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201704-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201705-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201706-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201707-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201708-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201709-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201710-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201711-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201712-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201801_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201802_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201803_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201804-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201805-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201806-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201807-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201808-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201809-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201810-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201811-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201812-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201901-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201902-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201903-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201904-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201905-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201906-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201907-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201908-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201909-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201910-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201911-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201912-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202001-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202002-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202003-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202004-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202005-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202006-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202007-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202008-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202009-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202010-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202011-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202012-bluebikes-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the Capital Staging Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "Queries.execute_query(conn, capital_staging_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the capital bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    # The purpose of the with block is to process and upload the files to the database\n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        # The try-except block handles all the possibilites when trying to read the data\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        # Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        # In hindsight we could have used the tab separator\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        Queries.upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Capital Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201801_capitalbikeshare_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201802-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201803-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201804-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201805-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201806-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201807-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201808-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201809-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201810-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201811-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201812-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201901-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201902-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201903-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201904-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201905-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201906-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201907-capitalbikeshare-tripdata\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201908-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201909-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201910-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201911-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201912-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202001-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202002-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202003-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202004-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202005-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202006-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202007-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202008-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202009-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202012-capitalbikeshare-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the CitiBike Staging Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "Queries.execute_query(conn, citi_staging_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the citi bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\", usecols=list(range(1,11)))   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)\n",
    "        \n",
    "        # Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        # In hindsight we could have used the tab separator\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        Queries.upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-09 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-10 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-11 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-12 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201306-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-01 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-02 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-03 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-04 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-05 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-06 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201409-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201410-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201411-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201412-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201501-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201502-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201503-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201504-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201505-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201506-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201507-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201508-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201509-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201510-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201511-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201512-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201601-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201602-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201603-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201604-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201605-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201606-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201607-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201608-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201609-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201610-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201611-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201612-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201701-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201702-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201703-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201704-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201705-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201706-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201707-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201708-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201709-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201710-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201711-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201712-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201801-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201802-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201803-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201804-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201805-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201806-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201807-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201808-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201809-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201810-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201811-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201812-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201901-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201902-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201903-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201904-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201905-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201906-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201907-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201908-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201909-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201910-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201911-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201912-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202001-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202002-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202003-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202004-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202005-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202006-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202007-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202008-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202009-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202010-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202011-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202012-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in citi_filenames:\n",
    "    populate_citi_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the Divvy Staging Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "Queries.execute_query(conn, divvy_staging_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the divvy bike data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    \n",
    "    # The purpose of the with block is to process and upload the files to the database\n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        # The try-except block handles all the possibilites when trying to read the data\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        \n",
    "        # Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        # In hindsight we could have used the tab separator\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        Queries.upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Divvy Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Construction II - Derving the Station Tables**\n",
    "\n",
    "There isn't an explicit list of stations for each service, however, embedded in each trip is the starting and ending station information. With that information it is possible to use SQL to derive the unique stations that are in each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.9.0-py2.py3-none-any.whl (994 kB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.0.1-cp37-cp37m-manylinux2010_x86_64.whl (6.5 MB)\n",
      "Collecting fiona>=1.8\n",
      "  Using cached Fiona-1.8.18-cp37-cp37m-manylinux1_x86_64.whl (14.8 MB)\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.0.1)\n",
      "Collecting shapely>=1.6\n",
      "  Using cached Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2.0->geopandas) (2020.12.5)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (1.14.0)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (7.0)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona>=1.8->geopandas) (19.3.0)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.7.1-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->geopandas) (2019.3)\n",
      "Installing collected packages: pyproj, munch, click-plugins, cligj, fiona, shapely, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.18 geopandas-0.9.0 munch-2.5.0 pyproj-3.0.1 shapely-1.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Geocoding Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address: str, state_initials = \"\") -> list:\n",
    "    \"\"\"Uses the name of the station to get the zipcode via the Google API with state region filtering\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    address: str\n",
    "        The name of the station\n",
    "    state_initials : str\n",
    "        The 2 CHAR initials of the state to use with componenet filtering\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of Dicts:\n",
    "        A list of the different components of the address\n",
    "    \"\"\"\n",
    "    \n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/json'\n",
    "    params = {'address': address, 'key': google_api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\" + f\"&components=administrative_area:{state_initials}|country:US\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return -1\n",
    "\n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latlong_components(lat: float, long: float) -> list:\n",
    "    \"\"\"Uses the coordinates of the station to get the zipcode via the Google API\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lat: float\n",
    "        The latitude coordinate of the station\n",
    "    long: float\n",
    "        The longitude corrdinate of the station\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    List of Dicts:\n",
    "        A list of the different components of the address\n",
    "    \"\"\"\n",
    "        \n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={google_api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return -1\n",
    "   \n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zipcode(components: list) -> int:\n",
    "    \"\"\"Iterates through the address components to find the postal code\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    components: list\n",
    "        The latitude coordinate of the station\n",
    "    long: float\n",
    "        The longitude corrdinate of the station\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    int:\n",
    "        The zip code value of the address component\n",
    "    \"\"\"\n",
    "    \n",
    "    if components == -1: \n",
    "        return -1\n",
    "    \n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df, state_initials = \"\"):\n",
    "    \"\"\"Uses the df data to determine a station's zip code\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DatFrame\n",
    "        The dataframe that the function will be applied on\n",
    "    state_initials: str\n",
    "        The 2 CHAR state initial to use for component filtering\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly, the df will have a zipcode column\n",
    "    \"\"\"\n",
    "    \n",
    "    if df.end_lat < 10:   # No coordinate data means we use the address\n",
    "        return extract_zipcode(get_address_components(address = df.endname, state_initials = state_initials))\n",
    "    else:   # Use the coordinates\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_zip_entry(geodf, entries: list):\n",
    "    \"\"\"Manual inputs zip code data into the dataframe based on the entries passed\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    geodf: geopandas.geodataframe.GeoDataFrame\n",
    "        The dataframe that the function will be applied on\n",
    "    entries: list of tuples\n",
    "        A list of tuples where each tuple is in the form (stationid, zipcode)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    geopandas.geodataframe.GeoDataFrame:\n",
    "        A geodataframe with all the missing zipcodes inputted\n",
    "    \"\"\"\n",
    "    \n",
    "    geodf = geodf.set_index('endid')\n",
    "    \n",
    "    for entry in entries:\n",
    "        geodf.loc[entry[0], 'zipcode'] = entry[1]\n",
    "    \n",
    "    return geodf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Subsection Structure** - Each of the 5 following subsections in this section have the same structure\n",
    "<ol>\n",
    "    <li> Create a table in the stations schema for the specific service.\n",
    "    <li> Derive the unique list of stations from the staging table and save it as a dataframe. Stations that aren't consumer stations are dropped in the retrieval process.\n",
    "<li> Convert dataframe into a geodataframe by combining the latitude and longitude coordinates into a point geometry.\n",
    "<li> Get the zipcode data for each station using the Google Cloud Platform API. Some stations require a manual zipcode entry.\n",
    "<li> Upload the data to the table. \n",
    "<li> Add a column to the table that identifies which service the stations are associated with.\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS stations;\"\"\"\n",
    "Queries.execute_query(conn, stations_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Data/Images/ERD-Staging-Stations.png\" width=\"500\" height=\"666\" align=\"center\"/>\n",
    "<p style = \"text-align:center\"> The Entity Relationship Diagram for the 5 Station Tables in the Stations Schema </p>\n",
    "<p style = \"text-align:center;font-style:italic\"> The stationID could either be VARCHAR OR NUMERIC depending on the service </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the BayWheels Station Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.bay_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, bay_station_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Derivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_remove = ['', '449', '449.0', '420.0', '408', '408.0', '484.0', \n",
    "              '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "              'SF Depot', 'SF Depot-2 (Minnesota St Outbound)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station = Queries.get_stations(conn, 'bay', bay_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_decimal(x):\n",
    "    \"\"\"Drops the .0 from a string, if it has it\"\"\"\n",
    "    \n",
    "    if x.endswith('.0'):\n",
    "        return(x[:-2])\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations that have the same ID was a station relocation\n",
    "bay_station['endid'] = bay_station.endid.apply(drop_decimal)\n",
    "bay_station.drop_duplicates(subset=['endid'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = gpd.GeoDataFrame(bay_station, geometry=gpd.points_from_xy(bay_station.end_long, bay_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Zip Code Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [('98', 94103)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = manual_zip_entry(bay_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, bay_spatial, 'stations.bay_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'bay_station','bay', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the BlueWheels Station Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "blue_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.blue_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, blue_station_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Derivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_remove = [153, 158, 164, 223, 229, 230, 308, 382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_station = Queries.get_stations(conn, 'blue', blue_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial = gpd.GeoDataFrame(blue_station, geometry=gpd.points_from_xy(blue_station.end_long, blue_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial['zipcode'] = blue_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, blue_spatial, 'stations.blue_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'blue_station','blue', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the Capital Station Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "capital_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.capital_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, capital_station_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Derivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -1 Isn't included because -1 isn't a \"stationary\" station so when we go to calculate distance, the values won't be correct\n",
    "capital_remove = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_station = Queries.get_stations(conn, 'capital', capital_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial = gpd.GeoDataFrame(capital_station, geometry=gpd.points_from_xy(capital_station.end_long, capital_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_input_zipcode(df):\n",
    "    \"\"\"Uses the df data to determine a station's zip code for capital bike only\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DatFrame\n",
    "        The dataframe that the function will be applied on\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly, the df will have a zipcode column\n",
    "    \"\"\"\n",
    "    \n",
    "    # component filter through the three states until we find a match\n",
    "    state_initials = ['DC', 'VA', 'MD']\n",
    "    \n",
    "    if df.end_lat < 10:   # No coordinates\n",
    "        for state in state_initials:\n",
    "            zip_code = extract_zipcode(get_address_components(address = df.endname, state_initials = state))\n",
    "            if isinstance(zip_code, str):   # If get_address fails it returns -1\n",
    "                return zip_code\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial['zipcode'] = capital_spatial.apply(capital_input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, capital_spatial, 'stations.capital_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'capital_station','capital', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the CitiBike Station Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "citi_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.citi_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, citi_station_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Derivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_remove = [-1, 3036, 3650, 3247, 3248, 3446, 3480, 3488, 3633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_station = Queries.get_stations(conn, 'citi', citi_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = gpd.GeoDataFrame(citi_station, geometry=gpd.points_from_xy(citi_station.end_long, citi_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial['zipcode'] = citi_spatial.apply(input_zipcode, axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Zip Code Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [(152, 10007)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = manual_zip_entry(citi_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, citi_spatial, 'stations.citi_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'citi_station','citi', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the Divvy Station Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "divvy_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.divvy_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, divvy_station_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Station Derivation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_remove =  ['-1', '1', '360', '361', '363', '512']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_station = Queries.get_stations(conn, 'divvy', divvy_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stations that have the same ID was a station relocation\n",
    "divvy_station['endid'] = divvy_station.startid.apply(drop_decimal)\n",
    "divvy_station.drop_duplicates(subset=['endid'], keep='last' inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Geocoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = gpd.GeoDataFrame(divvy_station, geometry=gpd.points_from_xy(divvy_station.end_long, divvy_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial['zipcode'] = divvy_spatial.apply(input_zipcode, state_initials='IL',axis=1).fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual Zip Code Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_zipcodes = [\n",
    "    ('606', 60302), ('609', 60305), ('610', 60302), ('613', 60302), \n",
    "    ('614',60302), ('617', 60304), ('669', 60611) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = manual_zip_entry(divvy_spatial, manual_zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Database Upload**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, divvy_spatial, 'stations.divvy_station', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'divvy_station','divvy', schema = 'stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Construction III - Creating the Trip Tables**\n",
    "\n",
    "We will use the staging table along with the station table to process all the data into the trip table that will be used for the remainder of the project. Remember, when deriving the station tables we removed stations that aren't consumer stations, those trips are still in the staging tables and before we do anything we need to drop them. \n",
    "\n",
    "The drop lists aren't exactly the same, there are some things that shouldn't be stations AND not included in the trips. For example DIVVVY PARTS TESTING shouldn't be a station and the trips that that 'station' are involved in aren't consumer trips. Then, there are some things that shouldn't be stations, but STILL be included in the trips. For example, a blank start name (startid = -1) corresponds to either an error recording the station information or a \"floating bike\" , but it is still a valid trip. A blank name shouldn't be in the stations table, but we shouldn't remove that trip from the trip table. \n",
    "\n",
    "Once the fake trips are dropped we are going to manufacture four additonal columns:\n",
    "\n",
    "<ol>\n",
    "    <li> Duration: How long each trip took in minutes (subtract the starttime from the endtime)\n",
    "    <li> Distance (Miles): The distance of the trip (distance between the station geometries)\n",
    "    <li> Speed (MPH): How fast the rider was going (distance divided by duration)\n",
    "    <li> Bikeshare: The name of the service the trip was in (a constant value for every entry in a table)\n",
    "</ol>      \n",
    "\n",
    "<p style=\"text-align:center;font-style:italic\"> All of the functions to processes the raw data are in the Queries package </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS trips;\"\"\"\n",
    "Queries.execute_query(conn, trips_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/ERD-Stations-Trips.png\" width=\"600\" height=\"800\" align=\"center\" />\n",
    "<p style = \"text-align:center\"> The Entity Relationship Diagram for the 5 Trip Tables in the Trips Schema </p>\n",
    "<p style = \"text-align:center;font-style:italic\"> The start/endID could either be VARCHAR OR NUMERIC depending on the service </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BayWheels Trip Derivation**\n",
    "The BayWheels table doesn't fit the generic format for the Queries.delete_non_trips function and has to be hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.trip_from_staging(conn, 'bay', id_type = 'VARCHAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_bay_non_trips_query = \"\"\"\n",
    "            DELETE FROM trips.bay_trip\n",
    "            WHERE startid IN ('449', '420', '408', '484', \n",
    "                              '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "                              'SF Depot', 'SF Depot-2 (Minnesota St Outbound)'\n",
    "                          )\n",
    "            \n",
    "               OR endid IN ('449', '420', '408', '484', \n",
    "                           '16th Depot Bike Station', '16th St Depot', 'San Jose Depot', \n",
    "                           'SF Depot', 'SF Depot-2 (Minnesota St Outbound)'\n",
    "                       );\n",
    "            \"\"\"\n",
    "Queries.execute_query(conn, delete_bay_non_trips_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'bay_trip','bay', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BlueBike Trips Derviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.trip_from_staging(conn, 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'blue_trip','blue', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CapitalBike Trips Derviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.trip_from_staging(conn, 'capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'capital_trip','capital', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CitiBike Trips Derviation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.trip_from_staging(conn, 'citi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.delete_non_trips(conn, 'citi', citi_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'citi_trip','citi', schema='trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DivvyBike Trips Derviation**\n",
    "The DivvyBike, like BayWheels doesn't fit the generic format for the Queries.delete_non_trips function and has to be hardcoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.trip_from_staging(conn, 'divvy', id_type='VARCHAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_divvy_non_trips_query = \"\"\"\n",
    "            DELETE FROM trips.divvy_trip\n",
    "            WHERE startid IN ('1', '360', '361', '363', '512')\n",
    "              OR endid IN ('1', '360', '361', '363', '512');\n",
    "            \"\"\"\n",
    "Queries.execute_query(conn, delete_divvy_non_trips_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.add_bike_service_name(conn, 'divvy_trip','divvy', schema='trips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Preparing the Neighborhood Titles Tables**\n",
    "\n",
    "The neighborhood names and the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS neighborhoods;\"\"\"\n",
    "Queries.execute_query(conn, neighborhoods_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhoods I - The Neighborhood Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to Furman to get the code-names of neighborhoods\n",
    "NYCHoodUrl = 'https://furmancenter.org/neighborhoods'\n",
    "\n",
    "try:\n",
    "    r2 = requests.get(NYCHoodUrl)\n",
    "    r2.raise_for_status()\n",
    "except request.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, 'html.parser')\n",
    "\n",
    "hood_code_names = [] # list of tuples: (code, neighborhood)\n",
    "\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_code_names.append((code.text[:4], code.text[6:].replace('/','-').replace(\" \", \"_\")))\n",
    "\n",
    "hood_df = pd.DataFrame(hood_code_names, columns=['code', 'hoodname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = {\n",
    "    \"BK\": \"Brooklyn\",\n",
    "    \"BX\": \"Bronx\",\n",
    "    \"MN\": \"Manhattan\",\n",
    "    \"QN\": \"Queens\",\n",
    "    \"SI\": \"Staten\"\n",
    "}\n",
    "\n",
    "hood_df['borough'] = hood_df['code'].str[0:2].map(borough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>hoodname</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BK01</td>\n",
       "      <td>Greenpoint-Williamsburg</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BK02</td>\n",
       "      <td>Fort_Greene-Brooklyn_Heights</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BK03</td>\n",
       "      <td>Bedford_Stuyvesant</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BK04</td>\n",
       "      <td>Bushwick</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK05</td>\n",
       "      <td>East_New_York-Starrett_City</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                      hoodname   borough\n",
       "0  BK01       Greenpoint-Williamsburg  Brooklyn\n",
       "1  BK02  Fort_Greene-Brooklyn_Heights  Brooklyn\n",
       "2  BK03            Bedford_Stuyvesant  Brooklyn\n",
       "3  BK04                      Bushwick  Brooklyn\n",
       "4  BK05   East_New_York-Starrett_City  Brooklyn"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hood_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhoods II - The GeoSpatial Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofile = \"s3://williams-citibike/GeoSpatial/NYC-Neighborhoods.geojson\"\n",
    "\n",
    "with fs.open(geofile, 'rb') as file:\n",
    "    nyc_spatial = gpd.read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boro_cd</th>\n",
       "      <th>shape_area</th>\n",
       "      <th>shape_leng</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>311</td>\n",
       "      <td>103177785.365</td>\n",
       "      <td>51549.5578567</td>\n",
       "      <td>MULTIPOLYGON (((-73.97299 40.60881, -73.97259 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313</td>\n",
       "      <td>88195686.2748</td>\n",
       "      <td>65821.875577</td>\n",
       "      <td>MULTIPOLYGON (((-73.98372 40.59582, -73.98305 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>312</td>\n",
       "      <td>99525500.0655</td>\n",
       "      <td>52245.8304843</td>\n",
       "      <td>MULTIPOLYGON (((-73.97140 40.64826, -73.97121 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>206</td>\n",
       "      <td>42664311.3238</td>\n",
       "      <td>35875.7111725</td>\n",
       "      <td>MULTIPOLYGON (((-73.87185 40.84376, -73.87192 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226</td>\n",
       "      <td>50566410.6415</td>\n",
       "      <td>32820.3983295</td>\n",
       "      <td>MULTIPOLYGON (((-73.86790 40.90294, -73.86796 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  boro_cd     shape_area     shape_leng  \\\n",
       "0     311  103177785.365  51549.5578567   \n",
       "1     313  88195686.2748   65821.875577   \n",
       "2     312  99525500.0655  52245.8304843   \n",
       "3     206  42664311.3238  35875.7111725   \n",
       "4     226  50566410.6415  32820.3983295   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-73.97299 40.60881, -73.97259 ...  \n",
       "1  MULTIPOLYGON (((-73.98372 40.59582, -73.98305 ...  \n",
       "2  MULTIPOLYGON (((-73.97140 40.64826, -73.97121 ...  \n",
       "3  MULTIPOLYGON (((-73.87185 40.84376, -73.87192 ...  \n",
       "4  MULTIPOLYGON (((-73.86790 40.90294, -73.86796 ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_spatial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_num_to_abr = {\n",
    "        \"3\": \"BK\", \n",
    "        \"2\": \"BX\",\n",
    "        \"1\": \"MN\",\n",
    "        \"4\": \"QN\",\n",
    "        \"5\": \"SI\"\n",
    "        }\n",
    "\n",
    "nyc_spatial[\"boro_cd\"] = nyc_spatial[\"boro_cd\"].str[0].map(borough_num_to_abr) + nyc_spatial['boro_cd'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyc_spatial = hood_df.merge(nyc_spatial, left_on='code', right_on='boro_cd', how='left').loc[:, ['code','hoodname','borough','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>hoodname</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BK01</td>\n",
       "      <td>Greenpoint-Williamsburg</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.92406 40.71411, -73.92404 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BK02</td>\n",
       "      <td>Fort_Greene-Brooklyn_Heights</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.96929 40.70709, -73.96839 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BK03</td>\n",
       "      <td>Bedford_Stuyvesant</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.91805 40.68721, -73.91800 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BK04</td>\n",
       "      <td>Bushwick</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.89647 40.68234, -73.89653 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BK05</td>\n",
       "      <td>East_New_York-Starrett_City</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>MULTIPOLYGON (((-73.86841 40.69473, -73.86868 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   code                      hoodname   borough  \\\n",
       "0  BK01       Greenpoint-Williamsburg  Brooklyn   \n",
       "1  BK02  Fort_Greene-Brooklyn_Heights  Brooklyn   \n",
       "2  BK03            Bedford_Stuyvesant  Brooklyn   \n",
       "3  BK04                      Bushwick  Brooklyn   \n",
       "4  BK05   East_New_York-Starrett_City  Brooklyn   \n",
       "\n",
       "                                            geometry  \n",
       "0  MULTIPOLYGON (((-73.92406 40.71411, -73.92404 ...  \n",
       "1  MULTIPOLYGON (((-73.96929 40.70709, -73.96839 ...  \n",
       "2  MULTIPOLYGON (((-73.91805 40.68721, -73.91800 ...  \n",
       "3  MULTIPOLYGON (((-73.89647 40.68234, -73.89653 ...  \n",
       "4  MULTIPOLYGON (((-73.86841 40.69473, -73.86868 ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhoods III - Upload to the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nychood_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS neighborhoods.nyc_hood (\n",
    "            code CHAR(4) PRIMARY KEY,\n",
    "            hoodname VARCHAR NOT NULL,\n",
    "            borough VARCHAR(16) NOT NULL,\n",
    "            geometry GEOGRAPHY(MULTIPOLYGON,4326) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "Queries.execute_query(conn, nychood_table_query)\n",
    "Queries.upload_data(conn, nyc_spatial, 'neighborhoods.nyc_hood', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **San Francisco Neighborhoods I - The GeoSpatial Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofile = \"s3://williams-citibike/GeoSpatial/San-Francisco-Neighborhoods.geojson\"\n",
    "\n",
    "with fs.open(geofile, 'rb') as file:\n",
    "    sanfran_spatial = gpd.read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nhood</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayview Hunters Point</td>\n",
       "      <td>MULTIPOLYGON (((-122.38158 37.75307, -122.3815...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernal Heights</td>\n",
       "      <td>MULTIPOLYGON (((-122.40361 37.74934, -122.4037...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castro/Upper Market</td>\n",
       "      <td>MULTIPOLYGON (((-122.42656 37.76948, -122.4269...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chinatown</td>\n",
       "      <td>MULTIPOLYGON (((-122.40623 37.79756, -122.4055...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excelsior</td>\n",
       "      <td>MULTIPOLYGON (((-122.42398 37.73155, -122.4239...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   nhood                                           geometry\n",
       "0  Bayview Hunters Point  MULTIPOLYGON (((-122.38158 37.75307, -122.3815...\n",
       "1         Bernal Heights  MULTIPOLYGON (((-122.40361 37.74934, -122.4037...\n",
       "2    Castro/Upper Market  MULTIPOLYGON (((-122.42656 37.76948, -122.4269...\n",
       "3              Chinatown  MULTIPOLYGON (((-122.40623 37.79756, -122.4055...\n",
       "4              Excelsior  MULTIPOLYGON (((-122.42398 37.73155, -122.4239..."
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanfran_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **San Francisco Neighborhoods II - Upload to the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanfran_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS neighborhoods.sanfran_hood (\n",
    "            hoodname VARCHAR NOT NULL,\n",
    "            geometry GEOGRAPHY(MULTIPOLYGON,4326) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "Queries.execute_query(conn, sanfran_table_query)\n",
    "upload_data(conn, sanfran_spatial, 'neighborhoods.sanfran_hood', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Database Construction V - The Neighborhood Statistics Tables**\n",
    "\n",
    "#### **NYC Neighborhoods I - Lookup Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_filenames = fs.ls(\"s3://williams-citibike/HoodData/\")[1:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "lookup_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS neighborhoods.nyc_lookup (\n",
    "                    alias VARCHAR(5) PRIMARY KEY,\n",
    "                    indicator VARCHAR,\n",
    "                    description VARCHAR\n",
    "                );\n",
    "                \"\"\"\n",
    "\n",
    "Queries.execute_query(conn, lookup_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lst = [2,3,4]\n",
    "names_lst = [\"indicator_category\", \"indicator\", \"description\"]\n",
    "lookup = pd.read_excel(\"s3://\" + hood_filenames[0], sheet_name=1, usecols = cols_lst, names = names_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.sort_values(by=[\"indicator_category\",'indicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Demographics': 'DEM',\n",
    "    'Housing Market and Conditions': 'HSC',\n",
    "    'Land Use and Development': 'LUD',\n",
    "    'Neighborhood Services and Conditions': 'NSC',\n",
    "    'Renters': 'RNT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['indicator_category'] = lookup[\"indicator_category\"].map(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.rename(columns={'indicator_category':'alias'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_group_order = lookup.groupby(\"alias\").cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['alias'] = lookup['alias'] + indicator_group_order.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, lookup, 'neighborhoods.nyc_lookup', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhoods II - The Profile Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hood_data(datafile: str) -> pd.DataFrame:\n",
    "    \"\"\"Grabs the data from the s3 bucket and flattens it to a single row consisting of the neighborhood attributes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        A single row DataFrame that contains the attributes of the neighborhood\n",
    "    \"\"\"\n",
    "    cols_lst = [0,2,3,8]\n",
    "    names_lst = [\"code\", \"indicator category\", \"indicator\", \"2018\"]\n",
    "\n",
    "    # This function is a mess\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'rb') as file:\n",
    "        data = pd.read_excel(file, sheet_name=1, usecols = cols_lst, names = names_lst)\n",
    "       \n",
    "        #In the previous section we did all the alias work, now we can simply input it into the df from lookup['alias']\n",
    "        data = data.sort_values(by=['indicator category','indicator'])\n",
    "        data.insert(1, 'alias', lookup['alias'])\n",
    "        data = data.drop(columns = ['indicator category', 'indicator'])\n",
    "\n",
    "        # Prep the '2018' column so that it can used as the value argument in the pivot_table \n",
    "        data['2018'] = data['2018'].str.replace('$',\"\")\n",
    "        data['2018'] = data['2018'].str.replace(',',\"\")\n",
    "\n",
    "        # Values that are percents get turned into decimals\n",
    "        for index, value in data['2018'].items():\n",
    "            if isinstance(value,str):\n",
    "                if value[-1] == '%':\n",
    "                    data['2018'][index] = float(value.strip('%')) / 100\n",
    "\n",
    "        data['2018'] = pd.to_numeric(data['2018'])\n",
    "\n",
    "        # The pivot_table alphabatizes the columns, but we want to maintain the original order\n",
    "        column_order = ['code'] + list(data['alias'])\n",
    "\n",
    "        data = data.pivot_table(index=['code'],values='2018', columns='alias', dropna=False)\n",
    "        data = data.rename_axis(None, axis=1).reset_index()   # The pivot creates a unnecessary column axis\n",
    "        data['code'] = data['code'][0].replace(\" \",\"\")\n",
    "        data = data.reindex(column_order, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = pd.DataFrame()\n",
    "\n",
    "# This loop only works successfully if there are those specific neighborhood excel files in the HoodData folder\n",
    "for hood in hood_filenames:\n",
    "    hood_profile = hood_profile.append(flatten_hood_data(hood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.fillna(-1)   # We need to fill NaN with -1 so they can be put into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhoods III - Uploading the Profile Table into the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Module\n",
    "profile_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS neighborhoods.nyc_profile(\n",
    "                );\n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, profile_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in hood_profile.columns:\n",
    "    if name == 'code':\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE neighborhoods.nyc_profile\n",
    "                    ADD COLUMN {name} CHAR(4) PRIMARY KEY;\n",
    "                    \"\"\"\n",
    "    else:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE neighborhoods.nyc_profile\n",
    "                    ADD COLUMN {name} REAL;\n",
    "                    \"\"\"\n",
    "    \n",
    "    Queries.execute_query(conn, import_column_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, hood_profile, 'neighborhoods.nyc_profile', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **San Francisco Neighborhoods I -  Population Dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3/PyPDF2-1.26.0-py3-none-any.whl\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfile = \"s3://williams-citibike/HoodData/SX01_SanFran-Neighborhoods-Data.pdf\"\n",
    "population_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "with fs.open(pdfile, 'rb') as file:\n",
    "    pdfread = PyPDF2.PdfFileReader(file)\n",
    "    \n",
    "    for page in range(13,94,2):\n",
    "        data = pdfread.getPage(page)\n",
    "        text = data.extractText()\n",
    "\n",
    "        first_line = text.split('\\n')[0]\n",
    "        neighborhood = ''.join([i for i in first_line.split('Demographics')[0] if not i.isdigit()])\n",
    "        population = ''.join([i for i in first_line.split()[-1] if i.isdigit()])\n",
    "\n",
    "        if population == '':\n",
    "            population = ''.join([i for i in first_line.split()[3] if i.isdigit()])\n",
    "\n",
    "        population_list.append((neighborhood, population))\n",
    "\n",
    "population_df = pd.DataFrame(population_list, columns= ['hoodname', 'population'])\n",
    "population_df = population_df.astype({'population':'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hoodname</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bayview Hunters Point</td>\n",
       "      <td>37600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernal Heights</td>\n",
       "      <td>26140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castro/Upper Market</td>\n",
       "      <td>21090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chinatown</td>\n",
       "      <td>14820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Excelsior</td>\n",
       "      <td>39340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                hoodname  population\n",
       "0  Bayview Hunters Point       37600\n",
       "1         Bernal Heights       26140\n",
       "2    Castro/Upper Market       21090\n",
       "3              Chinatown       14820\n",
       "4              Excelsior       39340"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **San Francisco Neighborhoods II - Uploading the Populations into Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanfranhood_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS neighborhoods.sanfran_profile (\n",
    "            hoodname VARCHAR NOT NULL,\n",
    "            population INTEGER\n",
    "        );\n",
    "        \"\"\"\n",
    "Queries.execute_query(conn, sanfranhood_table_query)\n",
    "Queries.upload_data(conn, population_df, 'neighborhoods.sanfran_profile', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Zip Codes I - Creating the Zip Code Profile Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipcode_file = \"s3://williams-citibike/HoodData/ZX01_Zipcodes-USA.csv\"\n",
    "\n",
    "with fs.open(zipcode_file, 'r') as file:\n",
    "    zipcodes = pd.read_csv(file, sep=',', low_memory = False)\n",
    "    zipcodes = zipcodes[zipcodes.country_name == 'United States']\n",
    "    zipcodes.drop(columns = ['cities_postalcode_id', 'country_name',\n",
    "                         'area_land_sq_miles', 'area_water_sq_miles',\n",
    "                         'units_in_structure_housing_units_total_housing_units'\n",
    "                        ], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_renames = ['zipcode', 'cbsa', 'state_name', 'state_code', 'ziptype', 'land_area_sqm',\n",
    "                  'water_area_sqm', 'total_population', 'total_population_18_over',\n",
    "                  'median_age', 'pct_labor_force_16_over', \n",
    "                  'pct_labor_force_unemployed_16_over', 'pct_armed_force_16_over', \n",
    "                  'pct_labor_force_employed_16_over', 'median_household_income',\n",
    "                  'family_income_per_capita', 'total_housing_units',\n",
    "                  'pct_vacant_housing', 'occupied_housing_units',\n",
    "                  'pct_no_vehicle_of_occupied_housing', 'median_price_owner_occupied_units',\n",
    "                  'median_rent_occupied_units_paying_rent', 'pct_1_unit_attached',\n",
    "                  'pct_1_unit_detached', 'pct_2_units', 'pct_3_4_units', 'pct_5_9_units',\n",
    "                  'pct_10_19_units', 'pct_20_over_units', 'avg_household_size', 'pct_bachelors_over_25_over',\n",
    "                  'pct_diff_housing_from_last_year', 'pct_same_housing_from_last_year',\n",
    "                  'resident_since_last_year', 'population_in_college_grad', 'population_density', \n",
    "                  'pct_population_in_college_grad', 'zipcode_segment'\n",
    "                 ]\n",
    "\n",
    "zipcodes.columns = column_renames\n",
    "zipcodes.zipcode = zipcodes.zipcode.str.zfill(5)\n",
    "zipcodes.loc[:,'land_area_sqm':'pct_population_in_college_grad'] = zipcodes.loc[:,'land_area_sqm':'pct_population_in_college_grad'].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>cbsa</th>\n",
       "      <th>state_name</th>\n",
       "      <th>state_code</th>\n",
       "      <th>ziptype</th>\n",
       "      <th>land_area_sqm</th>\n",
       "      <th>water_area_sqm</th>\n",
       "      <th>total_population</th>\n",
       "      <th>total_population_18_over</th>\n",
       "      <th>median_age</th>\n",
       "      <th>...</th>\n",
       "      <th>pct_20_over_units</th>\n",
       "      <th>avg_household_size</th>\n",
       "      <th>pct_bachelors_over_25_over</th>\n",
       "      <th>pct_diff_housing_from_last_year</th>\n",
       "      <th>pct_same_housing_from_last_year</th>\n",
       "      <th>resident_since_last_year</th>\n",
       "      <th>population_in_college_grad</th>\n",
       "      <th>population_density</th>\n",
       "      <th>pct_population_in_college_grad</th>\n",
       "      <th>zipcode_segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00212</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00213</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>NH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  zipcode cbsa     state_name state_code ziptype  land_area_sqm  \\\n",
       "0   00210  NaN  New Hampshire         NH     NaN           -1.0   \n",
       "1   00211  NaN  New Hampshire         NH     NaN           -1.0   \n",
       "2   00212  NaN  New Hampshire         NH     NaN           -1.0   \n",
       "3   00213  NaN  New Hampshire         NH     NaN           -1.0   \n",
       "4   00214  NaN  New Hampshire         NH     NaN           -1.0   \n",
       "\n",
       "   water_area_sqm  total_population  total_population_18_over  median_age  \\\n",
       "0            -1.0              -1.0                      -1.0        -1.0   \n",
       "1            -1.0              -1.0                      -1.0        -1.0   \n",
       "2            -1.0              -1.0                      -1.0        -1.0   \n",
       "3            -1.0              -1.0                      -1.0        -1.0   \n",
       "4            -1.0              -1.0                      -1.0        -1.0   \n",
       "\n",
       "   ...  pct_20_over_units  avg_household_size  pct_bachelors_over_25_over  \\\n",
       "0  ...               -1.0                -1.0                        -1.0   \n",
       "1  ...               -1.0                -1.0                        -1.0   \n",
       "2  ...               -1.0                -1.0                        -1.0   \n",
       "3  ...               -1.0                -1.0                        -1.0   \n",
       "4  ...               -1.0                -1.0                        -1.0   \n",
       "\n",
       "   pct_diff_housing_from_last_year  pct_same_housing_from_last_year  \\\n",
       "0                             -1.0                             -1.0   \n",
       "1                             -1.0                             -1.0   \n",
       "2                             -1.0                             -1.0   \n",
       "3                             -1.0                             -1.0   \n",
       "4                             -1.0                             -1.0   \n",
       "\n",
       "   resident_since_last_year  population_in_college_grad  population_density  \\\n",
       "0                      -1.0                        -1.0                -1.0   \n",
       "1                      -1.0                        -1.0                -1.0   \n",
       "2                      -1.0                        -1.0                -1.0   \n",
       "3                      -1.0                        -1.0                -1.0   \n",
       "4                      -1.0                        -1.0                -1.0   \n",
       "\n",
       "   pct_population_in_college_grad  zipcode_segment  \n",
       "0                            -1.0              NaN  \n",
       "1                            -1.0              NaN  \n",
       "2                            -1.0              NaN  \n",
       "3                            -1.0              NaN  \n",
       "4                            -1.0              NaN  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Module\n",
    "profile_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS neighborhoods.zipcodes_profile(\n",
    "                );\n",
    "                \"\"\"\n",
    "Queries.execute_query(conn, profile_table_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "varchar_list = ['zipcode', 'cbsa', 'state_name', 'state_code', 'ziptype', 'zipcode_segment']\n",
    "\n",
    "for name in zipcodes.columns:\n",
    "    if name in varchar_list:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE neighborhoods.zipcodes_profile\n",
    "                    ADD COLUMN {name} VARCHAR;\n",
    "                    \"\"\"\n",
    "    else:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE neighborhoods.zipcodes_profile\n",
    "                    ADD COLUMN {name} REAL;\n",
    "                    \"\"\"\n",
    "    \n",
    "    Queries.execute_query(conn, import_column_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.upload_data(conn, zipcodes, 'neighborhoods.zipcodes_profile', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
