{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "**Dataset I - CitiBike Trip Data**\n",
    "(https://www.citibikenyc.com/system-data)\n",
    "\n",
    "The goal of this notebook is to get all the required data needed to complete the project. The first dataset that will be compiled is the Trip data from CitiBike. The trip data holds key information about each trip that was taken by customers of the service. For example, columns such as the start time, end station, and gender are recorded for each trip.\n",
    "\n",
    "**Dataset II - Neighborhood Profiles**\n",
    "(https://furmancenter.org/neighborhoods)\n",
    "\n",
    "The next dataset that is needed is the characteristics of each neighborhood in New York City (NYC). The data was gathered by the Furman Center for Real Estate and Urban Policy at New York University. Each dataset has different categories of information about each neighborhood in the city. For example two categories that exist in the dataset are demographics and housing. \n",
    "\n",
    "**Dataset III - Community District GeoJson**\n",
    "(https://data.cityofnewyork.us/City-Government/Community-Districts/yfnk-k7r4)\n",
    "\n",
    "The third data is GeoJSON data that actually segments the community districts of NYC. That data is obtained directly from NYCOpenData. *Note: What Furman Center calls Neighborhoods, NYCOpenData calls Community Districts. NYCOpenData has a different dataset called Neighborhood Tabulation Areas which is a more granular division of the city.*\n",
    "\n",
    "**Dataset IV - Subway Entrance GeoJson**\n",
    "(https://data.cityofnewyork.us/Transportation/Subway-Entrances/drex-xx56)\n",
    "\n",
    "The final dataset is another GeoJSON file that has the information on the entrances of all the subway stations in the city. Again, this data is obtained directly from NYCOpendata. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping\n",
    "The purpose of this section is to connect, extract, and store all of the tripdata files from the CitiBike S3 bucket into a temporary folder in the working directory. We will use the requests, zipfile, and io packages to retrieve the zipped data and extract it to the temporary folder. \n",
    "\n",
    "*The vision for this project is that all files needed for any analysis be stored in the cloud (AWS S3), separate from the directory of the code. In the \"Upload...\" sections we will upload the extracted data from the temporary folder to a personal S3 bucket and then delete the temporary folder. For the remainder of the project, all data will be pulled from that S3 bucket*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zipper, datafile: str, folder: str):\n",
    "    if os.path.exists(folder + datafile):\n",
    "        print(f\"Skipped: {datafile} already extracted from S3 Bucket \\n\")\n",
    "        return None\n",
    "\n",
    "    zipper.extract(datafile, path = folder)\n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the BayWheels S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAYWHEELS_DATA_FOLDER = \"https://s3.amazonaws.com/baywheels-data/\" \n",
    "MY_BAYWHEELS = os.path.join(os.getcwd(),\"BayWheelsData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BAYWHEELS):\n",
    "    os.makedirs(MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_request(bay_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(bay_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201905 the buckets changed from fordgobike -> baywheels\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(bay_bucket + filename.replace('fordgobike','baywheels'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from BayWheels S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BayWheels S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"        \n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "\n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "        unzip(zip, datafile, folder)\n",
    "        \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Success: 2017-fordgobike-tripdata.csv.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: 2017-fordgobike-tripdata.csv unzipped and uploaded to /root/Citi-Bike-Expansion/BayWheelsData \n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = bay_request(BAYWHEELS_DATA_FOLDER, f\"2017-fordgobike-tripdata.csv.zip\")\n",
    "bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = bay_request(BAYWHEELS_DATA_FOLDER, f\"{year}{month}-fordgobike-tripdata.csv.zip\")\n",
    "        bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the BlueBike S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUEBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/hubway-data/\" \n",
    "MY_BLUEBIKE = os.path.join(os.getcwd(),\"BlueBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BLUEBIKE):\n",
    "    os.makedirs(MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_request(blue_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(blue_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201805 the buckets changed from hubway -> bluebikes\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(blue_bucket + filename.replace('hubway','bluebikes'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename.replace('hubway','bluebikes')} requested from BlueBike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BlueBike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "        unzip(zip,datafile,folder)\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = ['2011','2012','2013','2014_1','2014_2']\n",
    "\n",
    "for year in yearlist:\n",
    "    r = blue_request(BLUEBIKE_DATA_FOLDER, f\"hubway_Trips_{year}.csv\")\n",
    "    url_content = r.content\n",
    "    csv_file = open(f'/root/Citi-Bike-Expansion/BlueBikeData/{year}-hubway-tripdata.csv', 'wb')\n",
    "\n",
    "    csv_file.write(url_content)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2015\",\"2016\",\"2017\",\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = blue_request(BLUEBIKE_DATA_FOLDER, f\"{year}{month}-hubway-tripdata.zip\")\n",
    "        blue_download(r, MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the Capital Bikeshare S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPITAL_DATA_FOLDER = \"https://s3.amazonaws.com/capitalbikeshare-data/\" \n",
    "MY_CAPITAL = os.path.join(os.getcwd(),\"CapitalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CAPITAL):\n",
    "    os.makedirs(MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_request(capital_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(capital_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None   \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Capital S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        if zip.namelist()[0][4] == 'Q':\n",
    "            for i in range(len(zip.namelist())):\n",
    "                datafile = zip.namelist()[i]\n",
    "                unzip(zip,datafile,folder)\n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "            unzip(zip,datafile,folder)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = [\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}-capitalbikeshare-tripdata.zip\")\n",
    "    capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}{month}-capitalbikeshare-tripdata.zip\")\n",
    "        capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the CitiBike S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io   # Needed to pull data from CitiBike S3 bucket\n",
    "import os   # Needed to work with folders that will be created\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/tripdata/\" \n",
    "MY_CITIBIKE = os.path.join(os.getcwd(),\"CitiBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CITIBIKE):\n",
    "    os.makedirs(MY_CITIBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_request(citi_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(citi_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 2017 the bucket endings changed from .zip -> .csv.zip\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(citi_bucket + filename[:-4] + '.csv.' + filename[-3:], stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from Citibike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Citibike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0] \n",
    "        unzip(zip,datafile,folder)\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2013\",\"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        try:\n",
    "            r = citi_request(CITIBIKE_DATA_FOLDER, f\"{year}{month}-citibike-tripdata.zip\")\n",
    "            citi_download(r,MY_CITIBIKE)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the TripData from the Divvy S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVVY_DATA_FOLDER = \"https://divvy-tripdata.s3.amazonaws.com/\" \n",
    "MY_DIVVY = os.path.join(os.getcwd(),\"DivvyData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_DIVVY):\n",
    "    os.makedirs(MY_DIVVY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_request(divvy_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(divvy_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None    \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BayWheels S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_download(r: requests.models.Response, folder: str, year) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        if year == '2013':\n",
    "            datafile = zip.namelist()[2]\n",
    "            unzip(zip,datafile,folder)\n",
    "\n",
    "        elif int(year) < 2018:        \n",
    "            for file in zip.namelist():\n",
    "                if re.match('(Divvy_Trips_\\d{4}.{3,4}.csv)$', file):\n",
    "                    datafile = file\n",
    "                    unzip(zip,datafile,folder)\n",
    "                elif re.match('(Divvy_Trips_\\d{4}.{3,5}.csv)$', file):\n",
    "                    datafile = file\n",
    "                    unzip(zip,datafile,folder)\n",
    "\n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "            unzip(zip,datafile,folder)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request Success: Divvy_Stations_Trips_2014_Q1Q2.zip requested from BayWheels S3 Bucket\n",
      "Extract Success: Divvy_Trips_2014_Q1Q2.csv unzipped and uploaded to /root/Citi-Bike-Expansion/DivvyData \n",
      "\n",
      "Request Success: Divvy_Stations_Trips_2014_Q3Q4.zip requested from BayWheels S3 Bucket\n"
     ]
    }
   ],
   "source": [
    "yearlist = ['2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "yearlist = ['2014']\n",
    "\n",
    "for year in yearlist:\n",
    "    if year == '2013':\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)\n",
    "    \n",
    "    elif year == '2014':\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}_{half}.zip\")\n",
    "            divvy_download(r, MY_DIVVY, year)       \n",
    "    \n",
    "    elif int(year) < 2018:\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            try:\n",
    "                # 404 Error for 2015_Q1Q2 (the only file that gets run in the except)\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}-{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "    else:\n",
    "        for quarter in ['Q1','Q2','Q3','Q4']:\n",
    "            try:\n",
    "                # 404 Errors for 2020Q2 - 2020Q4\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{quarter}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2020\"]\n",
    "monthlist = [\"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"{year}{month}-divvy-tripdata.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move one file from a subdirectory into the main DivvyData folder\n",
    "source = os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2013','Divvy_Trips_2013.csv')\n",
    "destination = os.path.join(os.getcwd(),'DivvyData')\n",
    "remove = os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the file and delete the subdirectory\n",
    "shutil.move(source,destination)\n",
    "shutil.rmtree(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data I - Getting the Neighborhood Codes\n",
    "To download the xlsx files from Furman Center we need the 4 character code for each community district. To get those values we'll use beautifulsoup to scrap the dropdown menu and store the code:name pairs of each community in a dictionary. For example, BK04: Bushwick will be an entry in the dictionary (The BK portion represents the borough Brooklyn).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_codes = {}\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_codes[code.text[:4]] = code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")   # Borough names will be used as filename in the next secion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Neighborhood Data II - Getting the Neighborhood Data Files\n",
    "With the neighborhood codes available, we can send a request to the Furman Center, download their excel files, and store it in a temporary folder. This is going to be a simpler and similar process to the tripdata files because we don't have to deal with zipped folders. Later the data will be uploaded to the S3 Bucket and then deleted from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_HOOD_FOLDER = \"/root/Citi-Bike-Expansion/TempHoodData/\"\n",
    "\n",
    "if not os.path.exists(TEMP_HOOD_FOLDER):\n",
    "    os.makedirs(TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_hood_data(code: str, name: str, folder: str) -> None:\n",
    "    \"\"\"Uses the scraped neighborhood code to download the xlsx data from Furman Center\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    code: str\n",
    "        The 4 character neighborhood string\n",
    "    name: str\n",
    "        The actual name of the neighborhood\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be an XLSX file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = f\"https://furmancenter.org/files/NDP/{code}_NeighborhoodDataProfile.xlsx\"\n",
    "    \n",
    "    if os.path.exists(folder + f\"{code}_{name}.xlsx\"):\n",
    "        print(f\"Skipped: {code}_{name} already downloaded from Furman Center\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        r3 = requests.get(file)\n",
    "        r3.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Request Success: {file} from Furman Center\")\n",
    "    \n",
    "    with open(folder + f\"{code}_{name}.xlsx\", 'wb') as output:\n",
    "        output.write(r3.content)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hood_codes.items():\n",
    "    pull_hood_data(key, value, TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload TripData to Personal S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This code can has to be executed with your own S3 bucket by changing the following string values:\n",
    "# ACCESS_KEY_ID, ACCESS_SECRET_KEY, bucket, prefix \n",
    "\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "s3 = boto3.resource(\n",
    "     's3',\n",
    "     aws_access_key_id = ACCESS_KEY_ID,\n",
    "     aws_secret_access_key = ACCESS_SECRET_KEY\n",
    ")\n",
    "\n",
    "bucket = 'williams-citibike'   # Premade bucket in S3\n",
    "trip_prefix = 'TripData'   # Premade folder inside the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_upload(directory: str, prefix: str):\n",
    "    filenames = sorted([file for file in os.listdir(directory)])\n",
    "    \n",
    "    for key in filenames:\n",
    "        s3.Bucket(bucket).Object(os.path.join(trip_prefix,prefix,key)).upload_file(os.path.join(directory,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_folders = [(MY_BAYWHEELS,'BayWheels'),\n",
    "                      (MY_BLUEBIKE,'BlueBike'),\n",
    "                      (MY_CAPITAL, 'CaptialBike'),\n",
    "                      (MY_CITIBIKE, 'CitiBike'),\n",
    "                      (MY_DIVVY, 'DivvyBike')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, prefix in local_data_folders:\n",
    "    s3_upload(directory, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Neighborhood Data to Personal S3 Bucket\n",
    "The purpose of this section is to take the downloaded files and upload them to my own personal S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_prefix = \"HoodData\"\n",
    "filenames = sorted([file for file in os.listdir(TEMP_HOOD_FOLDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(hood_prefix,key)).upload_file(TEMP_HOOD_FOLDER + key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Local Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, name in local_data_folders:\n",
    "    shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_HOOD_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
