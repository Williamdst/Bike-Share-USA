{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modifying the Database**\n",
    "In the last notebook we took all of our gathered data and built created our database from scratch. In this section we will be modifying some tables through updates and deletes to get it ready for our Exploratory Data Analytics in the following section. The two schemas that we will focus on is the Trip and Stations schema. The modifying of these schemas has to be done on a massive scale and it requires us to clean it directly in the database itself. \n",
    "\n",
    "The other neighborhood schema has small datasets  that can be cleaned locally. More so than that, it is important that these tables remain raw in the database so that we conserve the integrity of the data. If we want to clean it we have the OPTION of cleaning it how we see fit at any time as compared to finalizing the clean. Directly related to that idea, another person querying from the database won't know how we chose to clean it and we might forget ourselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Connecting to the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.8.6-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = ''\n",
    "PGDATABASE = ''\n",
    "PGUSER = ''\n",
    "PGPASSWORD = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(),'Data','Scripts'))\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Station Table Update: Station Status**\n",
    "The ecosystem of stations in a bike share service is always changing. The companies add new stations, remove stations, and occasionally moves stations to nearby locations. In this section we are going to add two new columns to each station table in the stations schema called birth and death. \n",
    "\n",
    "The birth column will represent the date of the first trip that was taken from the station. The death column represents the date of the last trip that was taken from the station. Stations are considered dead if there wasn't a trip within Decemember of 2020. **Any station that is still active will have a null value in the death column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    add_columns_query = f\"\"\"\n",
    "            ALTER TABLE stations.{service}_station\n",
    "            ADD COLUMN birth TIMESTAMP,\n",
    "            ADD COLUMN death TIMESTAMP;\n",
    "            \"\"\"\n",
    "    Queries.execute_query(conn, add_columns_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in ['blue', 'capital','citi']:\n",
    "    Queries.birth_certificate(conn, service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in ['bay','divvy']:\n",
    "    Queries.birth_certificate(conn, service, id_type='VARCHAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the stations that are still alive to have a death of null\n",
    "for service in services:\n",
    "    set_death_null_query = f\"\"\"\n",
    "            UPDATE stations.{service}_station\n",
    "            SET death = null\n",
    "            WHERE death <= '2020-12-31'\n",
    "              AND death >= '2020-12-01';\n",
    "            \"\"\"\n",
    "    Queries.execute_query(conn, set_death_null_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;font-style:italic\"> Note: Stations are derived from the trip data, therefore it is important to determine which stations exist in the system before we start modifying trip data. Even if we determine that a 12 hour trip is an outlier and remove it from the data, the station that it started from is still valid. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Trip Table Cleaning** \n",
    "### **Trip Table Cleaning I: Handling Outliers - Speed**\n",
    "In this section we are going to modify trips whose speed (MPH) are physically unlikely. Luckily, we have a reference from the different services on what a speed outlier might look like. According to them, their pedal assisted e-bikes can go up to 18 MPH. In the sport of cycling, although not on a pedal assisted bike, a reasonably experienced cyclist can reach over 19 MPH. Using a nicer whole number, we'll use 20 MPH as a conservative cutoff and anything over that will be considered an outlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of deleting the outliers, let's make an effort to conserve as much data as possible. To handle speed outliers, we are going to cap the speed at 20 MPH. Any trip that has a speed over 20 MPH we are going to adjust the trip duration in a way that when the speed is calculated it will result in 20 MPH. $$\\frac{Distance}{\\frac{Duration}{60}} = 20$$\n",
    "\n",
    "$$\\frac{60 \\times Distance}{20} = Duration$$\n",
    "\n",
    "$$3 \\times Distance = Duration$$\n",
    "\n",
    "<p style=\"text-align:center; font-style:italic\"> Note: If strapped for time, there aren't that many trips that are outliers and there will still be millions of rows. We are performing UPDATES which are the most expensive transactions in PostgreSQL and it will require a VACUUM after the fact. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = ['bay', 'blue', 'capital', 'citi', 'divvy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set speed over 20 equal to 20 and adjust the duration\n",
    "for service in services:\n",
    "    speed_outliers_query = f\"\"\"\n",
    "            UPDATE trips.{service}_trip\n",
    "               SET speed = 20,\n",
    "                   duration = distance * 3\n",
    "             WHERE speed > 20;\n",
    "             \"\"\"\n",
    "    Queries.execute_query(conn,speed_outliers_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to update the speed column on the upper end because we are fairly certain that the bikes can't go over 20 MPH, but what about speed on the lower end. Specifically the trips that have a speed of 0 MPH which is a result of a round trip. Round trips occur when a rider starts and ends at the same station, resulting in a distance of zero miles, thus a speed of zero. So how do we deal with this? Almost the same as with the upper speed value. We are going to set the speed to be the average 6 MPH and use the same formula to find the distance instead. \n",
    "$$\\frac{Distance}{\\frac{Duration}{60}} = 6$$\n",
    "\n",
    "$$Distance = \\frac{6 \\times Duration}{60}$$\n",
    "\n",
    "$$Distance = \\frac{Duration}{10}$$\n",
    "\n",
    "\n",
    "\n",
    "<p style = \"text-align:center;font-style:italic\"> Note on Average Speed: It was found by excluding the 0 MPH trips, after the 20 MPH adjustment, and was 6 MPH for every bike service </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set round trips' speed to 6 and adjust the distance\n",
    "for service in services:\n",
    "    roundtrip_outliers_query = f\"\"\"\n",
    "                UPDATE trips.{service}_trip\n",
    "                   SET speed = 6,\n",
    "                       distance = duration / 10\n",
    "                 WHERE distance = 0;\n",
    "                 \"\"\"\n",
    "    Queries.execute_query(conn, roundtrip_outliers_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trip Table Cleaning II: Removing Time Errors - Start Time After End Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytrip with a start time greater than an end time is an error and will get deleted. It is possible that the two values were just swapped, but the cost of swapping them is more expensive than the trips are worth. Additionally, it's such a small amount of data that it's better to just remove them. The operations need to swap them would be:\n",
    "- Find the incorrect values\n",
    "- Move them to a temporary table with the correct order\n",
    "- Delete them from the original table\n",
    "- Reinsert them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    delete_swap_query = f\"\"\"\n",
    "             DELETE FROM trips.{service}_trip\n",
    "             WHERE starttime >= endtime\n",
    "             \"\"\"\n",
    "    Queries.execute_query(conn, delete_swap_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Trip Table Cleaning III: Removing Outliers - Trip Duration**\n",
    "Before we remove outliers, we have to define what an outlier is. First let's get a feeling for how the trip duration values are distributed. The trip tables range from 7M to 111M trips, which means we can't query an entire table everytime we want to analyze something. For each table we will take a sample of 1M rows. Our sampling needs to fufill three criteria:\n",
    "\n",
    "<ul style=\"list-style-type:none;\" disabled>\n",
    "    <li> <input type=\"checkbox\"> The sampling needs to be random \n",
    "    <li> <input type=\"checkbox\" disabled> The sampling needs to be large enough\n",
    "    <li> <input type=\"checkbox\" disabled> The query needs to be decently fast \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Sampling Procedure**\n",
    "\n",
    "Unfortunately the ideal Bernoulli sampling in PostgresSQL meets the random condition, but is way too slow for our needs. And the System sampling, although fast enough, isn't truly random. To overcome these limitations, we will use the System sampling for speed and then make the selection of the rows random. The sampling process is as follows:\n",
    "\n",
    "<ol>\n",
    "    <li> System sample 1% of the data from a table\n",
    "    <li> Order the results of that sample randomly <code> ORDER BY RANDOM() </code>\n",
    "    <li> Take the first 20,000 rows (100,000 for CitiBike)\n",
    "    <li> Repeat the previous steps 50 times (10 for CitiBike)\n",
    "</ol> \n",
    "    \n",
    "    \n",
    "<p style=\"text-align:center;font-style:italic\"> Note: there is a chance that there are duplicate rows in the sampling. The more trips that a service has, the less likely repeats will appear. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_sample = Queries.get_random_rows(conn, 'bay', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_sample = Queries.get_random_rows(conn, 'blue', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_sample = Queries.get_random_rows(conn, 'capital', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_sample = Queries.get_random_rows(conn, 'citi', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_sample = Queries.get_random_rows(conn, 'divvy', samples = 1000000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sampling Statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bay</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>16.379889</td>\n",
       "      <td>227.036407</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.12</td>\n",
       "      <td>9.89</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>90228.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>28.460052</td>\n",
       "      <td>1113.783447</td>\n",
       "      <td>1.02</td>\n",
       "      <td>6.98</td>\n",
       "      <td>11.81</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>580877.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>20.065739</td>\n",
       "      <td>307.928467</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6.68</td>\n",
       "      <td>11.42</td>\n",
       "      <td>19.549999</td>\n",
       "      <td>139572.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citi</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>17.041601</td>\n",
       "      <td>208.746826</td>\n",
       "      <td>1.00</td>\n",
       "      <td>6.38</td>\n",
       "      <td>10.77</td>\n",
       "      <td>18.690001</td>\n",
       "      <td>107215.867188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divvy</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>21.115494</td>\n",
       "      <td>426.904938</td>\n",
       "      <td>0.02</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>226020.281250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean          std   min   25%    50%        75%  \\\n",
       "bay      1000000.0  16.379889   227.036407  0.02  6.12   9.89  15.830000   \n",
       "blue     1000000.0  28.460052  1113.783447  1.02  6.98  11.81  19.799999   \n",
       "capital  1000000.0  20.065739   307.928467  0.02  6.68  11.42  19.549999   \n",
       "citi     1000000.0  17.041601   208.746826  1.00  6.38  10.77  18.690001   \n",
       "divvy    1000000.0  21.115494   426.904938  0.02  7.00  12.00  21.000000   \n",
       "\n",
       "                   max  \n",
       "bay       90228.796875  \n",
       "blue     580877.250000  \n",
       "capital  139572.796875  \n",
       "citi     107215.867188  \n",
       "divvy    226020.281250  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([bay_sample.duration.describe(), \n",
    "              blue_sample.duration.describe(),\n",
    "              capital_sample.duration.describe(),\n",
    "              citi_sample.duration.describe(),\n",
    "              divvy_sample.duration.describe()\n",
    "             ], index = ['bay','blue','capital','citi','divvy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our duration distrubtions look similar: trip duration is drastically skewed to the right. Because of the skew our mean and standard deviations aren't reliable measures to use for outlier detection. Our standard deviations range from 2.7hrs to 11hrs, yet our median values are all under 12m and the 75th percentiles barely breach the 20m mark. \n",
    "\n",
    "It appears that quantiles are a good measure to use to determine outliers and seem to be more representative of real life. However, each different service has their own distributions so there isn't a one-size-fits-all quantile measure to use. The 99th percentile may be 66m for service A, but 160m for service B. So the question becomes: ***Is there a universal duration cutoff value that can be used for all bike share services?***\n",
    "\n",
    "To find the answer to that question, we first need to asses whether \"Bike Share Trip Duration Behavior\" is universal. ***Do people that use Bike Share services take the same lenghts of trips, regardless of the service?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Universal Bike Share Behavior - Duration Statistical Tests**\n",
    "\n",
    "When we ask the question whether riders of bike share services behave the same way, what we are asking is if the underlying distributions are the same across all the services. We are looking at one dependent variable (duration), across five different services, on distributions that aren't normal. With those three characteristics we can use a Kruskal-Wallis Test to see if the underlying distributions are the same. \n",
    "\n",
    "- The Null Hypotheses: $H_0:$ The samples all originate from the same distribution and have the same median values\n",
    "- The Alternative Hypotheses: $H_1:$ At least one sample originates from a different distribution and has a different median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kruskal-Wallis H Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KruskalResult(statistic=42476.35929180635, pvalue=0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.kruskal(bay_sample.duration, blue_sample.duration, capital_sample.duration, citi_sample.duration, divvy_sample.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:** \n",
    "With a p-value of 0 there is enough evidence, at any signficant level, to reject the null hypothesis that the different samples all originate form the same distribution and have the same median values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pairwise Mann-Whitney U Test**\n",
    "\n",
    "From the Kruskal-Wallis test, we only know that at least one sample originates from a different distribution and we don't know how bike services compare pairwise. In this section we will compare each bike share service to another on a pairwise level. The null hypothesis is the same as the Kruskal-Wallis Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayWheels Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MannwhitneyuResult(statistic=432718288960.5, pvalue=0.0),\n",
       " MannwhitneyuResult(statistic=444568916420.5, pvalue=0.0),\n",
       " MannwhitneyuResult(statistic=469393838548.0, pvalue=0.0),\n",
       " MannwhitneyuResult(statistic=427988173571.0, pvalue=0.0))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mannwhitneyu(bay_sample.duration, blue_sample.duration), \\\n",
    "stats.mannwhitneyu(bay_sample.duration, capital_sample.duration), \\\n",
    "stats.mannwhitneyu(bay_sample.duration, citi_sample.duration), \\\n",
    "stats.mannwhitneyu(bay_sample.duration, divvy_sample.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlueBike Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MannwhitneyuResult(statistic=489600684066.5, pvalue=1.962683214484485e-143),\n",
       " MannwhitneyuResult(statistic=464290382392.0, pvalue=0.0),\n",
       " MannwhitneyuResult(statistic=493846914443.0, pvalue=1.2376110963922516e-51))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mannwhitneyu(blue_sample.duration, capital_sample.duration), \\\n",
    "stats.mannwhitneyu(blue_sample.duration, citi_sample.duration), \\\n",
    "stats.mannwhitneyu(blue_sample.duration, divvy_sample.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CapitalBike Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MannwhitneyuResult(statistic=475259769860.5, pvalue=0.0),\n",
       " MannwhitneyuResult(statistic=483904096488.5, pvalue=0.0))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mannwhitneyu(capital_sample.duration, citi_sample.duration), \\\n",
    "stats.mannwhitneyu(capital_sample.duration, divvy_sample.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CitiBike Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MannwhitneyuResult(statistic=458944167547.5, pvalue=0.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mannwhitneyu(citi_sample.duration, divvy_sample.duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "Every null hypothesis got rejected and no two bike share service samples come from the same underlying distribution. However, there may be a way to come up with a \"psuedo-universal\" cutoff time that can be used to determine outliers for ALL services. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Maximum Quantile Threshold**\n",
    "\n",
    "In a prior section, the example was given: \"The 99th percentile may be 66m for service A, but 160m for service B\". If 66m is the 99th percentile for service A, then 160m is greater than the 99th percentile for service A. If we were looking to capture AT LEAST 99% of the data in both services then using 160m would satisfy that condition. \n",
    "\n",
    "We can use this idea to get a single value that would give us **AT LEAST** a certain percentile of every distribution. Which is why we call it \"psuedo-univeral\". The aim is to keep a minimum of 95% of the data, we will look for one value that would work for the 2.5th percentile and one value that would work for the 97.5th percentile. Using this method, we can get the percentiles directly from the data using <code> percentile_cont </code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentiles = pd.DataFrame(columns= ['service', 'lower', 'upper'])\n",
    "\n",
    "\n",
    "for service in services:\n",
    "    percentile_query = f\"\"\"\n",
    "        SELECT \n",
    "          '{service}' AS service, \n",
    "          PERCENTILE_CONT(0.025) WITHIN GROUP (ORDER BY duration) AS lower,\n",
    "          PERCENTILE_CONT(0.975) WITHIN GROUP (ORDER BY duration) AS upper\n",
    "        FROM trips.{service}_trip;\n",
    "        \"\"\"\n",
    "    percentiles = percentiles.append(Queries.execute_query(conn, percentile_query, to_frame=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bay</td>\n",
       "      <td>2.21</td>\n",
       "      <td>53.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blue</td>\n",
       "      <td>2.82</td>\n",
       "      <td>69.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capital</td>\n",
       "      <td>2.60</td>\n",
       "      <td>87.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>citi</td>\n",
       "      <td>2.38</td>\n",
       "      <td>44.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>divvy</td>\n",
       "      <td>2.72</td>\n",
       "      <td>72.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   service  lower  upper\n",
       "0      bay   2.21  53.32\n",
       "0     blue   2.82  69.28\n",
       "0  capital   2.60  87.30\n",
       "0     citi   2.38  44.57\n",
       "0    divvy   2.72  72.00"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.21, 87.3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers = percentiles.lower.min(), percentiles.upper.max()\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\" > What are the actual percentile values for 2.21 and 87.3 for each service? </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_percentiles = pd.DataFrame(columns = ['service','lower_min_actual', 'upper_max_actual'])\n",
    "\n",
    "for service in services:\n",
    "    percentile_actual_query = f\"\"\"\n",
    "            SELECT \n",
    "              '{service}' AS service,\n",
    "              PERCENT_RANK({percentiles.lower.min()}) WITHIN GROUP (ORDER BY duration) * 100 AS lower_min_actual,\n",
    "              PERCENT_RANK({percentiles.upper.max()}) WITHIN GROUP (ORDER BY duration) * 100 AS upper_max_actual\n",
    "            FROM trips.{service}_trip;\n",
    "            \"\"\"\n",
    "    actual_percentiles = actual_percentiles.append(Queries.execute_query(conn, percentile_actual_query, to_frame=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>service</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>lower_min_actual</th>\n",
       "      <th>upper_max_actual</th>\n",
       "      <th>percentile_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bay</td>\n",
       "      <td>2.21</td>\n",
       "      <td>53.32</td>\n",
       "      <td>2.486306</td>\n",
       "      <td>98.884815</td>\n",
       "      <td>96.398510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blue</td>\n",
       "      <td>2.82</td>\n",
       "      <td>69.28</td>\n",
       "      <td>1.076714</td>\n",
       "      <td>98.352162</td>\n",
       "      <td>97.275449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>capital</td>\n",
       "      <td>2.60</td>\n",
       "      <td>87.30</td>\n",
       "      <td>1.424255</td>\n",
       "      <td>97.499375</td>\n",
       "      <td>96.075121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>citi</td>\n",
       "      <td>2.38</td>\n",
       "      <td>44.57</td>\n",
       "      <td>1.996280</td>\n",
       "      <td>99.365758</td>\n",
       "      <td>97.369479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>divvy</td>\n",
       "      <td>2.72</td>\n",
       "      <td>72.00</td>\n",
       "      <td>1.669957</td>\n",
       "      <td>98.239076</td>\n",
       "      <td>96.569119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   service  lower  upper  lower_min_actual  upper_max_actual  percentile_range\n",
       "0      bay   2.21  53.32          2.486306         98.884815         96.398510\n",
       "1     blue   2.82  69.28          1.076714         98.352162         97.275449\n",
       "2  capital   2.60  87.30          1.424255         97.499375         96.075121\n",
       "3     citi   2.38  44.57          1.996280         99.365758         97.369479\n",
       "4    divvy   2.72  72.00          1.669957         98.239076         96.569119"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = percentiles.merge(actual_percentiles, on='service')\n",
    "merged['percentile_range'] = merged.upper_max_actual - merged.lower_min_actual\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Removing Duration Outliers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    Queries.delete_duration_outliers(conn, service, outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Full Vacuum**\n",
    "With all of the updates and deletes, let's vacuum the data. We are performing a full vacuum to clear \"dead data\" that is taking up disk space. The reason we are doing this now is because the next section requires a lot of reading from the database and we don't want to query from a bloated database. \n",
    "\n",
    "<p style=\"text-align:center;font-style:italic\"> <b> The full vacuum takes a while to finish and locks the table. I recommend running the full vacuum from the PgAdmin interface and not the cell </b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries.VACUUM_FULL(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"line-height:11px\">\n",
    "    <p style=\"text-align:right;font-style:italic;\"> Data Science = Solving Problems = Happiness </p>\n",
    "    <p style=\"text-align:right\"> Denzel S. Williams </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
