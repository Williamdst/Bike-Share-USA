{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-bc4c36f384c8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-bc4c36f384c8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    jupyter nbconvert --to pdf --TemplateExporter.exclude_input=True my_notebook.ipynb\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "jupyter nbconvert --to pdf --TemplateExporter.exclude_input=True my_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.8.6-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4abaa8314473>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import seaborn as sns\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.join(os.getcwd(),'Data','Scripts'))\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = ['bay', 'blue', 'capital', 'citi', 'divvy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"> Bike Share USA </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Introduction\n",
    "\n",
    "Every major human advancement improved how people, things, or ideas moved from one point to another. Ancient innovations such as agriculture, providing us with a surplus of food, enabling us to stop moving and build civilizations. Present day innovations such as the Internet has taken movement to all new levels. On top of the back of the internet you can order a package and have it delivered by day end. On top of the back of the internet you can send software money, permissionlessly, from one end of the world to another. Most importantly, the internet enables the global movement of information at near instant speeds. \n",
    "\n",
    "When it comes to the physical transportation of people, intra-planet space travel and self-driving cars are the talk of the town. However, the greatest macro transportation revolution is happening on a micro level.  Micromobility refers to a range of small, lightweight vehicles operating at speeds typically below 15 mph and driven by users personally. Micromobility devices include, bicycles, e-bikes, electric scooters and skateboards, shared bicycles, and electric pedal assisted bicycles [1]. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"clear: right;\">\n",
    "   <p style=\"float: right;\"> \n",
    "       <figure style=\"float:right;text-align:center\">\n",
    "           <img src=\"./Data/Images/Report/Shared-Micromobility-Graph.png\" width=\"450\" height=\"450\" />\n",
    "           <figcaption> Caption </figcaption>\n",
    "       </figure>\n",
    "   </p>\n",
    "   <p>According to the 2019 Shared Micromobility Snapshot, published by the National Association of City Transportation Officials (NACTO), the number of trips taken on shared bikes, e-bikes, and scooters was 136 million up 60% from 2018’s 84 million and 288% from 2017’s 35 million [2].  Although there is not any official data about transportation mode shifting, their survey data suggests that Micromobility might be replacing car trips. The United States has about 19,495 incorporated cities, towns, and villages and of those, 310 are considered at least medium cities with populations of 100,000 or more [3]. Looking at the NACTO map there are only about 130 cities that have micromobility services. Imagine if there were micromobility services in all 310 of those cities. Better yet, imagine every part of the United States having micromobility services and instead of a sparse map the shared micromobility map resembled a 4G LTE Coverage map. \n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"./Data/Images/Report/Shared-Micromobility-Map.png\" style=\"max-width=50%;\" />\n",
    "    <figcaption style=\"text-align:center\"> Caption </figcaption>\n",
    "<figure>\n",
    "    \n",
    "<p style=\"text-align:center;\"> <b>The goal of this project is to expand the bike sharing sector of micromobility into every zip code across the country. The question that this project is looking to answer is: How many bike share stations should be in any given zip code in the United States?</b></p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. The Data Engineering\n",
    "\n",
    "The data used to complete the project can be broken into four major groups. The first two groups were fundamental to completing the project, the last two were only required for the Exploratory Data Analytics (EDA):\n",
    "\n",
    "<p style=\"text-align:right\"> <b> A. Bike Share Trip Datasets </b> </p>\n",
    "The subset of zip codes that have bike stations are derived from the five largest bike sharing services in the US: Bay Wheels, Blue Bike, Capital Bikeshare, Citi Bike, and Divvy Bike. Each company hosts their trip data on S3 buckets for public use. \n",
    "These datasets hold key information about each trip that was taken by their customers. The rows of the datasets represent a single trip, and the columns are the properties of the trip such as the starting station and the time when the trip ended. Since the trip data has the start and end station included, the trip data was used to derive the station data.\n",
    "\n",
    "<p style=\"text-align:right\"> <b> B. Zip Code Datasets </b> </p>\n",
    "All the zip codes of the US along with the properties of the zip code. Properties such as the total population, core based statistical area type, and water area are included. \n",
    "    \n",
    "<p style=\"text-align:right\"> <b> C. Geospatial Datasets </b> </p>\n",
    "New York City (NYC) and San Francisco has geospatial boundaries of the segmented neighborhoods in them. The datasets in this group contain those geospatial multi-polygons.\n",
    "    \n",
    "<p style=\"text-align:right\"> <b> D. Neighborhood Profile Datasets </b> </p>\n",
    "The datasets in this group have the demographics of the neighborhoods within New York City and San Francisco. These demographics, when combined with the geospatial data were used to do two custom analyses in the EDA portion of the project. The analysis used both the station location point geometries and the Voronoi polygons of the station locations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"> <b> 1.1 ETL & The Database</b> </p>\n",
    "\n",
    "All together there were ten different datasets of data which summed to 68 GB of data across 350+ files. To work with this data, the best course of action was to build a database. Leveraging the Amazon Web Services (AWS) Cloud a RDS Database running PostgreSQL 12.5 was created on a db.t3.micro instance.  \n",
    "\n",
    "With the blank database created, before doing anything, it was important to think about how the data was going to be used for analytics to determine how it should be feed into the database. With that idea in mind an Entity Relationship Diagram (ERD) was created to structure the database and guide the transformation portion of the upcoming Extract Transform Load (ETL) jobs. \n",
    "\n",
    "<figure style=\"text-align:center\">\n",
    "    <img src=\"./Data/Images/Report/ERD-Final.png\" style=\"max-width:65%\" />\n",
    "    <figcaption style=\"text-align:center\"> Caption </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"> <b> 1.1.A Extract</b> </p>\n",
    "    \n",
    "The first part of the project was a series of massive ETL jobs. The data had to be extracted from websites, S3 buckets, zip folders, csv files, pdf files, excel files, and geo spatial files. As with using RDS, one goal of the project was to leverage the basic services of AWS, so the data was extracted and indirectly uploaded  to a personal S3 bucket. \n",
    "\n",
    "Only the bike share trip data and the neighborhood profile data needed to be extracted via code. The other two groups were a simple download. For each individual bike service, a custom set of functions and loops were used to create the requests for all the relevant files from the company’s S3 bucket, unzipping the zip folder, and then extracting and saving the relevant files within the folder. \n",
    "    \n",
    "The NYC neighborhood data is hosted at the <a href=\"https://furmancenter.org/neighborhoods\"> Furman Center </a>. On the website there is a dropdown menu used to navigate to the different neighborhoods where you can download the excel file. Using the BeautifulSoup package the dropdown menu was scraped to get the codes which could be used to construct the request directly to the file.\n",
    "    \n",
    "When it comes to the San Francisco data it had to be pulled out of a PDF file. The file was a report that originated from the <a href=\"https://default.sfplanning.org/publications_reports/SF_NGBD_SocioEconomic_Profiles/2012-2016_ACS_Profile_Neighborhoods_Final.pdf\"> SF Planning Department</a> and the data in the file was structured exactly the same for each neighborhood. Using the PyPDF2 package, I iterated through every relevant page of the report extracting out the lines of data that was required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"> <b> 1.1.B Transform</b> </p>\n",
    "\n",
    "After the extraction three main technologies used in the transformation phase were: Python and the Pandas package, PostgreSQL, and the Google Cloud Platform (GCP) Geocoding API. \n",
    "    \n",
    "The files were going to be sent to their appropriate staging table which meant their formats had to align with the staging table design. This required replacing missing values, reordering columns, inserting columns, changing data types, and doing string replacements for every trip file for every service. To handle this, like the extraction phase, custom functions were made to transform all the trip files within a bike service. Once transformed loading the data into the database took a handful of lines of code. \n",
    "    \n",
    "    IMPORT SAMPLE\n",
    "    \n",
    "\n",
    "Embedded within every trip is the station information for both the starting station and the ending station for the trip. With all the trip data inside the staging tables a simple DISTINCT query returns a data frame that with all the stations and the information associated with them. When pulled down from the database, most stations had latitude and longitude data that was used to create append a point geometry to the returned data frame. However, the core piece of data needed to complete the project, the zip code, didn’t exist. To get the zip codes the geographic coordinates were sent to the GCP Geocoding API and the zip code was extracted. By 2020 most stations had coordinates attached to them. If the coordinates were not available, the name of the station was sent to the GCP Geocoding API combined with Region Filtering and the zip code was extracted. If both things failed the zip code was manually found through different search methods. \n",
    "    \n",
    "The zip codes found were then appended to the data frame. With the zip codes in place the station data aligned with the database design and was loaded into the corresponding station table. Using the data in the staging and the stations schema the trip table was created within the database using a query to select the desired columns. The query also manufactured 4 artificial columns:\n",
    "\n",
    "<ol>\n",
    "    <li> Subtracting the starting time from the ending time of the trips returned the duration in minutes.\n",
    "    <li> Using the station point geometries the distance between the starting station and the ending station was calculated in miles.\n",
    "    <li> If both the duration and the distance column weren’t null, then the speed of the trip was calculated in mph.\n",
    "    <li> The name of the service for each trip. This was a constant value for every trip within a trip table. \n",
    "</ol>\n",
    "\n",
    "    IMPORT SAMPLE\n",
    "    \n",
    "The NYC Neighborhood data downloaded from the Furman Center was split across two tables. The profile table contained the actual data, and the other table was just a lookup table that had the name and the descriptions of the columns in the profile table.\n",
    "The properties of the NYC Neighborhoods are grouped into different categories, such as Demographics and Housing Market and Conditions. In the lookup table each property is given an alias based on the group that it is in and its position in the group. For example, the percentage of people born in New York State is the first property in the demographics group, so it is aliased DEM1. Additionally, the full name and description will get their own columns.\n",
    "    \n",
    "Each NYC Neighborhood Profile excel file has, as rows, the properties of the neighborhood the neighborhood. The columns of the files are different years of recorded data. In the project we only used the most recent 2018 data. Each of the 59 files had to be individually pivoted into a single row. The single rows were glued together to create one data frame that was sent to the database.\n",
    "\n",
    "<i> The zip code data did not require that many transformations before sending it to the database. The only things that were done were changing column names and filling missing values.</i>  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center;\"> <b> 1.2 Cleaning & Updating</b> </p>\n",
    "\n",
    "The stations were derived from the entire history of trips. The problem with this is that the ecosystem of stations in a bike sharing service is always changing. The services add new stations, remove stations, and occasionally move stations to nearby locations. It is important that the current state of the ecosystem be known when making predictions about zip codes. The model shouldn’t count a retired station in a zip codes station count. Using the trip table, two new columns were added to the tables in the station schema. The birth column recorded the first time a station appeared in a trip and the death column recorded the last time a station appeared in a column. From there, stations that had a death date within December 2020 were considered alive and their death dates were set to NULL. The birth and death columns unlocked the ability to look at the state of ecosystem of the services at any point in time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"> <b> 1.2.A Trip Table Cleaning - Basic Cleaning</b> </p>\n",
    "\n",
    "The first set of cleaning regarding the trip table was handling outliers in the speed column. Trips that had speed values too high had to be handled different from speed values that were zero. \n",
    "    \n",
    "According to different facts from the services, reports, and articles the maximum that a pedal assisted e-bike can achieve is 18-mph. A conservative value of 20-mph was used to define a speed outlier. Trips that had speeds over 20 mph were adjusted. The speed was set to 20 mph and the duration was adjusted since the distance isn’t variable.\n",
    "\n",
    "$$\\frac{Distance}{\\frac{Duration}{60}} = 20 \\longrightarrow \\text{Solving for Duration} \\longrightarrow Duration = 3 \\times Distance$$\n",
    "\n",
    "Round trips have a speed of 0 mph because they start and end at the same station which means their distance is 0 resulting in a speed of 0. For round trips the speed was set to the average speed of 6 mph. Using the same as before, the speed was set to a constant value and since duration is there, the distance column was updated. \n",
    "\n",
    "$$\\frac{Distance}{\\frac{Duration}{60}} = 6 \\longrightarrow \\text{Solving for Distance} \\longrightarrow Distance = \\frac{Duration}{10}$$\n",
    "\n",
    "With the speed outliers handled the next cleaning task was to remove trips where the start time was after the end time. These trips were removed because the quantity of trips that had this error did not justify the cost associated with fixing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:right\"> <b> 1.2.B Trip Table Cleaning - Duration Outliers</b> </p>\n",
    "\n",
    "The first issue when trying to clean the duration outliers is handling the million of rows that are in each of the five trip tables. Trying to determine outliers by querying the entire table is extremely inefficient and thus required a sample. However, using the built in Bernoulli sampling of PostgreSQL was just as inefficient as query the entire table. The samples not only had to be random, be the number of trips selected in the sample had to be large enough, and the query had to be decently fast. To meet all three conditions a sampling procedure was created to get a million rows from each trip table. \n",
    "\n",
    "INSERT IMAGE\n",
    "\n",
    "From the boxplot it looks like all of duration distributions are similar in the sense that they are drastically skewed to the right. Standard deviation ranges from 2.9 – 19.2 hours and the 75% quantile barely break 20m whereas the maximum values are in the thousands of minutes. Because of this skew, if we want to determine outliers the mean and standard deviation are not reliable.  With such a drastic skew, the quantiles are a significantly better measure to determine outliers as they are more representative of real life.  Each different sample has their own distributions so there isn’t a one size fits all quantile measure that can be used for all the samples. 66 mins might be the 99th percentile for Service A, but the 99th percentile for Service B may be at 160 mins. The question then becomes: Do riders that use bike sharing take the same length of trips, regardless of the service? If so, is there a universal duration time that could be used across all bike share services. \n",
    "\n",
    "Asking if riders that use bike sharing services take the same length of trips, what is really being asked is if the duration of the samples taken are all pulled from the same underlying distribution.  Comparing a single dependent variable across five different non-normal samples meet the conditions for the Kruskal-Wallis Test with the:\n",
    "\n",
    "<ul>\n",
    "    <li> Null Hypothesis $H_0$: The samples all originate from the same distribution and have the same median values.\n",
    "    <li> Alternative Hypothesis $H_1$: At least one sample originates from a different distribution and has a different median. \n",
    "</ul>\n",
    "\n",
    "The Scipy package was used to conduct this test and the p-value returned was 0, meaning there is enough evidence to reject H_0 at any significance level. The alternative hypothesis of the Kruskal-Wallis Test states that at least one sample originates from a different distribution, but it doesn’t say anything about how the samples are compare pairwise.  On a pairwise level the Mann-Whitney U Test was used to compare the samples with the sample null and alternate as the Kruskal-Wallis Test. All 10 tests resulted in a p-value of 0, meaning that riders of different bike share companies don’t ride for the same amounts of time.\n",
    "\n",
    "Although the statistical tests failed, I still wanted to find a single outlier value that could work for all the services. So what we looked for is a “Quantile Threshold”. The aim was to keep no less than 95% of all the trip data for each service. Therefore, we were looking for two values, one for the lower end and the upper end, that would return at least 95% of the data for every service. To find those values we took found the 97.5th quantile for every service and took the highest value of the group as the upper cutoff. Similarly, the 2.5th quantile was found for every service and we took the lowest of the group.   Those two values were used as a “pseudo universal” cutoff value.  For every service, trips with a duration above 87.3 minutes or below 2.21 minutes was removed from the trip table. Using those values the percentage of data kept ranged from 96 to 97.5% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
