{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Construction I - Creating the Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A - Installs, Imports, Functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.5)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Collecting botocore<1.19.53,>=1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "\u001b[31mERROR: boto3 1.16.63 has requirement botocore<1.20.0,>=1.19.63, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.223 has requirement botocore==1.19.63, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.63\n",
      "    Uninstalling botocore-1.19.63:\n",
      "      Successfully uninstalled botocore-1.19.63\n",
      "Successfully installed botocore-1.19.52\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries\n",
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data: pd.DataFrame(), table: str):\n",
    "    datastream = StringIO()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    data.to_csv(datastream, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.execute('rollback;')\n",
    "    cursor.copy_from(datastream,table,sep=',')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address, data_type='json'):\n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/{data_type}'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def get_latlong_components(lat, long, data_type = 'json'):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def extract_zipcode(components):\n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df):\n",
    "    if df.end_lat < 10:\n",
    "        return extract_zipcode(get_address_components(df.endname))\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA staging;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "        \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn,data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")\n",
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CaptialBike\") # Delete this after re-running notebook 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bikeID:gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\", usecols=list(range(0,11)))   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in citi_filenames:\n",
    "    populate_citi_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "            \n",
    "            data.fillna({'startid': -1, 'endidd':-1}, inplace=True)\n",
    "\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        \n",
    "        upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "        \n",
    "    print(f\"Finished Uploading to Divvy Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database Construction II - Creating the Station Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.8.2-py2.py3-none-any.whl (962 kB)\n",
      "Collecting fiona\n",
      "  Using cached Fiona-1.8.18-cp37-cp37m-manylinux1_x86_64.whl (14.8 MB)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.0.1)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.0.0.post1-cp37-cp37m-manylinux2010_x86_64.whl (6.4 MB)\n",
      "Collecting shapely\n",
      "  Using cached Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (2019.11.28)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (1.14.0)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (19.3.0)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.7.1-py3-none-any.whl (7.1 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (7.0)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Installing collected packages: munch, cligj, click-plugins, fiona, pyproj, shapely, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.18 geopandas-0.8.2 munch-2.5.0 pyproj-3.0.0.post1 shapely-1.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(conn, service: str, drop_indices: list=[]):\n",
    "    station_query = f\"\"\"\n",
    "            SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "              FROM staging.{service}_trip\n",
    "             ORDER BY endid;\n",
    "            \"\"\"\n",
    "    station = pd.read_sql(station_query, conn)\n",
    "    station.dropna(inplace=True)\n",
    "    \n",
    "    if len(drop_indices) > 0:\n",
    "        station = station.set_index('endid').drop(drop_indices).reset_index()\n",
    "    \n",
    "    return station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_upload(conn, geodf, table):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    stream = StringIO()\n",
    "    geodf.to_csv(stream, sep='\\t', index=False, header=False)\n",
    "    stream.seek(0)\n",
    "    \n",
    "    cursor.copy_from(stream, table, sep='\\t')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endid has more distinct values than startid\n",
    "# Tables module\n",
    "bay_station_query = \"\"\"\n",
    "        SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "          FROM staging.bay_trip \n",
    "         ORDER BY endid;\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station = pd.read_sql(bay_station_query, conn) # Expect long execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_decimal(x):\n",
    "    \"\"\"Drops the .0 from a string, if it has it\"\"\"\n",
    "    if x.endswith('.0'):\n",
    "        return(x[:-2])\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station['endid'] = bay_station.endid.apply(drop_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station.drop_duplicates(inplace=True)\n",
    "bay_station = bay_station.set_index('endid').drop(['449','420', '408','484']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = gpd.GeoDataFrame(bay_station, geometry=gpd.points_from_xy(bay_station.end_long, bay_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endid</th>\n",
       "      <th>endname</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>37.800000</td>\n",
       "      <td>-122.420000</td>\n",
       "      <td>POINT (-122.42000 37.80000)</td>\n",
       "      <td>94109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Washington St at Kearny St</td>\n",
       "      <td>37.795390</td>\n",
       "      <td>-122.404770</td>\n",
       "      <td>POINT (-122.40477 37.79539)</td>\n",
       "      <td>94108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Bryant St at 15th St</td>\n",
       "      <td>37.767100</td>\n",
       "      <td>-122.410660</td>\n",
       "      <td>POINT (-122.41066 37.76710)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>15th St at Potrero Ave</td>\n",
       "      <td>37.767080</td>\n",
       "      <td>-122.407360</td>\n",
       "      <td>POINT (-122.40736 37.76708)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>Irwin St at 8th St</td>\n",
       "      <td>37.766884</td>\n",
       "      <td>-122.399580</td>\n",
       "      <td>POINT (-122.39958 37.76688)</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>SJ-Q4</td>\n",
       "      <td>Willow St at Blewett Ave</td>\n",
       "      <td>37.309013</td>\n",
       "      <td>-121.900010</td>\n",
       "      <td>POINT (-121.90001 37.30901)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>SJ-Q5</td>\n",
       "      <td>Bird Ave at Willow St</td>\n",
       "      <td>37.311234</td>\n",
       "      <td>-121.896300</td>\n",
       "      <td>POINT (-121.89630 37.31123)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>SJ-Q8</td>\n",
       "      <td>Palm St at Willow St</td>\n",
       "      <td>37.317300</td>\n",
       "      <td>-121.884995</td>\n",
       "      <td>POINT (-121.88500 37.31730)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>SJ-Q9</td>\n",
       "      <td>Willow St at Vine St</td>\n",
       "      <td>37.318450</td>\n",
       "      <td>-121.883170</td>\n",
       "      <td>POINT (-121.88317 37.31845)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>SJ-R11</td>\n",
       "      <td>South San Jose State (7th St at Humboldt St)</td>\n",
       "      <td>37.320263</td>\n",
       "      <td>-121.870030</td>\n",
       "      <td>POINT (-121.87003 37.32026)</td>\n",
       "      <td>95112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      endid                                       endname    end_lat  \\\n",
       "0                                                          37.800000   \n",
       "1        10                    Washington St at Kearny St  37.795390   \n",
       "2       100                          Bryant St at 15th St  37.767100   \n",
       "3       101                        15th St at Potrero Ave  37.767080   \n",
       "4       102                            Irwin St at 8th St  37.766884   \n",
       "..      ...                                           ...        ...   \n",
       "946   SJ-Q4                      Willow St at Blewett Ave  37.309013   \n",
       "947   SJ-Q5                         Bird Ave at Willow St  37.311234   \n",
       "948   SJ-Q8                          Palm St at Willow St  37.317300   \n",
       "949   SJ-Q9                          Willow St at Vine St  37.318450   \n",
       "950  SJ-R11  South San Jose State (7th St at Humboldt St)  37.320263   \n",
       "\n",
       "       end_long                     geometry zipcode  \n",
       "0   -122.420000  POINT (-122.42000 37.80000)   94109  \n",
       "1   -122.404770  POINT (-122.40477 37.79539)   94108  \n",
       "2   -122.410660  POINT (-122.41066 37.76710)   94103  \n",
       "3   -122.407360  POINT (-122.40736 37.76708)   94103  \n",
       "4   -122.399580  POINT (-122.39958 37.76688)   94107  \n",
       "..          ...                          ...     ...  \n",
       "946 -121.900010  POINT (-121.90001 37.30901)   95125  \n",
       "947 -121.896300  POINT (-121.89630 37.31123)   95125  \n",
       "948 -121.884995  POINT (-121.88500 37.31730)   95110  \n",
       "949 -121.883170  POINT (-121.88317 37.31845)   95110  \n",
       "950 -121.870030  POINT (-121.87003 37.32026)   95112  \n",
       "\n",
       "[951 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bay_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS stations;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(stations_schema_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "bay_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.bay_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, bay_spatial, 'stations.bay_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_station = get_stations(conn, 'blue', [153, 158, 223, 229, 230, 308, 382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial = gpd.GeoDataFrame(blue_station, geometry=gpd.points_from_xy(blue_station.end_long, blue_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "blue_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.blue_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, blue_spatial, 'stations.blue_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_station = get_stations(conn, 'capital', [-1,0,32902])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial = gpd.GeoDataFrame(capital_station, geometry=gpd.points_from_xy(capital_station.end_long, capital_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "capital_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.capital_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, capital_spatial, 'stations.capital_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_station = get_stations(conn, 'citi', [-1,3036,3650,3247,3248,3446,3480,3488,3633])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = gpd.GeoDataFrame(citi_station, geometry=gpd.points_from_xy(citi_station.end_long, citi_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "citi_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.citi_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, citi_spatial, 'stations.citi_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_station = get_stations(conn, 'divvy', [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = gpd.GeoDataFrame(divvy_station, geometry=gpd.points_from_xy(divvy_station.end_long, divvy_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "divvy_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.divvy_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, divvy_spatial, 'stations.divvy_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Geocoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address, data_type='json'):\n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/{data_type}'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def get_latlong_components(lat, long, data_type = 'json'):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def extract_zipcode(components):\n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(row):\n",
    "    if row['end_lat'] < 10:\n",
    "        return 'address'\n",
    "        #row['zipcode'] = extract_zipcode(get_address_components(row.endname))\n",
    "    else:\n",
    "        return 'latlong'\n",
    "        #row['zipcode'] = extract_zipcode(get_latlong_components(row.end_lat, row.end_long))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = np.where(\n",
    "    bay_spatial.end_lat < 10,\n",
    "    extract_zipcode(get_address_components(bay_spatial.endname)),\n",
    "    extract_zipcode(get_latlong_components(bay_spatial.end_lat, bay_spatial.end_long))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94109\n",
      "94108\n"
     ]
    }
   ],
   "source": [
    "for index, row in bay_spatial.iloc[:2,].iterrows():\n",
    "    print(extract_zipcode(get_latlong_components(row.end_lat, row.end_long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df):\n",
    "    if df.end_lat < 10:\n",
    "        return extract_zipcode(get_address_components(df.endname))\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endid</th>\n",
       "      <th>endname</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>37.800000</td>\n",
       "      <td>-122.420000</td>\n",
       "      <td>POINT (-122.42000 37.80000)</td>\n",
       "      <td>94109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Washington St at Kearny St</td>\n",
       "      <td>37.795390</td>\n",
       "      <td>-122.404770</td>\n",
       "      <td>POINT (-122.40477 37.79539)</td>\n",
       "      <td>94108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Bryant St at 15th St</td>\n",
       "      <td>37.767100</td>\n",
       "      <td>-122.410660</td>\n",
       "      <td>POINT (-122.41066 37.76710)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>15th St at Potrero Ave</td>\n",
       "      <td>37.767080</td>\n",
       "      <td>-122.407360</td>\n",
       "      <td>POINT (-122.40736 37.76708)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>Irwin St at 8th St</td>\n",
       "      <td>37.766884</td>\n",
       "      <td>-122.399580</td>\n",
       "      <td>POINT (-122.39958 37.76688)</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>SJ-Q4</td>\n",
       "      <td>Willow St at Blewett Ave</td>\n",
       "      <td>37.309013</td>\n",
       "      <td>-121.900010</td>\n",
       "      <td>POINT (-121.90001 37.30901)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>SJ-Q5</td>\n",
       "      <td>Bird Ave at Willow St</td>\n",
       "      <td>37.311234</td>\n",
       "      <td>-121.896300</td>\n",
       "      <td>POINT (-121.89630 37.31123)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>SJ-Q8</td>\n",
       "      <td>Palm St at Willow St</td>\n",
       "      <td>37.317300</td>\n",
       "      <td>-121.884995</td>\n",
       "      <td>POINT (-121.88500 37.31730)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>SJ-Q9</td>\n",
       "      <td>Willow St at Vine St</td>\n",
       "      <td>37.318450</td>\n",
       "      <td>-121.883170</td>\n",
       "      <td>POINT (-121.88317 37.31845)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>SJ-R11</td>\n",
       "      <td>South San Jose State (7th St at Humboldt St)</td>\n",
       "      <td>37.320263</td>\n",
       "      <td>-121.870030</td>\n",
       "      <td>POINT (-121.87003 37.32026)</td>\n",
       "      <td>95112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      endid                                       endname    end_lat  \\\n",
       "0                                                          37.800000   \n",
       "1        10                    Washington St at Kearny St  37.795390   \n",
       "2       100                          Bryant St at 15th St  37.767100   \n",
       "3       101                        15th St at Potrero Ave  37.767080   \n",
       "4       102                            Irwin St at 8th St  37.766884   \n",
       "..      ...                                           ...        ...   \n",
       "946   SJ-Q4                      Willow St at Blewett Ave  37.309013   \n",
       "947   SJ-Q5                         Bird Ave at Willow St  37.311234   \n",
       "948   SJ-Q8                          Palm St at Willow St  37.317300   \n",
       "949   SJ-Q9                          Willow St at Vine St  37.318450   \n",
       "950  SJ-R11  South San Jose State (7th St at Humboldt St)  37.320263   \n",
       "\n",
       "       end_long                     geometry zipcode  \n",
       "0   -122.420000  POINT (-122.42000 37.80000)   94109  \n",
       "1   -122.404770  POINT (-122.40477 37.79539)   94108  \n",
       "2   -122.410660  POINT (-122.41066 37.76710)   94103  \n",
       "3   -122.407360  POINT (-122.40736 37.76708)   94103  \n",
       "4   -122.399580  POINT (-122.39958 37.76688)   94107  \n",
       "..          ...                          ...     ...  \n",
       "946 -121.900010  POINT (-121.90001 37.30901)   95125  \n",
       "947 -121.896300  POINT (-121.89630 37.31123)   95125  \n",
       "948 -121.884995  POINT (-121.88500 37.31730)   95110  \n",
       "949 -121.883170  POINT (-121.88317 37.31845)   95110  \n",
       "950 -121.870030  POINT (-121.87003 37.32026)   95112  \n",
       "\n",
       "[951 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bay_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
