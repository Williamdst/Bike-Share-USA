{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction I - Creating the BayWheels Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data: pd.DataFrame(), table: str):\n",
    "    datastream = StringIO()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    data.to_csv(datastream, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.execute('rollback;')\n",
    "    cursor.copy_from(datastream,table,sep=',')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA staging;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_schema_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "        \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Creating the BlueBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn,data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction III - Creating the Capital Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")\n",
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CaptialBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction IV - Creating the CitiBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bikeID:gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL,\n",
    "                   bikeID INTEGER,\n",
    "                   usertype VARCHAR(16),\n",
    "                   birthyear REAL,\n",
    "                   gender SMALLINT                \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\")   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    datastream.close()\n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "for file in citi_filenames:\n",
    "    populate_staging(file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction V - Creating the Divvy Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "            \n",
    "            data.fillna({'startid': -1, 'endidd':-1}, inplace=True)\n",
    "\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        \n",
    "        upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2013.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-07.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-0809.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2014_Q1Q2.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q1.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q2.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_07.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_08.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_09.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_04.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_05.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_06.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q1.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q3.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q1.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q2.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q3.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q1.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q2.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q3.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q1\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q2\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q3.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q4.csv\n",
      "williams-citibike/TripData/DivvyBike/Divvy_Trips_2020_Q1.csv\n"
     ]
    }
   ],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altcolumns = ['starttime', 'stoptime',\n",
    "              'from_station_id', 'from_station_name',\n",
    "              'to_station_id','to_station_name']\n",
    "\n",
    "names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "\n",
    "data = pd.read_csv('/root/Citi-Bike-Expansion/DivvyData/Divvy_Trips_2013.csv', usecols=altcolumns, parse_dates=[0,1])[altcolumns]\n",
    "data.columns = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_divvy_staging('williams-citibike/TripData/DivvyBike/Divvy_Trips_2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Creating the Citi Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "citi_trip_table_query = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS citi_trip (\n",
    "                starttime TIMESTAMP,\n",
    "                endtime TIMESTAMP,\n",
    "                tripduration NUMERIC,\n",
    "                startID NUMERIC,\n",
    "                endID NUMERIC,\n",
    "                usertype VARCHAR(16),\n",
    "                age REAL,\n",
    "                gender SMALLINT\n",
    "            ) PARTITION BY RANGE (starttime);\n",
    "            \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_trip_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partition(year: int, month: int) -> None: #Tables\n",
    "    \"\"\"Docstring \n",
    "    \n",
    "    \"\"\"\n",
    "    nxt_month = month+1\n",
    "    nxt_year = year   # Always the same as current year unless the month is December\n",
    "    \n",
    "    if month == 12:   # If Decemember sets the year-mon to January of the next year\n",
    "        nxt_month = 1\n",
    "        nxt_year = year+1\n",
    "    \n",
    "    month = str(month).zfill(2)\n",
    "    nxt_month = str(nxt_month).zfill(2)\n",
    "    \n",
    "    # Move this to the Tables module\n",
    "    # ----- This can use Queries.execute_query(conn, partition_query)\n",
    "    partition_query = f\"\"\"\n",
    "            CREATE TABLE cititrip_y{year}m{month} PARTITION OF citi_trip\n",
    "            FOR VALUES FROM ('{year}-{month}-01') TO ('{nxt_year}-{nxt_month}-01');\n",
    "            \"\"\"\n",
    "    \n",
    "    cursor.execute(\"rollback;\")\n",
    "    cursor.execute(partition_query)\n",
    "    conn.commit()\n",
    "    # --------------------------\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist13 = [2013]\n",
    "monthlist13 = [6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for year in yearlist13:\n",
    "    for month in monthlist13:\n",
    "        create_partition(year, month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist14_20 = [2014, 2015, 2016, 2017, 2018, 2019,2020]\n",
    "monthlist14_20 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "\n",
    "for year in yearlist14_20:\n",
    "    for month in monthlist14_20:\n",
    "        create_partition(year, month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the following code we will be converting the tripduration from seconds to minutes and converting the birthyear to age. On a db.t3.micro rds instance it will take 3.3hrs to execute** \n",
    "\n",
    "*Style using CSS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Tables module\n",
    "insert_query2 = \"\"\"\n",
    "        INSERT INTO citi_trip\n",
    "        SELECT DISTINCT starttime, endtime, ROUND(tripduration/60,2) as duration, startid, endid, usertype, \n",
    "               CASE WHEN birthyear > 0 THEN 2020 - birthyear\n",
    "                    ELSE birthyear\n",
    "                    END AS age,\n",
    "               gender\n",
    "          FROM staging.citi_trip\n",
    "         ORDER BY starttime, endtime;\n",
    "        \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(insert_query2)\n",
    "conn.commit()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the DISTINCT clause we are filtering out trips that are exact duplicates. The reason is that in our data, only exact duplicates are trips that were accidentally duplicated. If all the values are the same except a single value then that represents a different trip. For example, two friends may take a ride from the same stations at the same exact time but one may be male and the other may be female. \n",
    "\n",
    "*Note: It is possible in reality that two separate trips have exactly the same data. However,that would require two people of the same age and gender, starting and stoping at the same stations at the exact same time (down to the second). Additionally, getting rid of duplicates removed only 0.004% of trips. Therefore on the off chance that all 4,797 counted duplicates weren't actually duplicates in real life we removed a miniscule amount of data from our dataset*\n",
    "\n",
    "*Note 2: Our trip table doesn't include the bikeid, so there is a chance that those 4,797 duplicates aren't errors. Those people with the same age and gender, starting and stoping at the same stations at the exact same time (down to the second) might be on different bikes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Neighborhood Table I - Without the Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_code_names = []\n",
    "\n",
    "#Instead of creating a dictionary like before, we create a list of tuples so that we can make a df\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_code_names.append((code.text[:4], code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_df = pd.DataFrame(hood_code_names, columns=[\"code\", \"hoodname\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = {\n",
    "        \"BK\": \"Brooklyn\", \n",
    "        \"BX\": \"Bronx\",\n",
    "        \"MN\": \"Manhattan\",\n",
    "        \"QN\": \"Queens\",\n",
    "        \"SI\": \"Staten\"\n",
    "        }\n",
    "\n",
    "hood_df[\"borough\"] = hood_df[\"code\"].str[0:2].map(borough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Neighborhood Table II - Adding the Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geofile = \"s3://williams-citibike/Community_Districts.geojson\"\n",
    "\n",
    "with fs.open(geofile, 'rb') as file:\n",
    "    districts = gpd.read_file(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The codes from the Furman Center are exactly the same as the codes seen in the boro_cd column. However, the first number in the boro_cd acts as a category that represents the borough. The original Furman codes, seen in the hood_df, have to be reversed engineered using a maping. Once the mapping is complete, the two dataframes can be merged together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough_num_to_abr = {\n",
    "        \"3\": \"BK\", \n",
    "        \"2\": \"BX\",\n",
    "        \"1\": \"MN\",\n",
    "        \"4\": \"QN\",\n",
    "        \"5\": \"SI\"\n",
    "        }\n",
    "\n",
    "districts[\"boro_cd\"] = districts[\"boro_cd\"].str[0].map(borough_num_to_abr) + districts['boro_cd'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = districts[['boro_cd','geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial = hood_df.merge(districts, left_on='code', right_on='boro_cd', how='left').loc[:,['code', 'hoodname', 'borough', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial.sort_values(by='code', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial = gpd.GeoDataFrame(hood_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction III - Creating the Neighborhood Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "neighborhood_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS neighborhood (\n",
    "            code CHAR(4) PRIMARY KEY,\n",
    "            hoodname VARCHAR NOT NULL,\n",
    "            borough VARCHAR(16) NOT NULL,\n",
    "            geometry GEOGRAPHY(MULTIPOLYGON,4326) NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(neighborhood_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the new function\n",
    "hoodstream = StringIO()\n",
    "\n",
    "hood_spatial.to_csv(hoodstream,sep='\\t', index=False, header=False)\n",
    "hoodstream.seek(0)\n",
    "\n",
    "cursor.copy_from(hoodstream,'neighborhood',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table I - Querying from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endid has more distinct values than startid\n",
    "# Tables module\n",
    "stations_query = \"\"\"\n",
    "        SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "          FROM staging \n",
    "         ORDER BY endid;\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = pd.read_sql(stations_query, conn) # Expect long execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations.end_long, stations.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table II - SJoining the Neighborhood Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inner join will remove stations that aren't in NYC (some stations are in NJ).\n",
    "# Additionally it will remove the handful of stations that didn't have information other than the ID\n",
    "\n",
    "stations_spatial = gpd.sjoin(stations_spatial, hood_spatial, how='inner', op='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = stations_spatial[['endid','endname','code','geometry']].rename(columns={'endid':'stationID','endname':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial.name = stations_spatial.name.str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Consruction IV - Creating the Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "station_table_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS station (\n",
    "                   stationID NUMERIC PRIMARY KEY,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   code CHAR(4) NOT NULL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(station_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with function\n",
    "stationstream = StringIO()\n",
    "stations_spatial.to_csv(stationstream,sep='\\t', index=False, header=False)\n",
    "stationstream.seek(0)\n",
    "\n",
    "cursor.copy_from(stationstream,'station',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction V - Creating the Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_filenames = fs.ls(\"s3://williams-citibike/HoodData/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "lookup_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS lookup(\n",
    "                    alias VARCHAR(5) PRIMARY KEY,\n",
    "                    indicator VARCHAR,\n",
    "                    description VARCHAR\n",
    "                );\n",
    "                \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(lookup_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lst = [2,3,4]\n",
    "names_lst = [\"indicator_category\", \"indicator\", \"description\"]\n",
    "lookup = pd.read_excel(\"s3://\" + hood_filenames[0], sheet_name=1, usecols = cols_lst, names = names_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.sort_values(by=[\"indicator_category\",'indicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Demographics': 'DEM',\n",
    "    'Housing Market and Conditions': 'HSC',\n",
    "    'Land Use and Development': 'LUD',\n",
    "    'Neighborhood Services and Conditions': 'NSC',\n",
    "    'Renters': 'RNT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['indicator_category'] = lookup[\"indicator_category\"].map(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.rename(columns={'indicator_category':'alias'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_group_order = lookup.groupby(\"alias\").cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['alias'] = lookup['alias'] + indicator_group_order.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with function\n",
    "lookupstream = StringIO()\n",
    "\n",
    "lookup.to_csv(lookupstream,sep='\\t', index=False, header=False)\n",
    "lookupstream.seek(0)\n",
    "\n",
    "cursor.copy_from(lookupstream,'lookup',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neighborhood Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hooddata(datafile: str) -> pd.DataFrame:\n",
    "    \"\"\"Grabs the data from the s3 bucket and flattens it to a single row consisting of the neighborhood attributes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        A single row DataFrame that contains the attributes of the neighborhood\n",
    "    \"\"\"\n",
    "    cols_lst = [0,2,3,8]\n",
    "    names_lst = [\"code\", \"indicator category\", \"indicator\", \"2018\"]\n",
    "\n",
    "    # This function is a mess\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'rb') as file:\n",
    "        data = pd.read_excel(file, sheet_name=1, usecols = cols_lst, names = names_lst)\n",
    "       \n",
    "        #In the previous section we did all the alias work, now we can simply input it into the df from lookup['alias']\n",
    "        data = data.sort_values(by=['indicator category','indicator'])\n",
    "        data.insert(1, 'alias', lookup['alias'])\n",
    "        data = data.drop(columns = ['indicator category', 'indicator'])\n",
    "\n",
    "        # Prep the '2018' column so that it can used as the value argument in the pivot_table \n",
    "        data['2018'] = data['2018'].str.replace('$',\"\")\n",
    "        data['2018'] = data['2018'].str.replace(',',\"\")\n",
    "\n",
    "        # Values that are percents get turned into decimals\n",
    "        for index, value in data['2018'].items():\n",
    "            if isinstance(value,str):\n",
    "                if value[-1] == '%':\n",
    "                    data['2018'][index] = float(value.strip('%')) / 100\n",
    "\n",
    "        data['2018'] = pd.to_numeric(data['2018'])\n",
    "\n",
    "        # The pivot_table alphabatizes the columns, but we want to maintain the original order\n",
    "        column_order = ['code'] + list(data['alias'])\n",
    "\n",
    "        data = data.pivot_table(index=['code'],values='2018', columns='alias', dropna=False)\n",
    "        data = data.rename_axis(None, axis=1).reset_index()   # The pivot creates a unnecessary column axis\n",
    "        data['code'] = data['code'][0].replace(\" \",\"\")\n",
    "        data = data.reindex(column_order, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = pd.DataFrame()\n",
    "\n",
    "# This loop only works successfully if there are those specific neighborhood excel files in the HoodData folder\n",
    "for hood in hood_filenames:\n",
    "    hood_profile = hood_profile.append(flatten_hooddata(hood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.fillna(-1)   # We need to fill NaN with -1 so they can be put into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VI - Importing the Neighborhood Profiles into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Module\n",
    "profile_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS profile(\n",
    "                );\n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(profile_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in hood_profile.columns:\n",
    "    if name == 'code':\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} CHAR(4) PRIMARY KEY;\n",
    "                    \"\"\"\n",
    "    else:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} REAL;\n",
    "                    \"\"\"\n",
    "        \n",
    "    cursor.execute(\"rollback;\")\n",
    "    cursor.execute(import_column_query)\n",
    "    conn.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use the function\n",
    "profilestream = StringIO()\n",
    "\n",
    "hood_profile.to_csv(profilestream,sep='\\t', index=False, header=False)\n",
    "profilestream.seek(0)\n",
    "\n",
    "cursor.copy_from(profilestream,'profile',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VII - Purging the Database: Removing Trips that aren't Contained in NYC\n",
    "\n",
    "When the neighbborhood data was inner joined to the station data, the stations that were not in NYC were dropped. Although removed from the stations table, there are still trips in the trip table that have the dropped stations. In this section the goal is to remove those trips that are not fully contained within NYC. \n",
    "\n",
    "*Note: Not in NYC is defined as trip either starting or ending at a station that is not in NYC.*\n",
    "\n",
    "**Before we drop the trips that involve New Jersey (NJ), let's see how much of the market share NJ is gathering over time.**\n",
    "\n",
    "*Note: There are other important questions that could be asked about the NJ data, however, this project is focused on NYC data. For now, more complex NJ based questions are out of scope.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Queries # This is actually going to be the Analyze module in the Queries package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of trips per year\n",
    "all_trips_df = Queries.countYearlyTrips(conn)    # Query-0001 in file # How to use the context manager in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_trips_df = Queries.countYearlyNJTrips(conn)   # Query-0002 in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share = NJ_trips_df.merge(all_trips_df, on='year',suffixes=['_nj','_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share['nj_percent'] = round(market_share['trips_nj'] / market_share['trips_all'], 4)* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share # Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the NJ data\n",
    "Queries.deleteNJTrips(conn)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
