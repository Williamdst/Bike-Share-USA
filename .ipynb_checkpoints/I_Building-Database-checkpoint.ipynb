{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.7/site-packages (2.8.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction I - Creating the BayWheels Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.5)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: botocore<1.19.53,>=1.19.52 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.19.52)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data: pd.DataFrame(), table: str):\n",
    "    datastream = StringIO()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    data.to_csv(datastream, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.execute('rollback;')\n",
    "    cursor.copy_from(datastream,table,sep=',')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA staging;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_schema_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "        \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/2017-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201801-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201802-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201803-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201804-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201805-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201806-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201807-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201808-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201809-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201810-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201811-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201812-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201901-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201902-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201903-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201904-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201905-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201906-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201907-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201908-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201909-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201910-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201911-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201912-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202001-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202002-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202003-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202004-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202005-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202006-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202007-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202008-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202009-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202010-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202011-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202012-baywheels-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Creating the BlueBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn,data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201501-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201502-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201503-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201504-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201505-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201506-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201507-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201508-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201509-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201510-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201511-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201512-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201601-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201602-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201603-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201604-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201605-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201606-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201607-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201608-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201609-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201610-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201611-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201612-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201701-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201702-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201703-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201704-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201705-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201706-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201707-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201708-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201709-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201710-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201711-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201712-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201801_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201802_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201803_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201804-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201805-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201806-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201807-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201808-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201809-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201810-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201811-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201812-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201901-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201902-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201903-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201904-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201905-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201906-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201907-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201908-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201909-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201910-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201911-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201912-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202001-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202002-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202003-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202004-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202005-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202006-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202007-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202008-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202009-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202010-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202011-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202012-bluebikes-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction III - Creating the Capital Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")\n",
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CaptialBike\") # Delete this after re-running notebook 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2012Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2012Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2012Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2012Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2013Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2013Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2013Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2013Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2014Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2014Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2014Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2014Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2015Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2015Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2015Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2015Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2016Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2016Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2016Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2016Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2017Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2017Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2017Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/2017Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201801_capitalbikeshare_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201802-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201803-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201804-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201805-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201806-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201807-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201808-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201809-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201810-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201811-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201812-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201901-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201902-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201903-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201904-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201905-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201906-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201907-capitalbikeshare-tripdata\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201908-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201909-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201910-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201911-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/201912-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202001-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202002-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202003-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202004-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202005-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202006-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202007-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202008-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202009-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CaptialBike/202012-capitalbikeshare-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction IV - Creating the CitiBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bikeID:gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\", usecols=list(range(0,11)))   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-09 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-10 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-11 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-12 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201306-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-01 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-02 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-03 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-04 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-05 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-06 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201409-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201410-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201411-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201412-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201501-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201502-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201503-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201504-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201505-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201506-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201507-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201508-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201509-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201510-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201511-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201512-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201601-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201602-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201603-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201604-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201605-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201606-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201607-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201608-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201609-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201610-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201611-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201612-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201701-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201702-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201703-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201704-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201705-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201706-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201707-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201708-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201709-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201710-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201711-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201712-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201801-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201802-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201803-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201804-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201805-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201806-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201807-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201808-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201809-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201810-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201811-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201812-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201901-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201902-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201903-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201904-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201905-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201906-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201907-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201908-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201909-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201910-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201911-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201912-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202001-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202002-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202003-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202004-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202005-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202006-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202007-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202008-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202009-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202010-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202011-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202012-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in citi_filenames:\n",
    "    populate_citi_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction V - Creating the Divvy Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "            \n",
    "            data.fillna({'startid': -1, 'endidd':-1}, inplace=True)\n",
    "\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        \n",
    "        upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "        \n",
    "    print(f\"Finished Uploading to Divvy Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202004-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202005-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202006-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202007-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202008-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202009-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202010-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202011-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2013.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-07.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-0809.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014_Q1Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_07.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_08.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_09.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_04.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_05.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_06.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q1\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q2\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2020_Q1.csv\n"
     ]
    }
   ],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table I - Querying from the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endid has more distinct values than startid\n",
    "# Tables module\n",
    "stations_query = \"\"\"\n",
    "        SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "          FROM staging.bay_trip \n",
    "         ORDER BY endid;\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations = pd.read_sql(stations_query, conn) # Expect long execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = gpd.GeoDataFrame(stations, geometry=gpd.points_from_xy(stations.end_long, stations.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Station Table II - SJoining the Neighborhood Spatial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial = stations_spatial[['endid','endname','code','geometry']].rename(columns={'endid':'stationID','endname':'name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial.name = stations_spatial.name.str.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_spatial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Consruction IV - Creating the Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "station_table_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS station (\n",
    "                   stationID NUMERIC PRIMARY KEY,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   code CHAR(4) NOT NULL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(station_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with function\n",
    "stationstream = StringIO()\n",
    "stations_spatial.to_csv(stationstream,sep='\\t', index=False, header=False)\n",
    "stationstream.seek(0)\n",
    "\n",
    "cursor.copy_from(stationstream,'station',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction V - Creating the Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_filenames = fs.ls(\"s3://williams-citibike/HoodData/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "lookup_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS lookup(\n",
    "                    alias VARCHAR(5) PRIMARY KEY,\n",
    "                    indicator VARCHAR,\n",
    "                    description VARCHAR\n",
    "                );\n",
    "                \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(lookup_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_lst = [2,3,4]\n",
    "names_lst = [\"indicator_category\", \"indicator\", \"description\"]\n",
    "lookup = pd.read_excel(\"s3://\" + hood_filenames[0], sheet_name=1, usecols = cols_lst, names = names_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.sort_values(by=[\"indicator_category\",'indicator'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias = {\n",
    "    'Demographics': 'DEM',\n",
    "    'Housing Market and Conditions': 'HSC',\n",
    "    'Land Use and Development': 'LUD',\n",
    "    'Neighborhood Services and Conditions': 'NSC',\n",
    "    'Renters': 'RNT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['indicator_category'] = lookup[\"indicator_category\"].map(alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = lookup.rename(columns={'indicator_category':'alias'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_group_order = lookup.groupby(\"alias\").cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['alias'] = lookup['alias'] + indicator_group_order.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with function\n",
    "lookupstream = StringIO()\n",
    "\n",
    "lookup.to_csv(lookupstream,sep='\\t', index=False, header=False)\n",
    "lookupstream.seek(0)\n",
    "\n",
    "cursor.copy_from(lookupstream,'lookup',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neighborhood Profile Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hooddata(datafile: str) -> pd.DataFrame:\n",
    "    \"\"\"Grabs the data from the s3 bucket and flattens it to a single row consisting of the neighborhood attributes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        A single row DataFrame that contains the attributes of the neighborhood\n",
    "    \"\"\"\n",
    "    cols_lst = [0,2,3,8]\n",
    "    names_lst = [\"code\", \"indicator category\", \"indicator\", \"2018\"]\n",
    "\n",
    "    # This function is a mess\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'rb') as file:\n",
    "        data = pd.read_excel(file, sheet_name=1, usecols = cols_lst, names = names_lst)\n",
    "       \n",
    "        #In the previous section we did all the alias work, now we can simply input it into the df from lookup['alias']\n",
    "        data = data.sort_values(by=['indicator category','indicator'])\n",
    "        data.insert(1, 'alias', lookup['alias'])\n",
    "        data = data.drop(columns = ['indicator category', 'indicator'])\n",
    "\n",
    "        # Prep the '2018' column so that it can used as the value argument in the pivot_table \n",
    "        data['2018'] = data['2018'].str.replace('$',\"\")\n",
    "        data['2018'] = data['2018'].str.replace(',',\"\")\n",
    "\n",
    "        # Values that are percents get turned into decimals\n",
    "        for index, value in data['2018'].items():\n",
    "            if isinstance(value,str):\n",
    "                if value[-1] == '%':\n",
    "                    data['2018'][index] = float(value.strip('%')) / 100\n",
    "\n",
    "        data['2018'] = pd.to_numeric(data['2018'])\n",
    "\n",
    "        # The pivot_table alphabatizes the columns, but we want to maintain the original order\n",
    "        column_order = ['code'] + list(data['alias'])\n",
    "\n",
    "        data = data.pivot_table(index=['code'],values='2018', columns='alias', dropna=False)\n",
    "        data = data.rename_axis(None, axis=1).reset_index()   # The pivot creates a unnecessary column axis\n",
    "        data['code'] = data['code'][0].replace(\" \",\"\")\n",
    "        data = data.reindex(column_order, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = pd.DataFrame()\n",
    "\n",
    "# This loop only works successfully if there are those specific neighborhood excel files in the HoodData folder\n",
    "for hood in hood_filenames:\n",
    "    hood_profile = hood_profile.append(flatten_hooddata(hood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile = hood_profile.fillna(-1)   # We need to fill NaN with -1 so they can be put into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VI - Importing the Neighborhood Profiles into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables Module\n",
    "profile_table_query = \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS profile(\n",
    "                );\n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(profile_table_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in hood_profile.columns:\n",
    "    if name == 'code':\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} CHAR(4) PRIMARY KEY;\n",
    "                    \"\"\"\n",
    "    else:\n",
    "        import_column_query = f\"\"\"\n",
    "                    ALTER TABLE profile\n",
    "                    ADD COLUMN {name} REAL;\n",
    "                    \"\"\"\n",
    "        \n",
    "    cursor.execute(\"rollback;\")\n",
    "    cursor.execute(import_column_query)\n",
    "    conn.commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use the function\n",
    "profilestream = StringIO()\n",
    "\n",
    "hood_profile.to_csv(profilestream,sep='\\t', index=False, header=False)\n",
    "profilestream.seek(0)\n",
    "\n",
    "cursor.copy_from(profilestream,'profile',sep='\\t')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction VII - Purging the Database: Removing Trips that aren't Contained in NYC\n",
    "\n",
    "When the neighbborhood data was inner joined to the station data, the stations that were not in NYC were dropped. Although removed from the stations table, there are still trips in the trip table that have the dropped stations. In this section the goal is to remove those trips that are not fully contained within NYC. \n",
    "\n",
    "*Note: Not in NYC is defined as trip either starting or ending at a station that is not in NYC.*\n",
    "\n",
    "**Before we drop the trips that involve New Jersey (NJ), let's see how much of the market share NJ is gathering over time.**\n",
    "\n",
    "*Note: There are other important questions that could be asked about the NJ data, however, this project is focused on NYC data. For now, more complex NJ based questions are out of scope.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Queries # This is actually going to be the Analyze module in the Queries package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts the number of trips per year\n",
    "all_trips_df = Queries.countYearlyTrips(conn)    # Query-0001 in file # How to use the context manager in the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NJ_trips_df = Queries.countYearlyNJTrips(conn)   # Query-0002 in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share = NJ_trips_df.merge(all_trips_df, on='year',suffixes=['_nj','_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share['nj_percent'] = round(market_share['trips_nj'] / market_share['trips_all'], 4)* 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_share # Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the NJ data\n",
    "Queries.deleteNJTrips(conn)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
