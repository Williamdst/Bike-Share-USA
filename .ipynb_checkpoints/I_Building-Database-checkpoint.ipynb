{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Data/Images/DatabaseDiagramW.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: If you cannot see the label names try editing the markdown code (double click diagram) and change the src from DatabaseDiagramW.png to DatabaseDiagramB.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.8.6-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction I - Creating the Staging Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A - Installs, Imports, Functions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: fsspec>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from s3fs) (0.8.5)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from s3fs) (1.2.1)\n",
      "Collecting botocore<1.19.53,>=1.19.52\n",
      "  Using cached botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (3.7.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Requirement already satisfied: typing_extensions>=3.7 in /opt/conda/lib/python3.7/site-packages (from aioitertools>=0.5.1->aiobotocore>=1.0.1->s3fs) (3.7.4.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (5.1.0)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (1.6.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.19.53,>=1.19.52->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "\u001b[31mERROR: boto3 1.17.12 has requirement botocore<1.21.0,>=1.20.12, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.19.12 has requirement botocore==1.20.12, but you'll have botocore 1.19.52 which is incompatible.\u001b[0m\n",
      "Installing collected packages: botocore\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.12\n",
      "    Uninstalling botocore-1.20.12:\n",
      "      Successfully uninstalled botocore-1.20.12\n",
      "Successfully installed botocore-1.19.52\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO\n",
    "import Queries\n",
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The S3 Bucket that will be used to store the data should be created beforehand\n",
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(conn, data: pd.DataFrame(), table: str):\n",
    "    datastream = StringIO()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    data.to_csv(datastream, index=False, header=False)\n",
    "    datastream.seek(0)\n",
    "    \n",
    "    cursor.execute('rollback;')\n",
    "    cursor.copy_from(datastream,table,sep=',')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address, data_type='json'):\n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/{data_type}'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_latlong_components(lat, long, data_type = 'json'):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "   \n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def extract_zipcode(components):\n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df):\n",
    "    if df.end_lat < 10:\n",
    "        return extract_zipcode(get_address_components(df.endname))\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS staging;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(staging_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_filenames = fs.ls(\"s3://williams-citibike/TripData/BayWheels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "bay_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.bay_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID VARCHAR,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID VARCHAR,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_bay_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    columns = ['start_time','end_time',\n",
    "               'start_station_id', 'start_station_name', \n",
    "               'start_station_latitude', 'start_station_longitude', \n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_station_latitude', 'end_station_longitude']\n",
    "\n",
    "\n",
    "    altcols = ['started_at','ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "        \n",
    "    na_fills = {'start_lat': -1,'start_lng': -1,\n",
    "               'end_lat': -1, 'end_lng': -1}\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols = columns, na_values=\"\")[columns]\n",
    "        except:    \n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols = altcols, na_values=\"\")[altcols]\n",
    "            data.fillna(value=na_fills, inplace=True)\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn, data, 'staging.bay_trip')\n",
    "\n",
    "    print(f\"Finished Uploading to Bay Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/2017-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201801-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201802-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201803-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201804-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201805-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201806-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201807-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201808-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201809-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201810-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201811-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201812-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201901-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201902-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201903-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201904-fordgobike-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201905-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201906-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201907-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201908-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201909-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201910-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201911-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/201912-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202001-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202002-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202003-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202004-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202005-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202006-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202007-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202008-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202009-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202010-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202011-baywheels-tripdata.csv\n",
      "Finished Uploading to Bay Staging Table: williams-citibike/TripData/BayWheels/202012-baywheels-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in bay_filenames:\n",
    "    populate_bay_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_filenames = fs.ls(\"s3://williams-citibike/TripData/BlueBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "blue_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.blue_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_blue_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "      \n",
    "    columns = ['starttime','stoptime',\n",
    "               'start station id', 'start station name',\n",
    "               'start station latitude', 'start station longitude',\n",
    "               'end station id', 'end station name',\n",
    "               'end station latitude', 'end station longitude']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        upload_data(conn,data,'staging.blue_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201501-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201502-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201503-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201504-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201505-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201506-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201507-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201508-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201509-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201510-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201511-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201512-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201601-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201602-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201603-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201604-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201605-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201606-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201607-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201608-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201609-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201610-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201611-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201612-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201701-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201702-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201703-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201704-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201705-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201706-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201707-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201708-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201709-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201710-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201711-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201712-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201801_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201802_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201803_hubway_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201804-hubway-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201805-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201806-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201807-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201808-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201809-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201810-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201811-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201812-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201901-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201902-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201903-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201904-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201905-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201906-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201907-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201908-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201909-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201910-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201911-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/201912-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202001-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202002-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202003-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202004-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202005-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202006-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202007-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202008-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202009-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202010-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202011-bluebikes-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/BlueBike/202012-bluebikes-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "# Data starts from 2015, any data before data doesn't have location data\n",
    "for file in blue_filenames[5:]:\n",
    "    populate_blue_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_filenames = fs.ls(\"s3://williams-citibike/TripData/CapitalBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "capital_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.capital_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_capital_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['Start date', 'End date',\n",
    "               'Start station number', 'Start station',\n",
    "               'End station number', 'End station']\n",
    "    \n",
    "    altcolumns = ['started_at','ended_at',\n",
    "                  'start_station_id', 'start_station_name',\n",
    "                  'start_lat', 'start_lng',\n",
    "                  'end_station_id', 'end_station_name',\n",
    "                  'end_lat', 'end_lng']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:   \n",
    "            data = pd.read_csv(file, usecols=columns, na_values = \"\")[columns]\n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "        except:\n",
    "            file.seek(0)\n",
    "            data = pd.read_csv(file, usecols=altcolumns, na_values = \"\")[altcolumns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "\n",
    "        upload_data(conn,data,'staging.capital_trip')\n",
    "    \n",
    "    print(f\"Finished Uploading to Blue Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2012Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2013Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2014Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2015Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2016Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q1-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q2-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q3-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/2017Q4-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201801_capitalbikeshare_tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201802-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201803-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201804-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201805-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201806-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201807-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201808-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201809-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201810-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201811-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201812-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201901-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201902-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201903-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201904-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201905-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201906-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201907-capitalbikeshare-tripdata\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201908-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201909-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201910-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201911-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/201912-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202001-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202002-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202003-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202004-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202005-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202006-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202007-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202008-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202009-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202010-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202011-capitalbikeshare-tripdata.csv\n",
      "Finished Uploading to Blue Staging Table: williams-citibike/TripData/CapitalBike/202012-capitalbikeshare-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in capital_filenames:\n",
    "    populate_capital_staging(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-770f8b06c80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mciti_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://williams-citibike/TripData/CitiBike\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fs' is not defined"
     ]
    }
   ],
   "source": [
    "citi_filenames = fs.ls(\"s3://williams-citibike/TripData/CitiBike\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get rid of bikeID:gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "citi_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.citi_trip (\n",
    "                   tripduration NUMERIC, \n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL              \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_citi_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "       \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\", usecols=list(range(0,11)))   # Can't use the C engine to speed this up\n",
    "        data.fillna(-1, inplace=True)   # Empty spaces need to be integers for birthyear REAL type in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        upload_data(conn,data,'staging.citi_trip')\n",
    "        \n",
    "    print(f\"Finished Uploading to Citi Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-09 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-10 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-11 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2013-12 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201306-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-01 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-02 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-03 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-04 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-05 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-06 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/2014-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201409-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201410-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201411-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201412-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201501-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201502-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201503-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201504-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201505-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201506-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201507-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201508-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201509-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201510-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201511-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201512-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201601-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201602-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201603-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201604-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201605-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201606-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201607-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201608-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201609-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201610-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201611-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201612-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201701-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201702-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201703-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201704-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201705-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201706-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201707-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201708-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201709-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201710-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201711-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201712-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201801-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201802-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201803-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201804-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201805-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201806-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201807-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201808-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201809-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201810-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201811-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201812-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201901-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201902-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201903-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201904-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201905-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201906-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201907-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201908-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201909-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201910-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201911-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/201912-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202001-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202002-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202003-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202004-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202005-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202006-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202007-citibike-tripdata.csv\n",
      "Finished Uploading to Citi Staging Table: williams-citibike/TripData/CitiBike/202008-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "for file in citi_filenames:\n",
    "    populate_citi_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_filenames = fs.ls(\"s3://williams-citibike/TripData/DivvyBike\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAbles module. One function for all the tables. \n",
    "divvy_staging_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS staging.divvy_trip (\n",
    "                   starttime TIMESTAMP,\n",
    "                   endtime TIMESTAMP,\n",
    "                   startID NUMERIC,\n",
    "                   startname VARCHAR(128),\n",
    "                   start_lat REAL,\n",
    "                   start_long REAL,\n",
    "                   endID NUMERIC,\n",
    "                   endname VARCHAR(128),\n",
    "                   end_lat REAL,\n",
    "                   end_long REAL             \n",
    "              );\n",
    "              \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_staging_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_divvy_staging(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    columns = ['started_at', 'ended_at',\n",
    "               'start_station_id', 'start_station_name',\n",
    "               'start_lat', 'start_lng',\n",
    "               'end_station_id', 'end_station_name',\n",
    "               'end_lat', 'end_lng']\n",
    "    \n",
    "    altcolumns = ['starttime', 'stoptime',\n",
    "                  'from_station_id', 'from_station_name',\n",
    "                  'to_station_id','to_station_name']\n",
    "    \n",
    "    alt3 = ['start_time', 'end_time',\n",
    "            'from_station_id', 'from_station_name',\n",
    "            'to_station_id','to_station_name']\n",
    "    \n",
    "    names = ['starttime', 'endtime','startid','startname','endid','endname']\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        try:\n",
    "            data = pd.read_csv(file, usecols=columns, na_values=\"\", parse_dates=[0,1])[columns]\n",
    "            data.fillna({'start_station_id': -1, 'end_station_id':-1, \n",
    "                         'start_lat': -1, 'start_lng': -1,\n",
    "                         'end_lat': -1, 'end_lng': -1}, inplace=True)            \n",
    "        except ValueError:\n",
    "            file.seek(0)\n",
    "            try:\n",
    "                data = pd.read_csv(file, usecols=altcolumns, na_values = \"\", parse_dates=[0,1])[altcolumns]\n",
    "                data.columns = names\n",
    "            except ValueError:\n",
    "                file.seek(0)\n",
    "                try:\n",
    "                    data = pd.read_csv(file, usecols=alt3, na_values = \"\", parse_dates=[0,1])[alt3]\n",
    "                    data.columns = names\n",
    "                except:\n",
    "                    file.seek(0)\n",
    "                    data = pd.read_csv(file, usecols=[1,2,5,6,7,8], na_values=\"\", parse_dates=[0,1])\n",
    "                    data.columns = names\n",
    "        \n",
    "            data.insert(4,'start_lat', -1)\n",
    "            data.insert(5,'start_lng',-1)\n",
    "\n",
    "            data.insert(8,'end_lat', -1)\n",
    "            data.insert(9,'end_lng',-1)\n",
    "            \n",
    "            data.fillna({'startid': -1, 'endidd':-1}, inplace=True)\n",
    "\n",
    "        data.iloc[:, 3] = data.iloc[:, 3].str.replace(',','_')\n",
    "        data.iloc[:, 7] = data.iloc[:, 7].str.replace(',','_')\n",
    "        \n",
    "        \n",
    "        upload_data(conn,data,'staging.divvy_trip')\n",
    "        \n",
    "        \n",
    "    print(f\"Finished Uploading to Divvy Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202004-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202005-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202006-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202007-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202008-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202009-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202010-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/202011-divvy-tripdata.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2013.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-07.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q3-0809.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014-Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2014_Q1Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015-Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_07.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_08.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_09.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2015_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_04.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_05.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_06.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2016_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2017_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q1.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q2.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2018_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q1\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q2\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q3.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2019_Q4.csv\n",
      "Finished Uploading to Divvy Staging Table: williams-citibike/TripData/DivvyBike/Divvy_Trips_2020_Q1.csv\n"
     ]
    }
   ],
   "source": [
    "for file in divvy_filenames:\n",
    "    populate_divvy_staging(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Construction II - Creating the Station Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geopandas\n",
      "  Using cached geopandas-0.8.2-py2.py3-none-any.whl (962 kB)\n",
      "Collecting pyproj>=2.2.0\n",
      "  Using cached pyproj-3.0.0.post1-cp37-cp37m-manylinux2010_x86_64.whl (6.4 MB)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from geopandas) (1.0.1)\n",
      "Collecting fiona\n",
      "  Using cached Fiona-1.8.18-cp37-cp37m-manylinux1_x86_64.whl (14.8 MB)\n",
      "Collecting shapely\n",
      "  Using cached Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from pyproj>=2.2.0->geopandas) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.23.0->geopandas) (1.18.1)\n",
      "Collecting cligj>=0.5\n",
      "  Using cached cligj-0.7.1-py3-none-any.whl (7.1 kB)\n",
      "Collecting munch\n",
      "  Using cached munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting click-plugins>=1.0\n",
      "  Using cached click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
      "Requirement already satisfied: click<8,>=4.0 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (7.0)\n",
      "Requirement already satisfied: attrs>=17 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (19.3.0)\n",
      "Requirement already satisfied: six>=1.7 in /opt/conda/lib/python3.7/site-packages (from fiona->geopandas) (1.14.0)\n",
      "Installing collected packages: pyproj, cligj, munch, click-plugins, fiona, shapely, geopandas\n",
      "Successfully installed click-plugins-1.1.1 cligj-0.7.1 fiona-1.8.18 geopandas-0.8.2 munch-2.5.0 pyproj-3.0.0.post1 shapely-1.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stations(conn, service: str, drop_indices: list=[]):\n",
    "    station_query = f\"\"\"\n",
    "            SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "              FROM staging.{service}_trip\n",
    "             ORDER BY endid;\n",
    "            \"\"\"\n",
    "    station = pd.read_sql(station_query, conn)\n",
    "    station.dropna(inplace=True)\n",
    "    \n",
    "    if len(drop_indices) > 0:\n",
    "        station = station.set_index('endid').drop(drop_indices).reset_index()\n",
    "    \n",
    "    return station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_upload(conn, geodf, table):\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    stream = StringIO()\n",
    "    geodf.to_csv(stream, sep='\\t', index=False, header=False)\n",
    "    stream.seek(0)\n",
    "    \n",
    "    cursor.copy_from(stream, table, sep='\\t')\n",
    "    conn.commit()\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_schema_query = \"\"\"CREATE SCHEMA IF NOT EXISTS stations;\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(stations_schema_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geocoding Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address, data_type='json'):\n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/{data_type}'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def get_latlong_components(lat, long, data_type = 'json'):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "   \n",
    "    try:\n",
    "        return r.json()['results'][0]['address_components']\n",
    "    except IndexError:\n",
    "        return -1\n",
    "\n",
    "\n",
    "def extract_zipcode(components):\n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df):\n",
    "    if df.end_lat < 10:\n",
    "        return extract_zipcode(get_address_components(df.endname))\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BayWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endid has more distinct values than startid\n",
    "# Tables module\n",
    "bay_station_query = \"\"\"\n",
    "        SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "          FROM staging.bay_trip \n",
    "         ORDER BY endid;\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station = pd.read_sql(bay_station_query, conn) # Expect long execution times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_decimal(x):\n",
    "    \"\"\"Drops the .0 from a string, if it has it\"\"\"\n",
    "    if x.endswith('.0'):\n",
    "        return(x[:-2])\n",
    "    else: return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station['endid'] = bay_station.endid.apply(drop_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_station.drop_duplicates(inplace=True)\n",
    "bay_station = bay_station.set_index('endid').drop(['449','420', '408','484']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial = gpd.GeoDataFrame(bay_station, geometry=gpd.points_from_xy(bay_station.end_long, bay_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial.zipcode = bay_spatial.zipcode.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "bay_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.bay_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(bay_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, bay_spatial, 'stations.bay_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the BlueWheels Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_station = get_stations(conn, 'blue', [153, 158, 223, 229, 230, 308, 382])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial = gpd.GeoDataFrame(blue_station, geometry=gpd.points_from_xy(blue_station.end_long, blue_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_spatial['zipcode'] = blue_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endid</th>\n",
       "      <th>endname</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>18 Dorrance Warehouse</td>\n",
       "      <td>42.387150</td>\n",
       "      <td>-71.075980</td>\n",
       "      <td>POINT (-71.07598 42.38715)</td>\n",
       "      <td>02129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Colleges of the Fenway - Fenway at Avenue Loui...</td>\n",
       "      <td>42.340115</td>\n",
       "      <td>-71.100620</td>\n",
       "      <td>POINT (-71.10062 42.34011)</td>\n",
       "      <td>02115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Tremont St at E Berkeley St</td>\n",
       "      <td>42.345390</td>\n",
       "      <td>-71.069620</td>\n",
       "      <td>POINT (-71.06962 42.34539)</td>\n",
       "      <td>02116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Northeastern University - North Parking Lot</td>\n",
       "      <td>42.341812</td>\n",
       "      <td>-71.090180</td>\n",
       "      <td>POINT (-71.09018 42.34181)</td>\n",
       "      <td>02115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Cambridge St at Joy St</td>\n",
       "      <td>42.361256</td>\n",
       "      <td>-71.065285</td>\n",
       "      <td>POINT (-71.06529 42.36126)</td>\n",
       "      <td>02114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   endid                                            endname    end_lat  \\\n",
       "0    1.0                              18 Dorrance Warehouse  42.387150   \n",
       "1    3.0  Colleges of the Fenway - Fenway at Avenue Loui...  42.340115   \n",
       "2    4.0                        Tremont St at E Berkeley St  42.345390   \n",
       "3    5.0        Northeastern University - North Parking Lot  42.341812   \n",
       "4    6.0                             Cambridge St at Joy St  42.361256   \n",
       "\n",
       "    end_long                    geometry zipcode  \n",
       "0 -71.075980  POINT (-71.07598 42.38715)   02129  \n",
       "1 -71.100620  POINT (-71.10062 42.34011)   02115  \n",
       "2 -71.069620  POINT (-71.06962 42.34539)   02116  \n",
       "3 -71.090180  POINT (-71.09018 42.34181)   02115  \n",
       "4 -71.065285  POINT (-71.06529 42.36126)   02114  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_spatial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "blue_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.blue_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(blue_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, blue_spatial, 'stations.blue_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Capital Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_station = get_stations(conn, 'capital', [-1,0,32902])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_spatial = gpd.GeoDataFrame(capital_station, geometry=gpd.points_from_xy(capital_station.end_long, capital_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "capital_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.capital_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(capital_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-dd46e6e54fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcapital_spatial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zipcode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcapital_spatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_zipcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-1cd07caf3c46>\u001b[0m in \u001b[0;36minput_zipcode\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_lat\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_address_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_latlong_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_long\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-0dfe4a7bed27>\u001b[0m in \u001b[0;36mget_address_components\u001b[0;34m(address, data_type)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address_components'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "capital_spatial['zipcode'] = capital_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, capital_spatial, 'stations.capital_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CitiBike Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_station = get_stations(conn, 'citi', [-1,3036,3650,3247,3248,3446,3480,3488,3633])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial = gpd.GeoDataFrame(citi_station, geometry=gpd.points_from_xy(citi_station.end_long, citi_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "citi_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.citi_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(citi_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial['zipcode'] = citi_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_spatial.zipcode = citi_spatial.zipcode.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endid</th>\n",
       "      <th>endname</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>72.0</td>\n",
       "      <td>W 52 St &amp; 11 Ave</td>\n",
       "      <td>40.767273</td>\n",
       "      <td>-73.993930</td>\n",
       "      <td>POINT (-73.99393 40.76727)</td>\n",
       "      <td>10019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>79.0</td>\n",
       "      <td>Franklin St &amp; W Broadway</td>\n",
       "      <td>40.719116</td>\n",
       "      <td>-74.006670</td>\n",
       "      <td>POINT (-74.00667 40.71912)</td>\n",
       "      <td>10013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82.0</td>\n",
       "      <td>St James Pl &amp; Pearl St</td>\n",
       "      <td>40.711174</td>\n",
       "      <td>-74.000170</td>\n",
       "      <td>POINT (-74.00017 40.71117)</td>\n",
       "      <td>10038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83.0</td>\n",
       "      <td>Atlantic Ave &amp; Fort Greene Pl</td>\n",
       "      <td>40.683826</td>\n",
       "      <td>-73.976326</td>\n",
       "      <td>POINT (-73.97633 40.68383)</td>\n",
       "      <td>11217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>116.0</td>\n",
       "      <td>W 17 St &amp; 8 Ave</td>\n",
       "      <td>40.741776</td>\n",
       "      <td>-74.001495</td>\n",
       "      <td>POINT (-74.00150 40.74178)</td>\n",
       "      <td>10011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   endid                        endname    end_lat   end_long  \\\n",
       "0   72.0               W 52 St & 11 Ave  40.767273 -73.993930   \n",
       "1   79.0       Franklin St & W Broadway  40.719116 -74.006670   \n",
       "2   82.0         St James Pl & Pearl St  40.711174 -74.000170   \n",
       "3   83.0  Atlantic Ave & Fort Greene Pl  40.683826 -73.976326   \n",
       "4  116.0                W 17 St & 8 Ave  40.741776 -74.001495   \n",
       "\n",
       "                     geometry zipcode  \n",
       "0  POINT (-73.99393 40.76727)   10019  \n",
       "1  POINT (-74.00667 40.71912)   10013  \n",
       "2  POINT (-74.00017 40.71117)   10038  \n",
       "3  POINT (-73.97633 40.68383)   11217  \n",
       "4  POINT (-74.00150 40.74178)   10011  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citi_spatial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, citi_spatial, 'stations.citi_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Divvy Station Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_station = get_stations(conn, 'divvy', [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial = gpd.GeoDataFrame(divvy_station, geometry=gpd.points_from_xy(divvy_station.end_long, divvy_station.end_lat), crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tables module\n",
    "divvy_station_query = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS stations.divvy_station (\n",
    "                   stationID VARCHAR,\n",
    "                   name VARCHAR(128) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL,\n",
    "                   geometry GEOGRAPHY(POINT,4326) NOT NULL,\n",
    "                   zipcode INTEGER\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(divvy_station_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-a08bc3fcd6e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdivvy_spatial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zipcode'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivvy_spatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_zipcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6876\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6877\u001b[0m         )\n\u001b[0;32m-> 6878\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6880\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# compute the result using the series generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m                 \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-1cd07caf3c46>\u001b[0m in \u001b[0;36minput_zipcode\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minput_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_lat\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_address_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mextract_zipcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_latlong_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_lat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_long\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1927c2bb7fca>\u001b[0m in \u001b[0;36mget_address_components\u001b[0;34m(address, data_type)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m299\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'address_components'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "divvy_spatial['zipcode'] = divvy_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_spatial.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "database_upload(conn, divvy_spatial, 'stations.divvy_station')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Geocoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlencode\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'AIzaSyCrG_VK47xMKjER4zpHyd3FJNLFn2weNFY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_address_components(address, data_type='json'):\n",
    "    endpoint = f'https://maps.googleapis.com/maps/api/geocode/{data_type}'\n",
    "    params = {'address': address, 'key': api_key}\n",
    "    url_params = urlencode(params)\n",
    "    url = f\"{endpoint}?{url_params}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def get_latlong_components(lat, long, data_type = 'json'):\n",
    "    url = f\"https://maps.googleapis.com/maps/api/geocode/json?latlng={lat}, {long}&key={api_key}\"\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    if r.status_code not in range(200,299):\n",
    "        return {}\n",
    "    return r.json()['results'][0]['address_components']\n",
    "\n",
    "\n",
    "def extract_zipcode(components):\n",
    "    for comp in components:\n",
    "        if comp.get('types')[0] == 'postal_code':\n",
    "            return comp.get('long_name')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(row):\n",
    "    if row['end_lat'] < 10:\n",
    "        return 'address'\n",
    "        #row['zipcode'] = extract_zipcode(get_address_components(row.endname))\n",
    "    else:\n",
    "        return 'latlong'\n",
    "        #row['zipcode'] = extract_zipcode(get_latlong_components(row.end_lat, row.end_long))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = np.where(\n",
    "    bay_spatial.end_lat < 10,\n",
    "    extract_zipcode(get_address_components(bay_spatial.endname)),\n",
    "    extract_zipcode(get_latlong_components(bay_spatial.end_lat, bay_spatial.end_long))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94109\n",
      "94108\n"
     ]
    }
   ],
   "source": [
    "for index, row in bay_spatial.iloc[:2,].iterrows():\n",
    "    print(extract_zipcode(get_latlong_components(row.end_lat, row.end_long)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_zipcode(df):\n",
    "    if df.end_lat < 10:\n",
    "        return extract_zipcode(get_address_components(df.endname))\n",
    "    else:\n",
    "        return extract_zipcode(get_latlong_components(df.end_lat, df.end_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_spatial['zipcode'] = bay_spatial.apply(input_zipcode, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endid</th>\n",
       "      <th>endname</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_long</th>\n",
       "      <th>geometry</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>37.800000</td>\n",
       "      <td>-122.420000</td>\n",
       "      <td>POINT (-122.42000 37.80000)</td>\n",
       "      <td>94109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Washington St at Kearny St</td>\n",
       "      <td>37.795390</td>\n",
       "      <td>-122.404770</td>\n",
       "      <td>POINT (-122.40477 37.79539)</td>\n",
       "      <td>94108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>Bryant St at 15th St</td>\n",
       "      <td>37.767100</td>\n",
       "      <td>-122.410660</td>\n",
       "      <td>POINT (-122.41066 37.76710)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>15th St at Potrero Ave</td>\n",
       "      <td>37.767080</td>\n",
       "      <td>-122.407360</td>\n",
       "      <td>POINT (-122.40736 37.76708)</td>\n",
       "      <td>94103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>Irwin St at 8th St</td>\n",
       "      <td>37.766884</td>\n",
       "      <td>-122.399580</td>\n",
       "      <td>POINT (-122.39958 37.76688)</td>\n",
       "      <td>94107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>SJ-Q4</td>\n",
       "      <td>Willow St at Blewett Ave</td>\n",
       "      <td>37.309013</td>\n",
       "      <td>-121.900010</td>\n",
       "      <td>POINT (-121.90001 37.30901)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>SJ-Q5</td>\n",
       "      <td>Bird Ave at Willow St</td>\n",
       "      <td>37.311234</td>\n",
       "      <td>-121.896300</td>\n",
       "      <td>POINT (-121.89630 37.31123)</td>\n",
       "      <td>95125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>SJ-Q8</td>\n",
       "      <td>Palm St at Willow St</td>\n",
       "      <td>37.317300</td>\n",
       "      <td>-121.884995</td>\n",
       "      <td>POINT (-121.88500 37.31730)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>SJ-Q9</td>\n",
       "      <td>Willow St at Vine St</td>\n",
       "      <td>37.318450</td>\n",
       "      <td>-121.883170</td>\n",
       "      <td>POINT (-121.88317 37.31845)</td>\n",
       "      <td>95110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>SJ-R11</td>\n",
       "      <td>South San Jose State (7th St at Humboldt St)</td>\n",
       "      <td>37.320263</td>\n",
       "      <td>-121.870030</td>\n",
       "      <td>POINT (-121.87003 37.32026)</td>\n",
       "      <td>95112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>951 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      endid                                       endname    end_lat  \\\n",
       "0                                                          37.800000   \n",
       "1        10                    Washington St at Kearny St  37.795390   \n",
       "2       100                          Bryant St at 15th St  37.767100   \n",
       "3       101                        15th St at Potrero Ave  37.767080   \n",
       "4       102                            Irwin St at 8th St  37.766884   \n",
       "..      ...                                           ...        ...   \n",
       "946   SJ-Q4                      Willow St at Blewett Ave  37.309013   \n",
       "947   SJ-Q5                         Bird Ave at Willow St  37.311234   \n",
       "948   SJ-Q8                          Palm St at Willow St  37.317300   \n",
       "949   SJ-Q9                          Willow St at Vine St  37.318450   \n",
       "950  SJ-R11  South San Jose State (7th St at Humboldt St)  37.320263   \n",
       "\n",
       "       end_long                     geometry zipcode  \n",
       "0   -122.420000  POINT (-122.42000 37.80000)   94109  \n",
       "1   -122.404770  POINT (-122.40477 37.79539)   94108  \n",
       "2   -122.410660  POINT (-122.41066 37.76710)   94103  \n",
       "3   -122.407360  POINT (-122.40736 37.76708)   94103  \n",
       "4   -122.399580  POINT (-122.39958 37.76688)   94107  \n",
       "..          ...                          ...     ...  \n",
       "946 -121.900010  POINT (-121.90001 37.30901)   95125  \n",
       "947 -121.896300  POINT (-121.89630 37.31123)   95125  \n",
       "948 -121.884995  POINT (-121.88500 37.31730)   95110  \n",
       "949 -121.883170  POINT (-121.88317 37.31845)   95110  \n",
       "950 -121.870030  POINT (-121.87003 37.32026)   95112  \n",
       "\n",
       "[951 rows x 6 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bay_spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
