{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Database\n",
    "\n",
    "Each citibike file records information about every single trip that was taken during a single month of the year. There are files for each month starting from June 2013. Each citibike file has the same format. The order and the description of the colomns are as follows:\n",
    "- Trip Duration (seconds): The length of the trip in seconds\n",
    "- Start Date & Time: The start time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- End Date & Time: The end time of the trip MM-DD-YYYY HH:MM:SS\n",
    "- Start Station ID: The ID for the station where the trip started\n",
    "- Start Station Name: The name of the station where the trip started\n",
    "- Start Station Latitude: The latitude of the station where the trip started\n",
    "- Start Station Longitude: The longitude of the station where the trip started\n",
    "- End Station ID: The ID for the station where the trip ended\n",
    "- End Station Name: The name of the station where the trip ended\n",
    "- End Station Latitude: The latitude of the station where the trip ended\n",
    "- End Station Longitude: The longitude of the station where the trip ended\n",
    "- Bike ID: The ID for the bike that was used in the trip\n",
    "- User Type: What type of user took the trip (Subscriber or Customer)\n",
    "- Gender: The gender of the user (Male - 1, Female - 2, None - 0)\n",
    "- Year of Birth: The year that the user was born"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"DatabaseDiagram.png\" width=\"600\" height=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.7/site-packages (2.8.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.4 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Staging Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting s3fs\n",
      "  Using cached s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "Collecting fsspec>=0.8.0\n",
      "  Using cached fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Using cached aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Using cached aioitertools-0.7.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /opt/conda/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs) (1.11.2)\n",
      "Collecting botocore<1.17.45,>=1.17.44\n",
      "  Using cached botocore-1.17.44-py2.py3-none-any.whl (6.5 MB)\n",
      "Collecting aiohttp>=3.3.1\n",
      "  Using cached aiohttp-3.7.3-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting typing_extensions>=3.7\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.15.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /opt/conda/lib/python3.7/site-packages (from botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.25.8)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-5.0.2-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
      "Collecting async-timeout<4.0,>=3.0\n",
      "  Using cached async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: chardet<4.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (3.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (19.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.17.45,>=1.17.44->aiobotocore>=1.0.1->s3fs) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs) (2.8)\n",
      "\u001b[31mERROR: boto3 1.16.21 has requirement botocore<1.20.0,>=1.19.21, but you'll have botocore 1.17.44 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: awscli 1.18.181 has requirement botocore==1.19.21, but you'll have botocore 1.17.44 which is incompatible.\u001b[0m\n",
      "Installing collected packages: fsspec, typing-extensions, aioitertools, botocore, multidict, yarl, async-timeout, aiohttp, aiobotocore, s3fs\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.6.2\n",
      "    Uninstalling fsspec-0.6.2:\n",
      "      Successfully uninstalled fsspec-0.6.2\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.19.21\n",
      "    Uninstalling botocore-1.19.21:\n",
      "      Successfully uninstalled botocore-1.19.21\n",
      "Successfully installed aiobotocore-1.1.2 aiohttp-3.7.3 aioitertools-0.7.1 async-timeout-3.0.1 botocore-1.17.44 fsspec-0.8.4 multidict-5.0.2 s3fs-0.5.1 typing-extensions-3.7.4.3 yarl-1.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install s3fs;c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import os\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCESS_KEY_ID = 'AKIARJEUISD2VILSZ6HM'\n",
    "ACCESS_SECRET_KEY = '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'\n",
    "bucket = \"s3://williams-citibike/TripData/\"\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False, key = ACCESS_KEY_ID, secret= ACCESS_SECRET_KEY)\n",
    "trip_filenames = fs.ls(\"s3://williams-citibike/TripData/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stagingtable = \"\"\"\n",
    "           CREATE TABLE IF NOT EXISTS staging (\n",
    "               tripduration INTEGER, \n",
    "               starttime TIMESTAMP,\n",
    "               endtime TIMESTAMP,\n",
    "               startID NUMERIC,\n",
    "               startname VARCHAR(64),\n",
    "               start_lat REAL,\n",
    "               start_long REAL,\n",
    "               endID NUMERIC,\n",
    "               endname VARCHAR(64),\n",
    "               end_lat REAL,\n",
    "               end_long REAL,\n",
    "               bikeID INTEGER,\n",
    "               usertype VARCHAR(16),\n",
    "               birthyear REAL,\n",
    "               gender SMALLINT                \n",
    "          );\n",
    "          \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(stagingtable)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_stage(datafile: str) -> None:\n",
    "    \"\"\"Grabs the data from the s3 bucket and edits it so that it can be uploaded to the staging table\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly the database should now have rows corresponding to the rows in the data\n",
    "    \"\"\"\n",
    "    \n",
    "    datastream = StringIO()\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'r') as file:\n",
    "        data = pd.read_csv(file, na_values =\"\") \n",
    "        data.fillna(-1, inplace=True) # Empty spaces need to be integers for birthyear column in database\n",
    "        \n",
    "        #Some stations have commas in their name causing the copy_from to register extra data fields\n",
    "        data.iloc[:, 4] = data.iloc[:, 4].str.replace(',','_')\n",
    "        data.iloc[:, 8] = data.iloc[:, 8].str.replace(',','_')\n",
    "        \n",
    "        # data.iloc[:, 3] = data.iloc[:, 3].astype('int32')\n",
    "        # data.iloc[:, 7] = data.iloc[:, 7].astype('int32')\n",
    "        \n",
    "        data.to_csv(datastream, index=False, header = False)\n",
    "        datastream.seek(0)\n",
    "\n",
    "        cursor.copy_from(datastream,'staging',sep=',')\n",
    "        conn.commit()\n",
    "    \n",
    "    datastream.close()\n",
    "    print(f\"Finished Uploading to Staging Table: {datafile}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Uploading to Raw: williams-citibike/TripData/2013-07 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/2013-08 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/2013-09 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/2013-10 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/2013-11 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/2013-12 - Citi Bike trip data.csv\n",
      "Finished Uploading to Raw: williams-citibike/TripData/201306-citibike-tripdata.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "for file in trip_filenames:\n",
    "    populate_staging(file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Station Table (Without the Neighborhood Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationtable = \"\"\"\n",
    "               CREATE TABLE IF NOT EXISTS station (\n",
    "                   stationID NUMERIC PRIMARY KEY,\n",
    "                   name VARCHAR(64) NOT NULL,\n",
    "                   latitude REAL,\n",
    "                   longitude REAL\n",
    "                );\n",
    "                \n",
    "                \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(stationtable)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_query = \"\"\"\n",
    "               INSERT INTO station\n",
    "               SELECT DISTINCT ON(endid) endid, endname, end_lat, end_long \n",
    "                FROM staging \n",
    "               ORDER BY endid;\n",
    "               \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(insert_query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Trip Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "triptable = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS trip (\n",
    "                startime TIMESTAMP,\n",
    "                endtime TIMESTAMP,\n",
    "                tripduration INTEGER,\n",
    "                startID NUMERIC,\n",
    "                endID NUMERIC,\n",
    "                usertype VARCHAR(16),\n",
    "                birthyear REAL,\n",
    "                gender SMALLINT\n",
    "            );\n",
    "            \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(triptable)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_query2 = \"\"\"\n",
    "                INSERT INTO trip\n",
    "                SELECT starttime, endtime, tripduration, startid, endid, usertype, birthyear, gender\n",
    "                  FROM staging\n",
    "                 ORDER BY starttime, endtime;\n",
    "                \"\"\"\n",
    "\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(insert_query2)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating the Neighborhood Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_code_names = []\n",
    "\n",
    "#Instead of creating a dictionary like before, we create a list of tuples so that we can make a df\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_code_names.append((code.text[:4], code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_df = pd.DataFrame(hood_code_names, columns=[\"Code\", \"Name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = {\n",
    "        \"BK\": \"Brooklyn\", \n",
    "        \"BX\": \"Bronx\",\n",
    "        \"MN\": \"Manhattan\",\n",
    "        \"QN\": \"Queens\",\n",
    "        \"SI\": \"Staten\"\n",
    "        }\n",
    "\n",
    "hood_df[\"Borough\"] = hood_df[\"Code\"].str[0:2].map(borough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoodtable = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS neighborhood (\n",
    "                code CHAR(4) PRIMARY KEY,\n",
    "                hoodname VARCHAR NOT NULL,\n",
    "                borough VARCHAR(16) NOT NULL\n",
    "            );\n",
    "            \"\"\"\n",
    "cursor.execute(\"rollback;\")\n",
    "cursor.execute(hoodtable)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoodstream = StringIO()\n",
    "\n",
    "hood_df.to_csv(hoodstream, index=False, header = False)\n",
    "hoodstream.seek(0)\n",
    "\n",
    "cursor.copy_from(hoodstream,'neighborhood',sep=',')\n",
    "conn.commit()\n",
    "    \n",
    "hoodstream.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Neighborhood Profile File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_filenames = fs.ls(\"s3://williams-citibike/HoodData/\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_hooddata(datafile: str) -> pd.DataFrame:\n",
    "    \"\"\"Grabs the data from the s3 bucket and flattens it to a single row consisting of the neighborhood attributes\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datafile : str\n",
    "        The name of a file in the s3 bucket without the s3:// prefix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame:\n",
    "        A single row DataFrame that contains the attributes of the neighborhood\n",
    "    \"\"\"\n",
    "    \n",
    "    cols_lst = [0,3,8]\n",
    "    names_lst = [\"code\", \"indicator\", \"2018\"]\n",
    "    \n",
    "    with fs.open(\"s3://\"+datafile, 'rb') as file:\n",
    "        data = pd.read_excel(file, sheet_name=1, usecols = cols_lst, names = names_lst)\n",
    "        \n",
    "        # Prep the '2018' column so that it can used as the value argument in the pivot_table \n",
    "        data['2018'] = data['2018'].str.replace('$',\"\")\n",
    "        data['2018'] = data['2018'].str.replace(',',\"\")\n",
    "\n",
    "        # Values that are percents get turned into decimals\n",
    "        for index, value in data['2018'].items():\n",
    "            if isinstance(value,str):\n",
    "                if value[-1] == '%':\n",
    "                    data['2018'][index] = float(value.strip('%')) / 100\n",
    "        \n",
    "        data['2018'] = pd.to_numeric(data['2018'])\n",
    "        \n",
    "        # The pivot_table alphabatizes the columns, but we want to maintain the original order\n",
    "        column_order = ['code'] + list(data['indicator'])\n",
    "        \n",
    "        data = data.pivot_table(index=['code'],values='2018', columns='indicator', dropna=False)\n",
    "        data = data.rename_axis(None, axis=1).reset_index()\n",
    "        data['code'] = data['code'][0].replace(\" \",\"\")\n",
    "        data = data.reindex(column_order, axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile_df = pd.DataFrame()\n",
    "\n",
    "for hood in hood_filenames:\n",
    "    hood_profile_df = hood_profile_df.append(flatten_hooddata(hood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile_df = hood_profile_df.set_index('code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading Neighborhood Profile Data to Personal S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "     's3',\n",
    "     aws_access_key_id = ACCESS_KEY_ID,\n",
    "     aws_secret_access_key = ACCESS_SECRET_KEY\n",
    ")\n",
    "\n",
    "bucket = 'williams-citibike'   # Premade bucket in S3\n",
    "hood_prefix = 'HoodData'   # Premade folder inside the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_profile_df.to_csv(\"Hood_Profile_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(bucket).Object(os.path.join(hood_prefix,\"Hood_Profile_Data.csv\")).upload_file(\"Hood_Profile_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(\"Hood_Profile_Data.csv\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
