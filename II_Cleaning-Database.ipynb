{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.8.6-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install psycopg2-binary;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the password in \n",
    "PGHOST = 'tripdatabase2.cmaaautpgbsf.us-east-2.rds.amazonaws.com'\n",
    "PGDATABASE = ''\n",
    "PGUSER = 'postgres'\n",
    "PGPASSWORD = 'Josh1234'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Success: ('PostgreSQL 12.5 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11), 64-bit',) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Database Context Manager\n",
    "try:   \n",
    "    # Set up a connection to the postgres server.    \n",
    "    conn = psycopg2.connect(user = PGUSER,\n",
    "                            port = \"5432\",\n",
    "                            password = PGPASSWORD,\n",
    "                            host = PGHOST,\n",
    "                            database = PGDATABASE)\n",
    "    # Create a cursor object\n",
    "    cursor = conn.cursor()   \n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    record = cursor.fetchone()\n",
    "    print(\"Connection Success:\", record,\"\\n\")\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Table Cleaning I: Handling Outliers - Speed\n",
    "In this section we are going to modify trips whose speed (MPH) are physically unlikely. Luckily, we have a reference from the different services on what a speed outlier might look like. According to them, their pedal assisted e-bikes can go up to 18 MPH. In the sport of cycling, although not on a pedal assisted bike, a reasonably experienced cyclist can reach over 19 MPH. Using a nicer whole number, we'll use 20 MPH as a conservative cutoff and anything over that will be considered an outlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already took the \"delete data\" path multiple times in this project. Although there is still millions of rows of data after all those deletes, let's make an effort to conserve data. To handle speed outliers, we are going to cap the speed at 20 MPH. Any trip that has a speed over 20 MPH we are going to adjust the trip duration in a way that when the speed is calculated it will result in 20 MPH. $$\\frac{Distance}{\\frac{Duration}{60}} = 20$$\n",
    "\n",
    "$$\\frac{60 \\times Distance}{20} = Duration$$\n",
    "\n",
    "$$3 \\times Distance = Duration$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off the formula, we will have to find the trips whose speed is above 20 MPH and set it to 20 MPH, then the duration column by multiplying the distance by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import Queries\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Queries' from '/root/Citi-Bike-Expansion/Queries.py'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(Queries) # Delete this for the publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "services = ['bay', 'blue', 'capital', 'citi', 'divvy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    speed_outliers_query = f\"\"\"\n",
    "            UPDATE trips.{service}_trip\n",
    "               SET speed = 20,\n",
    "                   duration = distance * 3\n",
    "             WHERE speed > 20;\n",
    "             \"\"\"\n",
    "    \n",
    "    Queries.execute_query(conn,speed_outliers_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to update the speed column on the upper end because we are fairly certain that the bikes can't go over 20 MPH, but what about speed on the lower end. Specifically the trips that have a speed of 0 MPH which is a result of a round trip. Round trips occur when a rider starts and ends at the same station, resulting in a distance of zero miles and a speed also being zero. So how do we deal with this? Almost the same as with the upper speed value. We are going to set the speed to be the average 6 MPH. This time instead of finding the duration, we are going to find the distance. \n",
    "$$\\frac{Distance}{\\frac{Duration}{60}} = 6$$\n",
    "\n",
    "$$Distance = \\frac{6 \\times Duration}{60}$$\n",
    "\n",
    "$$Distance = \\frac{Duration}{10}$$\n",
    "\n",
    "\n",
    "\n",
    "***Note on Average Speed:*** *It was found by excluding the 0 MPH trips and after the 20 MPH adjustment and was 6 MPH for every bike service*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    roundtrip_outliers_query = f\"\"\"\n",
    "                UPDATE trips.{service}_trip\n",
    "                   SET speed = 6,\n",
    "                       distance = duration / 10\n",
    "                 WHERE distance = 0;\n",
    "                 \"\"\"\n",
    "    Queries.execute_query(conn, roundtrip_outliers_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Table Cleaning II: Removing Outliers - Trip Duration\n",
    "Before we remove outliers, we have to define what an outlier is. First let's get a feeling for how the trip duration values are distributed. The trip tables range from 7M to 111M trips, which means we can't query the entire table everytime we want to analyze something. For each table we will take a saple of 1M rows. Our sampling needs to fufill three criteria:\n",
    "- The sampling needs to be random\n",
    "- The sampling needs to be large enough\n",
    "- The query needs to be decently fast "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Sampling Procedure\n",
    "\n",
    "Unfortunately the ideal Bernoulli sampling in PostgresSQL meets the random condition, but is way too slow for our needs. And the System sampling, although fast enough, isn't truly random. To overcome these limitations, we will use the System sampling for speed and then make the selection of the rows random. The sampling process is as follows:\n",
    "- System sample 1% of the data from a table\n",
    "- Order the results of that sample randomly (ORDER BY RANDOM())\n",
    "- Take the first 20,000 rows (100,000 for CitiBike)\n",
    "- Repeat the previous steps 50 times (10 for CitiBike)\n",
    "\n",
    "*Note: there is a chance that there are duplicate rows in the sampling. The more trips that a service has, the less likely repeats will appear.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bay_sample = Queries.get_random_rows(conn, 'bay', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_sample = Queries.get_random_rows(conn, 'blue', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_sample = Queries.get_random_rows(conn, 'capital', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "citi_sample = Queries.get_random_rows(conn, 'citi', samples = 1000000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "divvy_sample = Queries.get_random_rows(conn, 'divvy', samples = 1000000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bay</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>15.894836</td>\n",
       "      <td>179.677429</td>\n",
       "      <td>-28611.720703</td>\n",
       "      <td>6.12</td>\n",
       "      <td>9.89</td>\n",
       "      <td>15.840000</td>\n",
       "      <td>104612.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blue</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>27.167665</td>\n",
       "      <td>694.015686</td>\n",
       "      <td>-49.380001</td>\n",
       "      <td>6.98</td>\n",
       "      <td>11.80</td>\n",
       "      <td>19.770000</td>\n",
       "      <td>320467.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>19.211084</td>\n",
       "      <td>270.017792</td>\n",
       "      <td>-28950.429688</td>\n",
       "      <td>6.70</td>\n",
       "      <td>11.48</td>\n",
       "      <td>19.580000</td>\n",
       "      <td>117703.226562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>citi</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>16.648346</td>\n",
       "      <td>167.448410</td>\n",
       "      <td>-50.500000</td>\n",
       "      <td>6.30</td>\n",
       "      <td>10.60</td>\n",
       "      <td>18.450001</td>\n",
       "      <td>73588.578125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>divvy</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>19.962648</td>\n",
       "      <td>234.937729</td>\n",
       "      <td>-48.000000</td>\n",
       "      <td>7.00</td>\n",
       "      <td>12.00</td>\n",
       "      <td>20.980000</td>\n",
       "      <td>120301.382812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count       mean         std           min   25%    50%  \\\n",
       "bay      1000000.0  15.894836  179.677429 -28611.720703  6.12   9.89   \n",
       "blue     1000000.0  27.167665  694.015686    -49.380001  6.98  11.80   \n",
       "capital  1000000.0  19.211084  270.017792 -28950.429688  6.70  11.48   \n",
       "citi     1000000.0  16.648346  167.448410    -50.500000  6.30  10.60   \n",
       "divvy    1000000.0  19.962648  234.937729    -48.000000  7.00  12.00   \n",
       "\n",
       "               75%            max  \n",
       "bay      15.840000  104612.750000  \n",
       "blue     19.770000  320467.156250  \n",
       "capital  19.580000  117703.226562  \n",
       "citi     18.450001   73588.578125  \n",
       "divvy    20.980000  120301.382812  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([bay_sample.duration.describe(), \n",
    "              blue_sample.duration.describe(),\n",
    "              capital_sample.duration.describe(),\n",
    "              citi_sample.duration.describe(),\n",
    "              divvy_sample.duration.describe()\n",
    "             ], index = ['bay','blue','capital','citi','divvy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our trip duration distrubtions look the same: trip duration is drastically skewed to the right. Because of the skew our mean and standard deviation aren't reliable measures to use for outlier detection. Our standard deviations range from 2.7hrs to 11hrs, yet our median values are all under 12m and the 75th percentiles barely breach the 20m mark. \n",
    "\n",
    "It appears that quantiles are a good measure to use to determine outliers. Quantiles seem to be more representative of real life. However, each different service has their own distributions so there isn't a one-size-fits-all quantile measure to use. The 99th percentile may be 66m for one service, but 160m for another. So the question becomes: ***Is there a universal duration cutoff value that can be used for all bike share services?***\n",
    "\n",
    "To find the answer to that question, we first need to asses whether 'Bike Share Behavior' is universal. ***Do people that use Bike Share services behave the same way, regardless of the service?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal Behavior I - Trip Duration Permuation Test\n",
    "**Note: We are making a major assumption that the sample median is representative of the population median**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Table Cleaning II: Removing Time Errors - Start Time After End Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anytrip with a starttime that was greater than an endtime is an error and will get deleted. It is possible that the two values were just swapped, but the cost of swapping them is more expensive than the trips were worth. Additionally, it's just a small amount of data that it's better to just remove them. The operations of the swap are:\n",
    "- Find the incorrect values\n",
    "- Move them to a temporary table with the correct order\n",
    "- Delete them from the original table\n",
    "- Reinsert them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    Queries.delete_time_swaps(conn, service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Station Table Update I: Station Status\n",
    "The ecosystem of stations in a bike share service is always changing. They add new stations, removes stations, and occasionally moves stations to nearby locations. In this section we are going to add two new columns to each station table in the stations schema called birth and death. \n",
    "\n",
    "The birth column will represent the date of the first trip that was taken from the station. The death column represents the date of the last trip that was taken from the station. Stations are considered dead if there wasn't a trip within the last month of 2020. **Any station that is still active will have a null value in the death column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    add_columns_query = f\"\"\"\n",
    "            ALTER TABLE stations.{service}_station\n",
    "            ADD COLUMN birth TIMESTAMP,\n",
    "            ADD COLUMN death TIMESTAMP;\n",
    "            \"\"\"\n",
    "    Queries.execute_query(conn, add_columns_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the Bay Wheels data requires a format that is slightly different from the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_certificate_query = \"\"\"\n",
    "                WITH timestamps AS (\n",
    "                    SELECT DISTINCT startid, \n",
    "                                    MIN(DATE_TRUNC('day',starttime)::date) over w AS birth, \n",
    "                                    MAX(DATE_TRUNC('day',starttime)::date) over w AS death\n",
    "                      FROM trips.bay_trip\n",
    "                    WINDOW w as (PARTITION BY startid)\n",
    "                )\n",
    "            \n",
    "                UPDATE stations.bay_station AS s\n",
    "                   SET birth = ts.birth,\n",
    "                       death = ts.death\n",
    "                  FROM timestamps AS ts\n",
    "                 WHERE s.stationid = ts.startid;\n",
    "                \"\"\"  \n",
    "Queries.execute_query(conn, birth_certificate_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.birth_certificate(conn, 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.birth_certificate(conn, 'capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.birth_certificate(conn, 'citi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Queries.birth_certificate(conn, 'divvy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for service in services:\n",
    "    set_death_null = f\"\"\"\n",
    "        UPDATE stations.{service}_station\n",
    "           SET death = NULL\n",
    "         WHERE death <= '2020-12-31'\n",
    "           AND death > '2020-12-01';\n",
    "        \"\"\"\n",
    "    Queries.execute_query(conn, set_death_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
