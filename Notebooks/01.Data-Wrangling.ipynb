{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Wrangling**\n",
    "The goal of this notebook is to get all the required data needed to complete the project. The vision for this project is that all files needed for any analysis be stored in the cloud (AWS S3), separate from the directory of the code. In the \"Upload...\" sections we will upload the extracted data from the temporary folder to a personal S3 bucket and then delete the temporary folder. For the remainder of the project, all data will be pulled from that S3 bucket. \n",
    "\n",
    "**Bike Share Trip Data Datasets**\n",
    "\n",
    "This project will be looking at 5 different Bike Share companies: BayWheels, BlueBike, CapitalBike, CitiBike, and DivvyBike. The first five datasets need to be compiled from each companies individual S3 bucket. These datasets contain trip data and holds key information about each trip that was taken by customers of the service. The columns of the datasets include columns such as start time and end station.\n",
    "\n",
    "- Dataset I - <a href=\"https://s3.amazonaws.com/baywheels-data/index.html\"> BayWheels S3 Trip Data Bucket </a>\n",
    "\n",
    "- Dataset II - <a href=\"https://s3.amazonaws.com/hubway-data/index.html\"> BlueBike S3 Trip Data Bucket </a>\n",
    "\n",
    "- Dataset III - <a href=\"https://s3.amazonaws.com/capitalbikeshare-data/index.html\"> Capital S3 Trip Data Bucket </a>\n",
    "\n",
    "- Dataset IV - <a href=\"https://s3.amazonaws.com/tripdata/index.html\"> CitiBike S3 Trip Data Bucket </a>\n",
    "\n",
    "- Dataset V - <a href=\"https://divvy-tripdata.s3.amazonaws.com/index.html\"> DivvyBike S3 Trip Data Bucket </a>\n",
    "\n",
    "\n",
    "\n",
    "**Neighborhood Profiles Datasets**\n",
    "\n",
    "The next group of data that is used contains the the characteristics of each neighborhood in New York City (NYC) and San Francisco. For NYC The data was gathered by the Furman Center for Real Estate and Urban Policy at New York University. For San Francisco the dataset was published by the San Francisco Planning Department. Each dataset has different categories of information about the neighborhoods in both cities. \n",
    "\n",
    "- Dataset VI - <a href = \"https://furmancenter.org/neighborhoods\"> New York City Neighborhood Profiles </a>\n",
    "\n",
    "- Dataset VII - <a href = \"https://default.sfplanning.org/publications_reports/SF_NGBD_SocioEconomic_Profiles/2012-2016_ACS_Profile_Neighborhoods_Final.pdf\"> San Francisco Neighborhood Profiles\n",
    "\n",
    "\n",
    "\n",
    "**GeoSpatial Datasets**\n",
    "\n",
    "Two EDA portions of the project required the geospatial boundaries of the neighborhoods in NYC and San Francisco. The GeoJSON data for NYC was obtained from NYCOpenData and it segments NYC into community districts. *Note: What Furman Center calls Neighborhoods, NYCOpenData calls Community Districts. NYCOpenData has a different dataset called Neighborhood Tabulation Areas which is a more granular division of the city.* The GeoJson data for San Francisco was obtained from DataSF and it segments San Francisco into Analysis Neighborhoods. *Note: DataSF has many neighborhood division the one that matches the neighborhood data is called Analysis Neighborhoods*. The final GeoJSON file has the locations of all the entraces of the subway stations in NYC\n",
    "\n",
    "- Dataset VIII - <a href=\"https://data.cityofnewyork.us/api/geospatial/yfnk-k7r4?method=export&format=GeoJSON\"> NYC Community District GeoJSON File </a>\n",
    "\n",
    "- Dataset IX - <a href=\"https://data.sfgov.org/api/geospatial/p5b7-5n3h?method=export&format=GeoJSON\"> San Francisco Community District GeoJSON File </a>\n",
    "\n",
    "- Dataset X - <a href=\"https://data.cityofnewyork.us/api/geospatial/drex-xx56?method=export&format=GeoJSON\"> Subway Entrance GeoJSON File </a>\n",
    "\n",
    "\n",
    "**Zip Code Datasets**\n",
    "\n",
    "The final group of datasets contain the properties of every zipcode in the United States as well as the Core Based Statistical Areas (CBSAs) of the country.\n",
    "\n",
    "- Dataset XI - Zipcode USA Data\n",
    "- Dataset XII -<a href=\"https://www2.census.gov/programs-surveys/metro-micro/geographies/reference-files/2020/delineation-files/list1_2020.xls\" > Delineation File </a>\n",
    "\n",
    "- Dataset XIII -\n",
    "<a href=\"https://www.huduser.gov/portal/datasets/usps_crosswalk.html\"> USPS Zipcode Crosswalk Files</a>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gathering the Trip Data**\n",
    "The purpose of this section is to connect to and extract all of the trip data files from the S3 bucket of the five different bike share companies. The data will be stored into a temporary folder in the working directory. In a later step the files will then be uploaded to our personal S3 bucket. \n",
    "\n",
    "#### **Subsection Structure** -  Each of the 5 subsections in this section have the same structure\n",
    "<ol>\n",
    "    <li> Define the path to the company's S3 bucket and create an empty directory in working directory to house the files.\n",
    "    <li> Create a custom function to request the file.\n",
    "    <li> Create a custom function that downloads the file contained in the request.\n",
    "    <li> Use a series of for loops to get all the files available (up to 2020-12). \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<p style=\"text-align:center;font-style:italic\">I Understand & Don't Need to See the Code </p>](#Skip_Gathering) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io   # Needed to pull data from CitiBike S3 bucket\n",
    "import os   # Needed to work with folders that will be created\n",
    "import shutil   # Needed to delete the temporary folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unzip(zipper: zipfile.ZipFile, datafile: str, folder: str) -> None:\n",
    "    \"\"\"A helper function that is used in the *_download functions to extract and save data. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    zipper: zipfile.ZipFile\n",
    "        The zipfile that contains the datafile to be extracted.\n",
    "    datafile: str\n",
    "        The name of the file to be downloaded from the zipped folder.\n",
    "    folder: str\n",
    "        The folder where the unzipped datafile will be stored.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        If succesful there should be a datafile (most likely .csv) in the folder that was passed. \n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.exists(folder + datafile):\n",
    "        print(f\"Skipped: {datafile} already extracted from S3 Bucket \\n\")\n",
    "        return None\n",
    "\n",
    "    zipper.extract(datafile, path = folder)\n",
    "    print(f\"Extract Success: {datafile} unzipped and uploaded to {folder} \\n\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BayWheels S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAYWHEELS_DATA_FOLDER = \"https://s3.amazonaws.com/baywheels-data/\" \n",
    "MY_BAYWHEELS = os.path.join(os.getcwd(),\"BayWheelsData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BAYWHEELS):\n",
    "    os.makedirs(MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_request(bay_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    \n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(bay_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201905 the buckets changed from fordgobike -> baywheels\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(bay_bucket + filename.replace('fordgobike','baywheels'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from BayWheels S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BayWheels S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bay_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"        \n",
    "    \n",
    "    # The with block's purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "\n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "        unzip(zip, datafile, folder)\n",
    "        \n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one file doesn't follow the standard convention (see below)\n",
    "r = bay_request(BAYWHEELS_DATA_FOLDER, f\"2017-fordgobike-tripdata.csv.zip\")\n",
    "bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = bay_request(BAYWHEELS_DATA_FOLDER, f\"{year}{month}-fordgobike-tripdata.csv.zip\")\n",
    "        bay_download(r, MY_BAYWHEELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<p style=\"text-align:center;font-style:italic\">After Seeing it Once I Understand the Structure </p>](#Skip_Gathering) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BlueBike S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLUEBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/hubway-data/\" \n",
    "MY_BLUEBIKE = os.path.join(os.getcwd(),\"BlueBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_BLUEBIKE):\n",
    "    os.makedirs(MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_request(blue_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    blue_bucket: str\n",
    "        The URL to the BlueBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    \n",
    "    # The purpose of the following try block is to attempt to connect to the file in the BlueBike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(blue_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 201805 the buckets changed from hubway -> bluebikes\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(blue_bucket + filename.replace('hubway','bluebikes'), stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename.replace('hubway','bluebikes')} requested from BlueBike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from BlueBike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blue_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0]\n",
    "        unzip(zip,datafile,folder)\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = ['2011','2012','2013','2014_1','2014_2']\n",
    "\n",
    "# For these years the data isn't stored as a zip file, it's stored as a csv\n",
    "for year in yearlist:\n",
    "    r = blue_request(BLUEBIKE_DATA_FOLDER, f\"hubway_Trips_{year}.csv\")\n",
    "    url_content = r.content\n",
    "    csv_file = open(f'/root/Citi-Bike-Expansion/BlueBikeData/{year}-hubway-tripdata.csv', 'wb')\n",
    "\n",
    "    csv_file.write(url_content)\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-hubway-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2015\",\"2016\",\"2017\",\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = blue_request(BLUEBIKE_DATA_FOLDER, f\"{year}{month}-hubway-tripdata.zip\")\n",
    "        blue_download(r, MY_BLUEBIKE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CapitalBike S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPITAL_DATA_FOLDER = \"https://s3.amazonaws.com/capitalbikeshare-data/\" \n",
    "MY_CAPITAL = os.path.join(os.getcwd(),\"CapitalData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CAPITAL):\n",
    "    os.makedirs(MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_request(capital_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    capital_bucket: str\n",
    "        The URL to the Capital S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the CapitalBike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    print\n",
    "    try:\n",
    "        r = requests.get(capital_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None   \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Capital S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capital_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # This if-else block handles all the different cases that arise when downloading files from capital\n",
    "        if zip.namelist()[0][4] == 'Q':\n",
    "            for i in range(len(zip.namelist())):\n",
    "                datafile = zip.namelist()[i]\n",
    "                unzip(zip,datafile,folder)\n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "            unzip(zip,datafile,folder)\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = [\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}-capitalbikeshare-tripdata.zip\")\n",
    "    capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201805-capitalbikeshare-tripdata refers to\n",
    "# the file that contains all the trips for May 2018\n",
    "\n",
    "yearlist = [\"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = capital_request(CAPITAL_DATA_FOLDER, f\"{year}{month}-capitalbikeshare-tripdata.zip\")\n",
    "        capital_download(r, MY_CAPITAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CitiBike S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CITIBIKE_DATA_FOLDER = \"https://s3.amazonaws.com/tripdata/\" \n",
    "MY_CITIBIKE = os.path.join(os.getcwd(),\"CitiBikeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_CITIBIKE):\n",
    "    os.makedirs(MY_CITIBIKE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_request(citi_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    citi_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Citibike S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(citi_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        # The firt block might fail due to the inconsistency of the naming convention\n",
    "        # Starting in 2017 the bucket endings changed from .zip -> .csv.zip\n",
    "        # We try to connect again with the new ending \n",
    "        try:\n",
    "            r = requests.get(citi_bucket + filename[:-4] + '.csv.' + filename[-3:], stream=True)\n",
    "            r.raise_for_status()\n",
    "        except requests.exceptions.HTTPError as errh: \n",
    "            print(errh)\n",
    "            return None\n",
    "        else:\n",
    "            print(f\"Request Success: {filename[:-4] + '.csv.' + filename[-3:]} requested from Citibike S3 Bucket\")       \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Citibike S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citi_download(r: requests.models.Response, folder: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # Regardless of the change in naming conventions, the actual data appears first in every bucket\n",
    "        datafile = zip.namelist()[0] \n",
    "        unzip(zip,datafile,folder)\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 201705-citibike-tripdata refers to\n",
    "# the file that contains all the trips for May 2017\n",
    "\n",
    "yearlist = [\"2013\",\"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]\n",
    "monthlist = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs. The reason for the try-except\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        try:\n",
    "            r = citi_request(CITIBIKE_DATA_FOLDER, f\"{year}{month}-citibike-tripdata.zip\")\n",
    "            citi_download(r,MY_CITIBIKE)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DivvyBike S3 Bucket**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Needed to do a regex matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIVVY_DATA_FOLDER = \"https://divvy-tripdata.s3.amazonaws.com/\" \n",
    "MY_DIVVY = os.path.join(os.getcwd(),\"DivvyData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MY_DIVVY):\n",
    "    os.makedirs(MY_DIVVY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_request(divvy_bucket: str, filename: str) -> requests.models.Response:\n",
    "    \"\"\"Connects to CitiBike's S3 bucket and attempts to make a connection to the filename\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    divvy_bucket: str\n",
    "        The URL to the CitiBike S3 bucket\n",
    "    filename: str\n",
    "        The name of the file to be downloaded from the bucket\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r: requests.models.Response\n",
    "        If the connection is succesful the response to the file will be returned. If not it prints an error and returns None\n",
    "    \"\"\"\n",
    "    # The purpose of the following try block is to attempt to connect to the file in the Divvy S3 bucket \n",
    "    # and catch the different errors that may occur if the connection fails. A failed connection exits the function\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(divvy_bucket + filename, stream=True)   \n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None    \n",
    "    else:\n",
    "        print(f\"Request Success: {filename} requested from Divvy S3 Bucket\")\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divvy_download(r: requests.models.Response, folder: str, year: str) -> None:\n",
    "    \"\"\"Uses the response from the file_request function to unzip and download the citibike data to the output location\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    r: requests.models.Response\n",
    "        The response that was returned from the file_request function\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be a csv file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The with block belows purpose is to unzip the file and extract it to the Temporary Bike Folder defined above.\n",
    "    with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "        \n",
    "        # The if-else block handles all the cases that will be encountered when trying to download files. \n",
    "        if year == '2013':\n",
    "            datafile = zip.namelist()[2]\n",
    "            unzip(zip,datafile,folder)\n",
    "\n",
    "        elif int(year) < 2018:                    \n",
    "            for file in zip.namelist():\n",
    "                if re.match('(Divvy_Trips_\\d{4}.{3,4}.csv)$', file):\n",
    "                    datafile = file\n",
    "                    unzip(zip,datafile,folder)\n",
    "                elif re.match('(Divvy_Trips_\\d{4}.{3,5}.csv)$', file):\n",
    "                    datafile = file\n",
    "                    unzip(zip,datafile,folder)\n",
    "\n",
    "        else:\n",
    "            datafile = zip.namelist()[0]\n",
    "            unzip(zip,datafile,folder)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearlist = ['2013','2014','2015','2016','2017','2018','2019','2020']\n",
    "\n",
    "for year in yearlist:\n",
    "    # The naming conventions changes for different years and this if-else block handles all the cases\n",
    "    if year == '2013':\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)\n",
    "    \n",
    "    elif year == '2014':\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Stations_Trips_{year}_{half}.zip\")\n",
    "            divvy_download(r, MY_DIVVY, year)       \n",
    "    \n",
    "    elif int(year) < 2018:\n",
    "        for half in ['Q1Q2','Q3Q4']:\n",
    "            try:\n",
    "                # 404 Error for 2015_Q1Q2 (the only file that gets run in the except)\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}-{half}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "    else:\n",
    "        for quarter in ['Q1','Q2','Q3','Q4']:\n",
    "            try:\n",
    "                # 404 Errors for 2020Q2 - 2020Q4\n",
    "                r = divvy_request(DIVVY_DATA_FOLDER, f\"Divvy_Trips_{year}_{quarter}.zip\")\n",
    "                divvy_download(r, MY_DIVVY, year)\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some files from 2014 and 2016 that need to be specifically downloaded\n",
    "r = divvy_request(DIVVY_DATA_FOLDER, \"Divvy_Stations_Trips_2014_Q3Q4.zip\")\n",
    "with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip: \n",
    "    for file in zip.namelist()[2:5]:\n",
    "        datafile = file\n",
    "        unzip(zip, datafile, MY_DIVVY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some files from 2014 and 2016 that need to be specifically downloaded\n",
    "r = divvy_request(DIVVY_DATA_FOLDER, \"Divvy_Trips_2016_Q1Q2.zip\")\n",
    "with zipfile.ZipFile(io.BytesIO(r.content), 'r') as zip:\n",
    "    for file in zip.namelist()[1:5]:\n",
    "        datafile = file\n",
    "        unzip(zip, datafile, MY_DIVVY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the datafiles have the same prefix before the .zip. For example the file with the prefix 202005-divvy-tripdata refers to\n",
    "# the file that contains all the trips for May 2020\n",
    "\n",
    "yearlist = [\"2020\"]\n",
    "monthlist = [\"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n",
    "\n",
    "# Citibike starts in 201306 so there should be 404 errors for the first 5 runs\n",
    "for year in yearlist:\n",
    "    for month in monthlist:\n",
    "        r = divvy_request(DIVVY_DATA_FOLDER, f\"{year}{month}-divvy-tripdata.zip\")\n",
    "        divvy_download(r, MY_DIVVY, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Move Files into the Main DivvyBike Temporary Directory**\n",
    "\n",
    "There are 3 files that when downloaded they went to a subfolder instead of the main temporary folder. In this sub-subsection we will move those file into the main temporary directory with the other files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move one file from a subdirectory into the main DivvyData folder\n",
    "destination = os.path.join(os.getcwd(),'DivvyData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders = [os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2013'),\n",
    "              os.path.join(os.getcwd(),'DivvyData','Divvy_Stations_Trips_2014_Q3Q4'),\n",
    "              os.path.join(os.getcwd(),'DivvyData','Divvy_Trips_2016_Q1Q2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moves the files out of the subfolders\n",
    "for folder in subfolders:\n",
    "    for file in os.listdir(folder):\n",
    "        shutil.move(os.path.join(folder,file), destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deletes the subfolders\n",
    "for i in range(len(subfolders)):\n",
    "    shutil.rmtree(subfolders[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<a id=\"Skip_Gathering\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scraping NYC Neighborhood Data**\n",
    "The purpose of this section is to connect to and extract all 59 of the NYC neighborhood profile files from the Furman Center. Like the trip data the files will be stored into a temporary folder in the working directory and then in a later step uploaded to our personal S3 bucket. We will now need to add BeautifulSoup to our toolbox. \n",
    "\n",
    "#### **Phase I - Getting the Neighborhood Codes**\n",
    "To download the xlsx files from Furman Center we need the 4 character code for each community district. To get those values we'll use the beautifulsoup package to scrap the dropdown menu and store the code:name key-value pairs of each community in a dictionary. For example, BK04:Bushwick will be an entry in the dictionary (The BK portion represents the borough Brooklyn).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   # Needed to parse the Furman Center website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt connection to the URL\n",
    "HoodURL = \"https://furmancenter.org/neighborhoods\"\n",
    "try:\n",
    "    r2 = requests.get(HoodURL)\n",
    "    r2.raise_for_status()\n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(errh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r2.content, \"html.parser\")\n",
    "\n",
    "# The website has a dropdown with all the neighborhood codes and names\n",
    "hood_codes = {}\n",
    "for code in soup.find_all('option')[1:]:\n",
    "    hood_codes[code.text[:4]] = code.text[6:].replace(\"/\",\"-\").replace(\" \",\"_\")   # Borough names will be used as filename in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Phase II - Getting the Neighborhood Data Files**\n",
    "With the neighborhood codes available, we can send a request to the Furman Center, download their excel files, and store it in a temporary folder. This is going to be a similar process to the trip data files, but simpler since we don't have to deal with zipped folders. Later the data will be uploaded to the S3 Bucket and then deleted from the local repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_HOOD_FOLDER = MY_CAPITAL = os.path.join(os.getcwd(),\"TempHoodData/\")\n",
    "\n",
    "if not os.path.exists(TEMP_HOOD_FOLDER):\n",
    "    os.makedirs(TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_hood_data(code: str, name: str, folder: str) -> None:\n",
    "    \"\"\"Uses the scraped neighborhood code to download the xlsx data from Furman Center\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    code: str\n",
    "        The 4 character neighborhood string\n",
    "    name: str\n",
    "        The actual name of the neighborhood\n",
    "    folder: str\n",
    "        The output location of the file download\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly there should be an XLSX file in the specified folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    file = f\"https://furmancenter.org/files/NDP/{code}_NeighborhoodDataProfile.xlsx\"\n",
    "    \n",
    "    if os.path.exists(folder + f\"{code}_{name}.xlsx\"):\n",
    "        print(f\"Skipped: {code}_{name} already downloaded from Furman Center\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        r3 = requests.get(file)\n",
    "        r3.raise_for_status()\n",
    "    except requests.exceptions.HTTPError as errh:\n",
    "        print(errh)\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Request Success: {file} from Furman Center\")\n",
    "    \n",
    "    with open(folder + f\"{code}_{name}.xlsx\", 'wb') as output:\n",
    "        output.write(r3.content)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in hood_codes.items():\n",
    "    pull_hood_data(key, value, TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Upload All the Data to our Personal S3 Bucket**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div aling=\"center\" class=\"alert alert-block alert-danger\">\n",
    "    <b>Danger: The upload into S3 is free, but there is a cost attached to storing it in S3. </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **BikeShare Trip Data Uploads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3   # AWS SDK Needed to work with our Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This code can has to be executed with your own S3 bucket by changing the following string values:\n",
    "# ACCESS_KEY_ID, ACCESS_SECRET_KEY, bucket, trip_prefix \n",
    "\n",
    "ACCESS_KEY_ID = ''\n",
    "ACCESS_SECRET_KEY = ''\n",
    "\n",
    "s3 = boto3.resource(\n",
    "     's3',\n",
    "     aws_access_key_id = ACCESS_KEY_ID,\n",
    "     aws_secret_access_key = ACCESS_SECRET_KEY\n",
    ")\n",
    "\n",
    "bucket = 'williams-citibike'   # Premade bucket in S3\n",
    "trip_prefix = 'TripData'   # Premade folder inside the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_upload(directory: str, prefix: str):\n",
    "    \"\"\"Goes through the temporary folders and uploads the data to the S3 buckets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: str\n",
    "        The path to the directory that has the downloaded files\n",
    "    prefix: str\n",
    "        The name of the 'folder' in S3 where the data will be transfered to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None:\n",
    "        If executed properly all the files will be within the S3 bucket in the prefix 'folder'\n",
    "    \"\"\"\n",
    "        \n",
    "    filenames = sorted([file for file in os.listdir(directory)])\n",
    "    \n",
    "    for key in filenames:\n",
    "        s3.Bucket(bucket).Object(os.path.join(trip_prefix,prefix,key)).upload_file(os.path.join(directory,key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_data_folders = [(MY_BAYWHEELS,'BayWheels'),\n",
    "                      (MY_BLUEBIKE,'BlueBike'),\n",
    "                      (MY_CAPITAL, 'CapitalBike'),\n",
    "                      (MY_CITIBIKE, 'CitiBike'),\n",
    "                      (MY_DIVVY, 'DivvyBike')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, prefix in local_data_folders:\n",
    "    s3_upload(directory, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NYC Neighborhood Data Uploads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_prefix = \"HoodData\"\n",
    "filenames = sorted([file for file in os.listdir(TEMP_HOOD_FOLDER)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in filenames:\n",
    "    s3.Bucket(bucket).Object(os.path.join(hood_prefix,key)).upload_file(TEMP_HOOD_FOLDER + key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Manual Uploads**\n",
    "The remaining files were manually downloaded and uploaded to S3 using the management console. ***Note: It isn't recommended that you change the 4 character codes for files: SX01, ZX01, ZX02, ZX03. They are in the HoodData folder with the other 59 NYC files and changing the codes will require major changes to the code in future notebooks.***\n",
    "\n",
    "Inside the HoodData \"Folder\" within the Bucket:\n",
    "- San Francisco Profiles = \"SX01_SanFran-Neighborhoods-Data.pdf\"\n",
    "- Zipcodes USA = \"ZX01_Zipcodes-USA.csv\"\n",
    "- Delineations = \"ZX02_Delineation.xls\"\n",
    "- USPS Crosswalk = \"ZX03_USPS-Crosswalk.xlsx\"\n",
    "\n",
    "Inside the GeoSpatial \"Folder\" within the Bucket:\n",
    "- NYC GeoJSON File = \"NYC-Neighborhoods.geojson\"\n",
    "- San Francisco GeoJSON File = \"San-Francisco-Neighborhoods.geojson\"\n",
    "- Subway Entrance GeoJSON File = \"MTA-Subway-Entrances.geojson\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Delete the Local Repositories**\n",
    "\n",
    "The purpose of this section is to clear out our local repository now that we uploaded all the files to our S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for directory, name in local_data_folders:\n",
    "    shutil.rmtree(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_HOOD_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Project Extension**\n",
    "\n",
    "In the future I would like to convert the download and/or upload process to a python script that can be run outside of the Jupyter Notebook.\n",
    "\n",
    "<div style=\"line-height:11px\">\n",
    "    <p style=\"text-align:right;font-style:italic;color:#c1121f\"> <b> Data Science = Solving Problems = Happiness </b> </p>\n",
    "    <p style=\"text-align:right;\"> <b> Denzel S. Williams <b> </p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
